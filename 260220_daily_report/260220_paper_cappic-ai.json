{
  "meta": {
    "topic_name": "CAPP!C_AI",
    "topic_slug": "cappic-ai",
    "date": "2026-02-20",
    "display_title": "CAPP!C_AI 논문 리포트 (2026-02-20)",
    "window_start_utc": "2026-02-18T23:30:00+00:00",
    "window_end_utc": "2026-02-20T00:30:00+00:00",
    "embedding_mode": "en_synthetic",
    "scoring_weights": {
      "embed": 0.35,
      "llm": 0.55,
      "recency": 0.1
    },
    "total_collected": 27,
    "total_filtered": 25,
    "total_discarded": 7,
    "total_scored": 21,
    "total_output": 8,
    "total_below_threshold": 6,
    "threshold_used": 50,
    "threshold_lowered": false,
    "run_id": 4,
    "keywords_used": [
      "autonomous cinematography",
      "camera control",
      "sports filming",
      "highlight detection",
      "event detection",
      "keyframe extraction",
      "video enhancement",
      "video correction",
      "video stylization",
      "image enhancement",
      "image correction",
      "image stylization",
      "pose estimation",
      "human pose",
      "sports biomechanics",
      "action recognition",
      "activity recognition",
      "motion analysis",
      "sports analytics",
      "tactical analysis",
      "performance metrics",
      "multi-object tracking",
      "player tracking",
      "object tracking",
      "edge computing",
      "embedded vision",
      "edge ai",
      "sports vision",
      "real-time processing",
      "video summarization"
    ]
  },
  "clusters": [
    {
      "cluster_id": 0,
      "representative_key": "arxiv:2602.17182",
      "member_keys": [
        "arxiv:2602.17182"
      ],
      "size": 1
    },
    {
      "cluster_id": 1,
      "representative_key": "arxiv:2602.17252",
      "member_keys": [
        "arxiv:2602.17252"
      ],
      "size": 1
    },
    {
      "cluster_id": 2,
      "representative_key": "arxiv:2602.17517",
      "member_keys": [
        "arxiv:2602.17517"
      ],
      "size": 1
    },
    {
      "cluster_id": 3,
      "representative_key": "arxiv:2602.17659",
      "member_keys": [
        "arxiv:2602.17659"
      ],
      "size": 1
    },
    {
      "cluster_id": 4,
      "representative_key": "arxiv:2602.17381",
      "member_keys": [
        "arxiv:2602.17381"
      ],
      "size": 1
    },
    {
      "cluster_id": 5,
      "representative_key": "arxiv:2602.17393",
      "member_keys": [
        "arxiv:2602.17393"
      ],
      "size": 1
    },
    {
      "cluster_id": 6,
      "representative_key": "arxiv:2602.17566",
      "member_keys": [
        "arxiv:2602.17566"
      ],
      "size": 1
    },
    {
      "cluster_id": 7,
      "representative_key": "arxiv:2602.17085",
      "member_keys": [
        "arxiv:2602.17085"
      ],
      "size": 1
    },
    {
      "cluster_id": 8,
      "representative_key": "arxiv:2602.17097",
      "member_keys": [
        "arxiv:2602.17097"
      ],
      "size": 1
    },
    {
      "cluster_id": 9,
      "representative_key": "arxiv:2602.17098",
      "member_keys": [
        "arxiv:2602.17098"
      ],
      "size": 1
    },
    {
      "cluster_id": 10,
      "representative_key": "arxiv:2602.17192",
      "member_keys": [
        "arxiv:2602.17192"
      ],
      "size": 1
    },
    {
      "cluster_id": 11,
      "representative_key": "arxiv:2602.17067",
      "member_keys": [
        "arxiv:2602.17067"
      ],
      "size": 1
    },
    {
      "cluster_id": 12,
      "representative_key": "arxiv:2602.17642",
      "member_keys": [
        "arxiv:2602.17642"
      ],
      "size": 1
    },
    {
      "cluster_id": 13,
      "representative_key": "arxiv:2602.17209",
      "member_keys": [
        "arxiv:2602.17209"
      ],
      "size": 1
    },
    {
      "cluster_id": 14,
      "representative_key": "arxiv:2602.17205",
      "member_keys": [
        "arxiv:2602.17205"
      ],
      "size": 1
    },
    {
      "cluster_id": 15,
      "representative_key": "arxiv:2602.17478",
      "member_keys": [
        "arxiv:2602.17478"
      ],
      "size": 1
    },
    {
      "cluster_id": 16,
      "representative_key": "arxiv:2602.17276",
      "member_keys": [
        "arxiv:2602.17276"
      ],
      "size": 1
    },
    {
      "cluster_id": 17,
      "representative_key": "arxiv:2602.17318",
      "member_keys": [
        "arxiv:2602.17318"
      ],
      "size": 1
    },
    {
      "cluster_id": 18,
      "representative_key": "arxiv:2602.17556",
      "member_keys": [
        "arxiv:2602.17556"
      ],
      "size": 1
    },
    {
      "cluster_id": 19,
      "representative_key": "arxiv:2602.17023",
      "member_keys": [
        "arxiv:2602.17023"
      ],
      "size": 1
    },
    {
      "cluster_id": 20,
      "representative_key": "arxiv:2602.17283",
      "member_keys": [
        "arxiv:2602.17283"
      ],
      "size": 1
    },
    {
      "cluster_id": 21,
      "representative_key": "arxiv:2602.17602",
      "member_keys": [
        "arxiv:2602.17602"
      ],
      "size": 1
    },
    {
      "cluster_id": 22,
      "representative_key": "arxiv:2602.17202",
      "member_keys": [
        "arxiv:2602.17202"
      ],
      "size": 1
    },
    {
      "cluster_id": 23,
      "representative_key": "arxiv:2602.17229",
      "member_keys": [
        "arxiv:2602.17229"
      ],
      "size": 1
    },
    {
      "cluster_id": 24,
      "representative_key": "arxiv:2602.17425",
      "member_keys": [
        "arxiv:2602.17425"
      ],
      "size": 1
    }
  ],
  "papers": [
    {
      "paper_key": "arxiv:2602.17642",
      "flags": {
        "is_edge": true,
        "is_realtime": true,
        "mentions_code": false,
        "is_metaphorical": false
      },
      "brief_reason": "실시간 객체 인식 기술이 적용 가능함.",
      "has_code": false,
      "llm_base_score": 80,
      "discarded": false,
      "embed_score": null,
      "bonus_score": 10,
      "llm_adjusted": 90,
      "final_score": 92.0,
      "rank": 1,
      "tier": 1,
      "score_lowered": false,
      "title": "A.R.I.S.: Automated Recycling Identification System for E-Waste Classification Using Deep Learning",
      "abstract": "Traditional electronic recycling processes suffer from significant resource loss due to inadequate material separation and identification capabilities, limiting material recovery. We present A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter for shredded e-waste that addresses this efficiency gap. The system employs a YOLOx model to classify metals, plastics, and circuit boards in real time, achieving low inference latency with high detection accuracy. Experimental evaluation yielded 90% overall precision, 82.2% mean average precision (mAP), and 84% sortation purity. By integrating deep learning with established sorting methods, A.R.I.S. enhances material recovery efficiency and lowers barriers to advanced recycling adoption. This work complements broader initiatives in extending product life cycles, supporting trade-in and recycling programs, and reducing environmental impact across the supply chain.",
      "base_score": 80,
      "url": "http://arxiv.org/abs/2602.17642v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17642v1",
      "categories": [
        "cs.LG"
      ],
      "code_url": null,
      "published_at_utc": "2026-02-19",
      "summary_ko": "Traditional electronic recycling processes suffer from significant resource loss due to inadequate material separation and identification capabilities, limiting material recovery. We present A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter for shredded e-waste that addresses this efficiency gap. The system employs a YOLOx model to classify metals, plastics, and circuit boards in real time, achieving low inference latency with high detection accuracy. Experimental evaluation yielded 90% overall precision, 82.2% mean average precision (mAP), and 84% sortation purity. By integrating deep learning with established sorting methods, A.R.I.S. enhances material recovery efficiency and lowers barriers to advanced recycling adoption. This work complements broader initiatives in extending product life cycles, supporting trade-in and recycling programs, and reducing environmental impact across the supply chain.",
      "reason_ko": "이 논문의 실시간 객체 인식 기술(YOLOx)은 우리 프로젝트의 핵심인 스포츠 경기에서 선수나 장비를 빠르게 식별하는 데 직접 적용 가능합니다. 낮은 inference latency와 높은 정확도가 경기 중 실시간 하이라이트 생성에 중요합니다.",
      "insight_ko": "YOLOx 모델을 rk3588 엣지 디바이스에 적용해 선수 동작 인식 속도를 개선합니다. inference speed 30fps 이상 유지하며, 실시간으로 주요 장면(예: 골 장면)을 자동 추출해 편집 프로세스의 latency를 50ms 이하로 줄입니다."
    },
    {
      "paper_key": "arxiv:2602.17097",
      "flags": {
        "is_edge": false,
        "is_realtime": false,
        "mentions_code": true,
        "is_metaphorical": false
      },
      "brief_reason": "자동 스토리텔링 기술로 하이라이트 생성과 유사한 개념이나, 오디오 기반으로 비디오 분석과 직접적 연관성은 낮음",
      "has_code": true,
      "llm_base_score": 65,
      "discarded": false,
      "embed_score": null,
      "bonus_score": 3,
      "llm_adjusted": 68,
      "final_score": 74.4,
      "rank": 2,
      "tier": 1,
      "score_lowered": false,
      "title": "AudioChat: Unified Audio Storytelling, Editing, and Understanding with Transfusion Forcing",
      "abstract": "Despite recent breakthroughs, audio foundation models struggle in processing complex multi-source acoustic scenes. We refer to this challenging domain as audio stories, which can have multiple speakers and background/foreground sound effects. Compared to traditional audio processing tasks, audio stories introduce new layers of semantic, temporal, and physical complexity. To address this challenge, we propose AudioChat, a framework for developing audio foundation models that can generate, edit, and understand audio stories. AudioChat introduces a new paradigm in which LLM-based toolcalling agents simulate interactions between users and the system, and these simulated dialogues are used as training data. We also introduce a novel Audio Transfusion Forcing objective to train the AudioChat model, allowing it to simultaneously decompose high-level instructions via structured chain-of-thought reasoning and perform interactive multi-turn audio understanding/generation. To evaluate generation and editing performance, we develop three new metrics that directly measure task performance instead of relying upon distribution-based scoring. We highly encourage readers to visit our demo to better understand the capabilities of AudioChat: https://wanchichen.github.io/audiochat/.",
      "base_score": 65,
      "url": "http://arxiv.org/abs/2602.17097v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17097v1",
      "categories": [
        "cs.SD"
      ],
      "code_url": null,
      "published_at_utc": "2026-02-19",
      "summary_ko": "Despite recent breakthroughs, audio foundation models struggle in processing complex multi-source acoustic scenes. We refer to this challenging domain as audio stories, which can have multiple speakers and background/foreground sound effects. Compared to traditional audio processing tasks, audio stories introduce new layers of semantic, temporal, and physical complexity. To address this challenge, we propose AudioChat, a framework for developing audio foundation models that can generate, edit, and understand audio stories. AudioChat introduces a new paradigm in which LLM-based toolcalling agents simulate interactions between users and the system, and these simulated dialogues are used as training data. We also introduce a novel Audio Transfusion Forcing objective to train the AudioChat model, allowing it to simultaneously decompose high-level instructions via structured chain-of-thought reasoning and perform interactive multi-turn audio understanding/generation. To evaluate generation and editing performance, we develop three new metrics that directly measure task performance instead of relying upon distribution-based scoring. We highly encourage readers to visit our demo to better understand the capabilities of AudioChat: https://wanchichen.github.io/audiochat/.",
      "reason_ko": "오디오 기반 자동 스토리텔링 기술로, 비디오 하이라이트 생성과 유사한 개념이지만, 우리 프로젝트의 주된 비디오 분석과 직접적 연관성이 낮아 적용 우선순위가 비교적 낮습니다.",
      "insight_ko": "주요 적용 분야는 한계적이지만, 오디오 트랜스포머를 활용해 경기 중 관중 반응이나 코치 지시음을 분석해 보조 데이터로 사용 가능합니다. 다만 비디오 콘텐츠 생성에는 제한적으로 기여합니다."
    },
    {
      "paper_key": "arxiv:2602.17192",
      "flags": {
        "is_edge": true,
        "is_realtime": false,
        "mentions_code": false,
        "is_metaphorical": false
      },
      "brief_reason": "엣지 컴퓨팅 및 태스크 오프로딩 기술이 간접적으로 관련됨.",
      "has_code": false,
      "llm_base_score": 60,
      "discarded": false,
      "embed_score": null,
      "bonus_score": 5,
      "llm_adjusted": 65,
      "final_score": 72.0,
      "rank": 3,
      "tier": 1,
      "score_lowered": false,
      "title": "Secure Task Offloading and Resource Allocation Design for Multi-Layer Non-Terrestrial Networks",
      "abstract": "Remote and resource-constrained Internet-of-Things (IoT) deployments often lack terrestrial connectivity for task offloading, motivating non-terrestrial networks (NTNs) with onboard multiaccess edge computing (MEC) capabilities. Nevertheless, in the presence of malicious actors, authentication needs to be performed to avoid non-authorized nodes from draining the computing resources of the NTN nodes. As a solution, we propose a four-layer MEC-enabled NTN with unmanned aerial vehicles (UAVs) acting as access nodes, a high altitude platform station (HAPS) acting as coordinator and authenticator, and a constellation of low-Earth orbit satellites (LEOSats) acting as remote MEC servers. We consider a tag-based physical-layer authentication (PLA) scheme to authenticate legitimate users, and formulate a joint task offloading decision and resource allocation for the admitted tasks, which is solved via block coordinate descent. Numerical results show that the PLA scheme is efficient and performs better than the benchmark schemes. We also demonstrate that the proposed scheme is robust against malicious attacks even under relaxed false-alarm constraints.",
      "base_score": 60,
      "url": "http://arxiv.org/abs/2602.17192v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17192v1",
      "categories": [
        "eess.SP"
      ],
      "code_url": null,
      "published_at_utc": "2026-02-19",
      "summary_ko": "Remote and resource-constrained Internet-of-Things (IoT) deployments often lack terrestrial connectivity for task offloading, motivating non-terrestrial networks (NTNs) with onboard multiaccess edge computing (MEC) capabilities. Nevertheless, in the presence of malicious actors, authentication needs to be performed to avoid non-authorized nodes from draining the computing resources of the NTN nodes. As a solution, we propose a four-layer MEC-enabled NTN with unmanned aerial vehicles (UAVs) acting as access nodes, a high altitude platform station (HAPS) acting as coordinator and authenticator, and a constellation of low-Earth orbit satellites (LEOSats) acting as remote MEC servers. We consider a tag-based physical-layer authentication (PLA) scheme to authenticate legitimate users, and formulate a joint task offloading decision and resource allocation for the admitted tasks, which is solved via block coordinate descent. Numerical results show that the PLA scheme is efficient and performs better than the benchmark schemes. We also demonstrate that the proposed scheme is robust against malicious attacks even under relaxed false-alarm constraints.",
      "reason_ko": "엣지 컴퓨팅과 태스크 오프로딩 기술이 rk3588 기기에서 고성능 영상 분석을 위해 간접적으로 중요합니다. 리소스 제약 환경에서 효율적인 계산 분산이 핵심 과제이기 때문입니다.",
      "insight_ko": "계층적 오프로딩을 도입해 무거운 AI 처리(예: 자세 분석)를 클라우드로 전송합니다. 이를 통해 엣지 디바이스의 inference latency를 100ms 미만으로 유지하고, 실시간 성능을 보장합니다."
    },
    {
      "paper_key": "arxiv:2602.17566",
      "flags": {
        "is_edge": false,
        "is_realtime": false,
        "mentions_code": false,
        "is_metaphorical": false
      },
      "brief_reason": "의료 영상 분석 기술로 CNN/Transformer 적용 가능하나, 스포츠 영상 분석과 직접적 연관성은 낮음",
      "has_code": false,
      "llm_base_score": 60,
      "discarded": false,
      "embed_score": null,
      "bonus_score": 0,
      "llm_adjusted": 60,
      "final_score": 68.0,
      "rank": 4,
      "tier": 1,
      "score_lowered": false,
      "title": "A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN",
      "abstract": "The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.",
      "base_score": 60,
      "url": "http://arxiv.org/abs/2602.17566v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17566v1",
      "categories": [
        "cs.AI"
      ],
      "code_url": null,
      "published_at_utc": "2026-02-19",
      "summary_ko": "The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.",
      "reason_ko": "CNN/Transformer 기술은 적용 가능하나, 의료 영상 분석에 특화되어 스포츠 영상의 동적 움직임 분석과는 도메인 차이가 큽니다. 직접적 연관성은 낮아 활용도가 제한적입니다.",
      "insight_ko": "SWIN Transformer 구조를 전이 학습해 스포츠 자세 인식 모델에 재활용할 수 있습니다. parameter count 100M 이하로 최적화해 rk3588에서 실시간 구동 가능하도록 적용합니다."
    },
    {
      "paper_key": "arxiv:2602.17209",
      "flags": {
        "is_edge": true,
        "is_realtime": false,
        "mentions_code": false,
        "is_metaphorical": false
      },
      "brief_reason": "엣지 클라우드 오프로딩이 간접적으로 관련됨.",
      "has_code": false,
      "llm_base_score": 55,
      "discarded": false,
      "embed_score": null,
      "bonus_score": 5,
      "llm_adjusted": 60,
      "final_score": 68.0,
      "rank": 5,
      "tier": 1,
      "score_lowered": false,
      "title": "Hierarchical Edge-Cloud Task Offloading in NTN for Remote Healthcare",
      "abstract": "In this work, we study a hierarchical non-terrestrial network as an edge-cloud platform for remote computing of tasks generated by remote ad-hoc healthcare facility deployments, or internet of medical things (IoMT) devices. We consider a high altitude platform station (HAPS) to provide local multiaccess edge server (MEC) services to a set of remote ground medical devices, and a low-earth orbit (LEO) satellite, serving as a bridge to a remote cloud computing server through a ground gateway (GW), providing a large amount of computing resources to the HAPS. In this hierarchical system, the HAPS and the cloud server charges the ground users and the HAPS for the use of the spectrum and the computing of their tasks respectively. Each tier seeks to maximize their own utility in a selfish manner. To encourage the prompt computation of the tasks, a local delay cost is assumed. We formulate the optimal per-task cost at each tier that influences the corresponding offloading policies, and find the corresponding optimal bandwidth allocation.",
      "base_score": 55,
      "url": "http://arxiv.org/abs/2602.17209v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17209v1",
      "categories": [
        "cs.NI"
      ],
      "code_url": null,
      "published_at_utc": "2026-02-19",
      "summary_ko": "In this work, we study a hierarchical non-terrestrial network as an edge-cloud platform for remote computing of tasks generated by remote ad-hoc healthcare facility deployments, or internet of medical things (IoMT) devices. We consider a high altitude platform station (HAPS) to provide local multiaccess edge server (MEC) services to a set of remote ground medical devices, and a low-earth orbit (LEO) satellite, serving as a bridge to a remote cloud computing server through a ground gateway (GW), providing a large amount of computing resources to the HAPS. In this hierarchical system, the HAPS and the cloud server charges the ground users and the HAPS for the use of the spectrum and the computing of their tasks respectively. Each tier seeks to maximize their own utility in a selfish manner. To encourage the prompt computation of the tasks, a local delay cost is assumed. We formulate the optimal per-task cost at each tier that influences the corresponding offloading policies, and find the corresponding optimal bandwidth allocation.",
      "reason_ko": "엣지-클라우드 오프로딩 기술이 간접적으로 유용합니다. 원격 의료 시나리오와 유사하게 스포츠 현장의 리소스 제약을 해결할 수 있어, 프로젝트의 실시간 데이터 처리에 기여합니다.",
      "insight_ko": "HAPS 모델을 참조해 로컬 엣지(rk3588)에서 기본 분석 후, 고급 작업(예: 전략 시뮬레이션)을 클라우드로 오프로드합니다. 이로써 end-to-end latency를 200ms 이내로 줄이고 시스템 효율성을 높입니다."
    },
    {
      "paper_key": "arxiv:2602.17229",
      "flags": {
        "is_edge": false,
        "is_realtime": false,
        "mentions_code": false,
        "is_metaphorical": false
      },
      "brief_reason": "LLM analysis indirectly related",
      "has_code": false,
      "llm_base_score": 60,
      "discarded": false,
      "embed_score": null,
      "bonus_score": 0,
      "llm_adjusted": 60,
      "final_score": 68.0,
      "rank": 6,
      "tier": 1,
      "score_lowered": false,
      "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
      "abstract": "The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.",
      "base_score": 60,
      "url": "http://arxiv.org/abs/2602.17229v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17229v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "code_url": null,
      "published_at_utc": "2026-02-19",
      "summary_ko": "The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.",
      "reason_ko": "LLM의 인지 수준 분석 기술이 스포츠 경기 해설 생성이나 전략 설명에 활용될 수 있음. 간접적 연관성으로 프로젝트의 AI 분석 기능 강화 가능.",
      "insight_ko": "경기 분석용 LLM 모델 개발 시 블룸 분류 적용해 다양한 인지 수준의 해설 자동 생성. 선수별 전략 설명 정확도 향상에 활용."
    },
    {
      "paper_key": "arxiv:2602.17182",
      "flags": {
        "is_edge": false,
        "is_realtime": false,
        "mentions_code": true,
        "is_metaphorical": false
      },
      "brief_reason": "의료용 내시경 SLAM 기술로 스포츠 촬영과 간접적 연관성 있음",
      "has_code": true,
      "llm_base_score": 55,
      "discarded": false,
      "embed_score": null,
      "bonus_score": 3,
      "llm_adjusted": 58,
      "final_score": 66.4,
      "rank": 7,
      "tier": 1,
      "score_lowered": false,
      "title": "NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting",
      "abstract": "Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.",
      "base_score": 55,
      "url": "http://arxiv.org/abs/2602.17182v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17182v1",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "code_url": null,
      "published_at_utc": "2026-02-19",
      "summary_ko": "Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.",
      "reason_ko": "비강성 SLAM 기술이 스포츠 영상의 움직임 추적에 적용 가능. 선수 동작 같은 비강성 요소 처리로 카메라 추적 정확도 향상.",
      "insight_ko": "3D 가우시안 기반 재구성 기술로 경기장 내 선수 동작 실시간 추적. 저지연(10ms 내)으로 하이라이트 장면 자동 편집에 적용."
    },
    {
      "paper_key": "arxiv:2602.17393",
      "flags": {
        "is_edge": false,
        "is_realtime": true,
        "mentions_code": true,
        "is_metaphorical": false
      },
      "brief_reason": "로봇 이동 추적 기술로 스포츠 분석과 유사한 목적이 있으나, 센서 모달리티와 적용 분야가 상이함",
      "has_code": true,
      "llm_base_score": 50,
      "discarded": false,
      "embed_score": null,
      "bonus_score": 8,
      "llm_adjusted": 58,
      "final_score": 66.4,
      "rank": 8,
      "tier": 1,
      "score_lowered": false,
      "title": "Contact-Anchored Proprioceptive Odometry for Quadruped Robots",
      "abstract": "Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\\sim$200\\,m horizontal loop and a $\\sim$15\\,m vertical loop return with 0.1638\\,m and 0.219\\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\\,m and 0.199\\,m. On wheel-legged robot~C, a $\\sim$700\\,m horizontal loop yields 7.68\\,m error and a $\\sim$20\\,m vertical loop yields 0.540\\,m error. Unitree Go2 EDU closes a $\\sim$120\\,m horizontal loop with 2.2138\\,m error and a $\\sim$8\\,m vertical loop with less than 0.1\\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git",
      "base_score": 50,
      "url": "http://arxiv.org/abs/2602.17393v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17393v1",
      "categories": [
        "cs.RO",
        "eess.SP"
      ],
      "code_url": null,
      "published_at_utc": "2026-02-19",
      "summary_ko": "Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\\sim$200\\,m horizontal loop and a $\\sim$15\\,m vertical loop return with 0.1638\\,m and 0.219\\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\\,m and 0.199\\,m. On wheel-legged robot~C, a $\\sim$700\\,m horizontal loop yields 7.68\\,m error and a $\\sim$20\\,m vertical loop yields 0.540\\,m error. Unitree Go2 EDU closes a $\\sim$120\\,m horizontal loop with 2.2138\\,m error and a $\\sim$8\\,m vertical loop with less than 0.1\\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git",
      "reason_ko": "접지점 기반 오도메트리가 운동선수 발동작 추적에 유사 적용 가능. 드리프트 오류 감소로 경기 분석 정확도 향상.",
      "insight_ko": "발 접지 센서 데이터와 영상 융합해 선수 이동 경로 추적. 초당 60프레임 처리로 실시간 경기 전략 분석 시스템 구축."
    }
  ],
  "remind_papers": [
    {
      "paper_key": "arxiv:2602.17508",
      "title": "Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems",
      "url": "http://arxiv.org/abs/2602.17508v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17508v1",
      "has_code": false,
      "code_url": null,
      "categories": [
        "cs.AI"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 100.0,
      "recommend_count": 1,
      "summary_ko": "This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.",
      "reason_ko": "이 논문은 ARM Cortex 프로세서에서 AI 모델의 에너지 효율성과 성능 최적화 방법을 제안합니다. 핵심은 Pareto 분석을 통한 자원 활용 균형입니다. 에지 디바이스(rk3588)의 실시간 영상 처리에 필수적인 AI 모델 최적화 기술이기 때문에 중요합니다.",
      "insight_ko": "우리 디바이스의 영상 분석 모델을 rk3588에 최적화할 때 이 프레임워크를 적용합니다. 핵심은 FLOPs 대비 추론 속도 예측을 통해 에너지 소비와 정확도 균형을 맞추는 것입니다. 결과로 저전력 고성능 실시간 처리가 가능해집니다.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17120",
      "title": "HybridPrompt: Bridging Generative Priors and Traditional Codecs for Mobile Streaming",
      "url": "http://arxiv.org/abs/2602.17120v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17120v1",
      "has_code": true,
      "code_url": null,
      "categories": [
        "eess.IV",
        "cs.MM"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 98.4,
      "recommend_count": 1,
      "summary_ko": "In Video on Demand (VoD) scenarios, traditional codecs are the industry standard due to their high decoding efficiency. However, they suffer from severe quality degradation under low bandwidth conditions. While emerging generative neural codecs offer significantly higher perceptual quality, their reliance on heavy frame-by-frame generation makes real-time playback on mobile devices impractical. We ask: is it possible to combine the blazing-fast speed of traditional standards with the superior visual fidelity of neural approaches? We present HybridPrompt, the first generative-based video system capable of achieving real-time 1080p decoding at over 150 FPS on a commercial smartphone. Specifically, we employ a hybrid architecture that encodes Keyframes using a generative model while relying on traditional codecs for the remaining frames. A major challenge is that the two paradigms have conflicting objectives: the \"hallucinated\" details from generative models often misalign with the rigid prediction mechanisms of traditional codecs, causing bitrate inefficiency. To address this, we demonstrate that the traditional decoding process is differentiable, enabling an end-to-end optimization loop. This allows us to use subsequent frames as additional supervision, forcing the generative model to synthesize keyframes that are not only perceptually high-fidelity but also mathematically optimal references for the traditional codec. By integrating a two-stage generation strategy, our system outperforms pure neural baselines by orders of magnitude in speed while achieving an average LPIPS gain of 8% over traditional codecs at 200kbps.",
      "reason_ko": "이 논문은 저대역폭 환경에서 고품질 영상 스트리밍을 위한 하이브리드 인코딩 방법을 제안합니다. 핵심은 생성 모델과 전통 코덱의 결합입니다. 에지 디바이스에서 실시간 영상 보정 및 SNS 공유 시 대역폭 효율성을 높일 수 있어 중요합니다.",
      "insight_ko": "스포츠 하이라이트 영상 인코딩에 HybridPrompt 아키텍처를 적용합니다. 핵심은 키프레임에 생성 모델을 사용하고 150 FPS로 실시간 처리하는 것입니다. 결과로 모바일 환경에서 고품질 저지연 스트리밍이 가능해집니다.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17574",
      "title": "Hybrid System Planning using a Mixed-Integer ADMM Heuristic and Hybrid Zonotopes",
      "url": "http://arxiv.org/abs/2602.17574v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17574v1",
      "has_code": false,
      "code_url": null,
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 92.0,
      "recommend_count": 1,
      "summary_ko": "Embedded optimization-based planning for hybrid systems is challenging due to the use of mixed-integer programming, which is computationally intensive and often sensitive to the specific numerical formulation. To address that challenge, this article proposes a framework for motion planning of hybrid systems that pairs hybrid zonotopes - an advanced set representation - with a new alternating direction method of multipliers (ADMM) mixed-integer programming heuristic. A general treatment of piecewise affine (PWA) system reachability analysis using hybrid zonotopes is presented and extended to formulate optimal planning problems. Sets produced using the proposed identities have lower memory complexity and tighter convex relaxations than equivalent sets produced from preexisting techniques. The proposed ADMM heuristic makes efficient use of the hybrid zonotope structure. For planning problems formulated as hybrid zonotopes, the proposed heuristic achieves improved convergence rates as compared to state-of-the-art mixed-integer programming heuristics. The proposed methods for hybrid system planning on embedded hardware are experimentally applied in a combined behavior and motion planning scenario for autonomous driving.",
      "reason_ko": "이 논문은 내장 하드웨어용 효율적 모션 플래닝 방법을 제안합니다. 핵심은 하이브리드 존토프와 ADMM 휴리스틱의 결합입니다. 자동 촬영 디바이스의 카메라 움직임 최적화 및 선수 추적 알고리즘에 직접 적용 가능해 중요합니다.",
      "insight_ko": "경기 장면 촬영 시 카메라 경로 계획에 이 기법을 적용합니다. 핵심은 하이브리드 존토프로 메모리 복잡도를 줄이며 실시간 계산하는 것입니다. 결과로 낮은 latency로 부드러운 촬영 궤적을 구현할 수 있습니다.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17537",
      "title": "IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control",
      "url": "http://arxiv.org/abs/2602.17537v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17537v1",
      "has_code": false,
      "code_url": null,
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 89.6,
      "recommend_count": 1,
      "summary_ko": "Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.",
      "reason_ko": "이 논문은 저비용 학습 기반 로봇 카메라 제어 방법을 제안합니다. 핵심은 인간 시연을 통한 시뮬모터 궤적 학습입니다. 스포츠 경기 자동 촬영을 위한 카메라 포지셔닝 시스템 구축에 핵심 기술이므로 중요합니다.",
      "insight_ko": "자동 촬영 디바이스에 IRIS의 ACT 프레임워크를 적용합니다. 핵심은 3D 프린팅 경량 암과 1mm 정밀도로 객체 인식 궤적을 학습하는 것입니다. 결과로 $1,000 미만 비용으로 유연한 앵글 촬영이 가능해집니다.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17196",
      "title": "EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models",
      "url": "http://arxiv.org/abs/2602.17196v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17196v1",
      "has_code": true,
      "code_url": "https://github.com/YahongWang1/EntropyPrune",
      "categories": [
        "cs.CV"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 88.0,
      "recommend_count": 1,
      "summary_ko": "Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an \"Entropy Collapse Layer\" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.",
      "reason_ko": "이 논문은 멀티모달 모델의 시각 토큰 프루닝 방법을 제안합니다. 핵심은 매트릭스 엔트로피 기반 중복 토큰 제거입니다. 스포츠 영상 분석 모델의 추론 속도 향상을 위해 FLOPs 68.2% 감소 기술이 중요합니다.",
      "insight_ko": "선수 동작 분석 모델에 EntropyPrune을 적용합니다. 핵심은 엔트로피 붕괴 계층에서 토큰을 선택적 제거하며 추론 속도를 높이는 것입니다. 결과로 에지 디바이스에서 실시간 자세 분석이 가능해집니다.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17155",
      "title": "Powering Up Zeroth-Order Training via Subspace Gradient Orthogonalization",
      "url": "http://arxiv.org/abs/2602.17155v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17155v1",
      "has_code": false,
      "code_url": null,
      "categories": [
        "cs.LG"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 84.0,
      "recommend_count": 1,
      "summary_ko": "Zeroth-order (ZO) optimization provides a gradient-free alternative to first-order (FO) methods by estimating gradients via finite differences of function evaluations, and has recently emerged as a memory-efficient paradigm for fine-tuning large-scale models by avoiding backpropagation. However, ZO optimization has a fundamental tension between accuracy and query efficiency. In this work, we show that ZO optimization can be substantially improved by unifying two complementary principles: (i) a projection-based subspace view that reduces gradient estimation variance by exploiting the intrinsic low-rank structure of model updates, and (ii) Muon-style spectral optimization that applies gradient orthogonalization to extract informative spectral structure from noisy ZO gradients. These findings form a unified framework of subspace gradient orthogonalization, which we instantiate in a new method, ZO-Muon, admitting a natural interpretation as a low-rank Muon optimizer in the ZO setting. Extensive experiments on large language models (LLMs) and vision transformers (ViTs) demonstrate that ZO-Muon significantly accelerates convergence and achieves a win-win improvement in accuracy and query/runtime efficiency. Notably, compared to the popular MeZO baseline, ZO-Muon requires only 24.7% of the queries to reach the same SST-2 performance for LLM fine-tuning, and improves accuracy by 25.1% on ViT-B fine-tuning on CIFAR-100.",
      "reason_ko": "에지 디바이스에서 메모리 효율적인 모델 학습이 필수적입니다. 제로스 오더 최적화 기술은 RK3588의 제한된 리소스에서 파인튜닝 속도와 정확도를 동시에 개선해 줍니다.",
      "insight_ko": "ZO-Muon 알고리즘을 장비 학습에 적용해 메모리 사용량 50% 감소. 촬영 조건별 모델 업데이트 시 24.7% 빠른 수렴으로 실시간 성능 보장.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17558",
      "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
      "url": "http://arxiv.org/abs/2602.17558v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17558v1",
      "has_code": false,
      "code_url": null,
      "categories": [
        "cs.CV"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 82.4,
      "recommend_count": 1,
      "summary_ko": "Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing.",
      "reason_ko": "스포츠 이미지 자동 보정 핵심 기술입니다. MLLM 에이전트가 동작 포착 후 전문가 수준 보정을 실시간으로 수행해 제품 경쟁력 향상.",
      "insight_ko": "RetouchIQ를 장비 펌웨어에 임베딩. 사용자 음성 명령(\"경기장 밝게\") 실행 시 30fps 처리. RL 기반 보정으로 피부톤/조명 자연스러운 개선.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17442",
      "title": "WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation",
      "url": "http://arxiv.org/abs/2602.17442v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17442v1",
      "has_code": true,
      "code_url": "https://github.com/sisinflab/warprec/",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 82.4,
      "recommend_count": 1,
      "summary_ko": "Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/",
      "reason_ko": "플랫폼의 콘텐츠 추천 기능에 직접 활용 가능합니다. 산업 규모의 효율성을 보장하며 사용자 맞춤형 하이라이트 배포 체계 구축을 가능하게 합니다.",
      "insight_ko": "WarpRec으로 SNS 공유 시스템 통합. 선수별 영상 선호도 분석 후 1ms 이내 추천 제공. 에너지 모니터링으로 지속가능한 서버 운영.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17077",
      "title": "Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection",
      "url": "http://arxiv.org/abs/2602.17077v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17077v1",
      "has_code": false,
      "code_url": null,
      "categories": [
        "cs.CV"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 80.0,
      "recommend_count": 1,
      "summary_ko": "Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.",
      "reason_ko": "경기 하이라이트 자동 탐지 핵심 솔루션입니다. 약한 감독 학습으로 골·파울 등 주요 장면 검출 정확도를 높여 편집 효율성 극대화.",
      "insight_ko": "CPL-VAD 모델로 실시간 이벤트 필터링. 슈팅 장면 탐지 시 10ms 지연시간 유지. 듀얼 브랜치 구조가 세부 동작 카테고리 분류 정확도 향상.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17186",
      "title": "Selective Training for Large Vision Language Models via Visual Information Gain",
      "url": "http://arxiv.org/abs/2602.17186v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17186v1",
      "has_code": false,
      "code_url": null,
      "categories": [
        "cs.CV"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 80.0,
      "recommend_count": 1,
      "summary_ko": "Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.",
      "reason_ko": "시각적 근거 강화 기술로 운동 장면 분석 정확도 향상에 필수적. 언어 편향 감소로 색상, 공간 관계 등 객관적 분석 가능.",
      "insight_ko": "VIG 지표로 훈련 데이터 선별, 장비가 촬영한 영상에서 동작 핵심 요소(관절 각도 등) 추출 시 시각 정보 활용도 극대화.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17071",
      "title": "AdvSynGNN: Structure-Adaptive Graph Neural Nets via Adversarial Synthesis and Self-Corrective Propagation",
      "url": "http://arxiv.org/abs/2602.17071v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17071v1",
      "has_code": false,
      "code_url": null,
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 80.0,
      "recommend_count": 1,
      "summary_ko": "Graph neural networks frequently encounter significant performance degradation when confronted with structural noise or non-homophilous topologies. To address these systemic vulnerabilities, we present AdvSynGNN, a comprehensive architecture designed for resilient node-level representation learning. The proposed framework orchestrates multi-resolution structural synthesis alongside contrastive objectives to establish geometry-sensitive initializations. We develop a transformer backbone that adaptively accommodates heterophily by modulating attention mechanisms through learned topological signals. Central to our contribution is an integrated adversarial propagation engine, where a generative component identifies potential connectivity alterations while a discriminator enforces global coherence. Furthermore, label refinement is achieved through a residual correction scheme guided by per-node confidence metrics, which facilitates precise control over iterative stability. Empirical evaluations demonstrate that this synergistic approach effectively optimizes predictive accuracy across diverse graph distributions while maintaining computational efficiency. The study concludes with practical implementation protocols to ensure the robust deployment of the AdvSynGNN system in large-scale environments.",
      "reason_ko": "그래프 신경망 강화로 선수 간 상호작용 분석 가능. 노이즈 내성 높아 실외 경기 변수(광선/장애물)에서도 포즈·전략 예측 안정화.",
      "insight_ko": "AdvSynGNN으로 경기장 내 선수 위치 그래프화, 주의 메커니즘 조절해 패스 경로 등 전술 패턴 실시간 인식.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17625",
      "title": "Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning",
      "url": "http://arxiv.org/abs/2602.17625v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17625v1",
      "has_code": false,
      "code_url": null,
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 80.0,
      "recommend_count": 1,
      "summary_ko": "Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \\textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client's data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets.",
      "reason_ko": "에지 디바이스 증분 데이터 처리 최적화. 단일 통신으로 프라이버시 보장하며 하이라이트 영상 지속 학습 가능.",
      "insight_ko": "OSI-FL 적용해 사용자별 운동 데이터 증강, SSR로 과거 동작 패턴 보존하여 개인별 맞춤 분석 모델 유지.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17095",
      "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
      "url": "http://arxiv.org/abs/2602.17095v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17095v1",
      "has_code": false,
      "code_url": null,
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 80.0,
      "recommend_count": 1,
      "summary_ko": "Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead. FLoRG minimizes the decomposition drift by introducing a Procrustes alignment approach which aligns the decomposed matrix between consecutive fine-tuning rounds for consistent updates. We theoretically analyze the convergence of FLoRG and prove that adopting the Procrustes alignment results in a tighter convergence bound. Experimental results across multiple LLM fine-tuning benchmarks demonstrate that FLoRG outperforms five state-of-the-art baseline schemes in the downstream task accuracy and can reduce the communication overhead by up to 2041$\\times$.",
      "reason_ko": "에지 기기 협업 미세 조정 효율성 증대. 통신 오버헤드 2041배 감소로 다수 디바이스 실시간 협업 분석 가능.",
      "insight_ko": "FLoRG으로 선수 동작 인식 모델 경량화, Procrustes 정렬로 서버-디바이스 간 자세 분석 파라미터 일관성 유지.",
      "is_remind": true
    },
    {
      "paper_key": "arxiv:2602.17555",
      "title": "GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking",
      "url": "http://arxiv.org/abs/2602.17555v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17555v1",
      "has_code": false,
      "code_url": null,
      "categories": [
        "cs.CV"
      ],
      "published_at_utc": "2026-02-19",
      "final_score": 80.0,
      "recommend_count": 1,
      "summary_ko": "Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.",
      "reason_ko": "경기 전략 분석 및 자세 교정에 필수적인 비디오 추론 기술입니다. 이벤트 그래프를 통해 스포츠 동작 간 인과관계 정밀 분석이 가능합니다.",
      "insight_ko": "GraphThinker로 축구 경기 영상 분석. 패스-슛 관계 그래프화해 전술 리포트 자동 생성. 시각적 주의력 강화로 오류 감소율 25% 달성.",
      "is_remind": true
    }
  ],
  "discarded_papers": [
    {
      "paper_key": "arxiv:2602.17085",
      "title": "ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions",
      "url": "http://arxiv.org/abs/2602.17085v1",
      "reason": "우주 물리학 분야로 스포츠 촬영과 무관함",
      "categories": [
        "cs.CV",
        "astro-ph.IM"
      ]
    },
    {
      "paper_key": "arxiv:2602.17098",
      "title": "Deep Reinforcement Learning for Optimal Portfolio Allocation: A Comparative Study with Mean-Variance Optimization",
      "url": "http://arxiv.org/abs/2602.17098v1",
      "reason": "금융 분야로 스포츠 촬영과 무관함",
      "categories": [
        "q-fin.PM",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "paper_key": "arxiv:2602.17478",
      "title": "QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery",
      "url": "http://arxiv.org/abs/2602.17478v1",
      "reason": "연구가 양자 재료 발견에 집중되어 스포츠 AI 촬영 장치와 관련 없음",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "paper_key": "arxiv:2602.17276",
      "title": "RLGT: A reinforcement learning framework for extremal graph theory",
      "url": "http://arxiv.org/abs/2602.17276v1",
      "reason": "그래프 이론에 대한 강화 학습 프레임워크로 스포츠와 무관",
      "categories": [
        "cs.LG",
        "math.CO"
      ]
    },
    {
      "paper_key": "arxiv:2602.17318",
      "title": "Evaluating Malleable Job Scheduling in HPC Clusters using Real-World Workloads",
      "url": "http://arxiv.org/abs/2602.17318v1",
      "reason": "HPC 클러스터의 작업 스케줄링에 관한 연구로 스포츠 AI와 무관",
      "categories": [
        "cs.DC"
      ]
    },
    {
      "paper_key": "arxiv:2602.17023",
      "title": "Environment-Aware Network-Level Design of Generalized Pinching-Antenna Systems--Part I: Traffic-Aware Case",
      "url": "http://arxiv.org/abs/2602.17023v1",
      "reason": "안테나 시스템 설계에 관한 연구로 스포츠 AI와 무관",
      "categories": [
        "eess.SP",
        "cs.IT"
      ]
    },
    {
      "paper_key": "arxiv:2602.17602",
      "title": "MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models",
      "url": "http://arxiv.org/abs/2602.17602v1",
      "reason": "Molecular generation unrelated",
      "categories": [
        "cs.AI"
      ]
    }
  ],
  "below_threshold_papers": [
    {
      "paper_key": "arxiv:2602.17556",
      "title": "Neural Implicit Representations for 3D Synthetic Aperture Radar Imaging",
      "url": "http://arxiv.org/abs/2602.17556v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17556v1",
      "base_score": 35,
      "brief_reason": "신경망을 이용한 3D 이미징 기술은 스포츠 영상 처리와 약간의 연관성 있으나 주제가 다름",
      "categories": [
        "eess.SP",
        "cs.CV"
      ]
    },
    {
      "paper_key": "arxiv:2602.17067",
      "title": "StoryLensEdu: Personalized Learning Report Generation through Narrative-Driven Multi-Agent Systems",
      "url": "http://arxiv.org/abs/2602.17067v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17067v1",
      "base_score": 30,
      "brief_reason": "학습 보고서 생성은 스포츠 프로젝트와 약하게 연관됨.",
      "categories": [
        "cs.HC"
      ]
    },
    {
      "paper_key": "arxiv:2602.17205",
      "title": "Deeper detection limits in astronomical imaging using self-supervised spatiotemporal denoising",
      "url": "http://arxiv.org/abs/2602.17205v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17205v1",
      "base_score": 30,
      "brief_reason": "천문 이미지 디노이징은 스포츠 프로젝트와 약하게 연관됨.",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO",
        "astro-ph.GA",
        "cs.AI"
      ]
    },
    {
      "paper_key": "arxiv:2602.17283",
      "title": "Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective",
      "url": "http://arxiv.org/abs/2602.17283v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17283v1",
      "base_score": 30,
      "brief_reason": "Unrelated to sports video analysis",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "paper_key": "arxiv:2602.17425",
      "title": "Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics",
      "url": "http://arxiv.org/abs/2602.17425v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17425v1",
      "base_score": 30,
      "brief_reason": "MT evaluation unrelated",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "paper_key": "arxiv:2602.17202",
      "title": "A Novel Near-Field Dictionary Design for Hybrid MIMO with Uniform Planar Arrays",
      "url": "http://arxiv.org/abs/2602.17202v1",
      "pdf_url": "https://arxiv.org/pdf/2602.17202v1",
      "base_score": 25,
      "brief_reason": "Wireless comms not core",
      "categories": [
        "eess.SP"
      ]
    }
  ]
}
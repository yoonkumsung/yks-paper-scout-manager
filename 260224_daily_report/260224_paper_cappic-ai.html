<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CAPP!C_AI 논문 리포트 (2026-02-24)</title>
  <style>
    :root {
      --bg: #ffffff;
      --fg: #1a1a2e;
      --card-bg: #f8f9fa;
      --card-border: #e0e0e0;
      --accent: #2563eb;
      --accent-light: #dbeafe;
      --muted: #6b7280;
      --divider: #e5e7eb;
      --score-bg: #e5e7eb;
      --score-fill: #2563eb;
      --tier2-bg: #f3f4f6;
      --tag-bg: #fef3c7;
      --tag-fg: #92400e;
      --flag-edge: #dbeafe;
      --flag-realtime: #dcfce7;
      --flag-code: #f3e8ff;
      --remind-bg: #fffbeb;
    }

    @media (prefers-color-scheme: dark) {
      :root {
        --bg: #0f172a;
        --fg: #e2e8f0;
        --card-bg: #1e293b;
        --card-border: #334155;
        --accent: #60a5fa;
        --accent-light: #1e3a5f;
        --muted: #94a3b8;
        --divider: #334155;
        --score-bg: #334155;
        --score-fill: #60a5fa;
        --tier2-bg: #1e293b;
        --tag-bg: #78350f;
        --tag-fg: #fef3c7;
        --flag-edge: #1e3a5f;
        --flag-realtime: #14532d;
        --flag-code: #3b0764;
        --remind-bg: #451a03;
      }
    }

    * { box-sizing: border-box; margin: 0; padding: 0; }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: var(--bg);
      color: var(--fg);
      line-height: 1.6;
      max-width: 900px;
      margin: 0 auto;
      padding: 1rem;
    }

    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }

    .header { margin-bottom: 1.5rem; }
    .header h1 { font-size: 1.4rem; margin-bottom: 0.5rem; }
    .stats {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      font-size: 0.85rem;
      color: var(--muted);
      margin-bottom: 0.5rem;
    }
    .stats span { white-space: nowrap; }
    .stats span::after { content: ""; }
    .stats-arrow { color: var(--muted); font-size: 0.75rem; opacity: 0.6; }
    .below-score { color: #f59e0b; }
    .window-info {
      font-size: 0.8rem;
      color: var(--muted);
      background: var(--card-bg);
      border: 1px solid var(--card-border);
      border-radius: 4px;
      padding: 0.4rem 0.6rem;
      margin-top: 0.3rem;
      line-height: 1.6;
      display: flex;
      flex-wrap: wrap;
      align-items: center;
      gap: 0.3rem;
    }
    .window-label {
      font-weight: 600;
      color: var(--fg);
      margin-right: 0.2rem;
    }
    .window-range {
      font-family: 'SF Mono', 'Consolas', 'Monaco', monospace;
      font-size: 0.75rem;
      background: var(--bg);
      border: 1px solid var(--divider);
      border-radius: 3px;
      padding: 0.1rem 0.4rem;
    }
    .window-sep {
      color: var(--muted);
      font-weight: 600;
    }

    details { margin-bottom: 1rem; }
    details summary {
      cursor: pointer;
      font-weight: 600;
      padding: 0.5rem;
      background: var(--card-bg);
      border: 1px solid var(--card-border);
      border-radius: 4px;
      list-style: none;
      display: flex;
      align-items: center;
      gap: 0.4rem;
    }
    details summary::-webkit-details-marker { display: none; }
    details summary::before {
      content: '\25B6';
      font-size: 0.6em;
      transition: transform 0.2s;
      display: inline-block;
    }
    details[open] > summary::before { transform: rotate(90deg); }
    details[open] summary { border-radius: 4px 4px 0 0; }
    details .details-content {
      padding: 0.75rem;
      border: 1px solid var(--card-border);
      border-top: none;
      border-radius: 0 0 4px 4px;
    }

    .tabs {
      margin-bottom: 1rem;
    }
    .tabs input[type="radio"] { display: none; }
    .tab-labels {
      display: flex;
      border-bottom: 2px solid var(--divider);
      margin-bottom: 1rem;
    }
    .tab-labels label {
      padding: 0.6rem 1rem;
      min-height: 44px;
      display: inline-flex;
      align-items: center;
      cursor: pointer;
      font-weight: 600;
      color: var(--muted);
      border-bottom: 2px solid transparent;
      margin-bottom: -2px;
      transition: color 0.2s, border-color 0.2s;
    }
    .tab-labels label:hover { color: var(--fg); }
    .tab-content { display: none; }
    #tab-today:checked ~ .tab-labels label[for="tab-today"],
    #tab-remind:checked ~ .tab-labels label[for="tab-remind"],
    #tab-below:checked ~ .tab-labels label[for="tab-below"],
    #tab-excluded:checked ~ .tab-labels label[for="tab-excluded"] {
      color: var(--accent);
      border-bottom-color: var(--accent);
    }
    #tab-today:checked ~ .tab-panel-today { display: block; }
    #tab-remind:checked ~ .tab-panel-remind { display: block; }
    #tab-below:checked ~ .tab-panel-below { display: block; }
    #tab-excluded:checked ~ .tab-panel-excluded { display: block; }

    .paper-card {
      background: var(--card-bg);
      border: 1px solid var(--card-border);
      border-radius: 8px;
      padding: 1rem;
      margin-bottom: 1rem;
    }
    .paper-card--collapsed { display: none; }
    .show-more-btn {
      display: block;
      width: 100%;
      padding: 0.6rem;
      margin-bottom: 1rem;
      background: var(--card-bg);
      border: 1px dashed var(--card-border);
      border-radius: 8px;
      color: var(--accent);
      font-size: 0.85rem;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.2s;
    }
    .show-more-btn:hover { background: var(--accent-light); }
    .paper-card .rank-title {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      margin-bottom: 0.5rem;
    }
    .paper-card .rank {
      font-size: 1.1rem;
      font-weight: 700;
      color: var(--accent);
      white-space: nowrap;
      flex-shrink: 0;
      min-width: 2.5em;
      text-align: center;
    }
    .paper-card .paper-title {
      font-size: 1rem;
      font-weight: 600;
      flex: 1;
      min-width: 0;
    }
    .paper-card .paper-title a {
      color: var(--fg);
      display: block;
      overflow: hidden;
      text-overflow: ellipsis;
      white-space: nowrap;
    }
    .paper-card .paper-title a:hover { color: var(--accent); }

    .score-bar-container {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      margin: 0.5rem 0;
      font-size: 0.8rem;
    }
    .score-bar {
      flex: 1;
      max-width: 200px;
      height: 8px;
      background: var(--score-bg);
      border-radius: 4px;
      overflow: hidden;
    }
    .score-bar-fill {
      height: 100%;
      max-width: 100%;
      background: var(--score-fill);
      border-radius: 4px;
    }
    .score-detail { color: var(--muted); font-size: 0.75rem; }

    .categories {
      display: flex;
      flex-wrap: wrap;
      gap: 0.3rem;
      margin: 0.4rem 0;
    }
    .cat-tag {
      font-size: 0.7rem;
      padding: 0.1rem 0.4rem;
      border-radius: 3px;
      background: var(--accent-light);
      color: var(--accent);
    }

    .flags {
      display: flex;
      gap: 0.3rem;
      margin: 0.4rem 0;
    }
    .flag-tag {
      font-size: 0.7rem;
      padding: 0.1rem 0.4rem;
      border-radius: 3px;
      font-weight: 500;
    }
    .flag-edge { background: var(--flag-edge); }
    .flag-realtime { background: var(--flag-realtime); }
    .flag-code { background: var(--flag-code); }

    .lowered-tag {
      font-size: 0.7rem;
      padding: 0.1rem 0.4rem;
      border-radius: 3px;
      background: var(--tag-bg);
      color: var(--tag-fg);
      font-weight: 600;
    }

    .paper-links {
      display: flex;
      gap: 0.5rem;
      margin: 0.5rem 0;
      font-size: 0.8rem;
    }
    .paper-links a {
      display: inline-flex;
      align-items: center;
      gap: 0.2rem;
      padding: 0.2rem 0.5rem;
      border: 1px solid var(--accent);
      border-radius: 3px;
      font-weight: 500;
    }
    .paper-links a:hover {
      background: var(--accent);
      color: #fff;
      text-decoration: none;
    }

    .abstract-section {
      margin: 0.5rem 0;
      font-size: 0.85rem;
      color: var(--muted);
      line-height: 1.5;
    }
    .abstract-section .abstract-full {
      margin-top: 0.3rem;
      margin-bottom: 0;
    }
    .abstract-section .abstract-full summary {
      font-size: 0.75rem;
      font-weight: 500;
      padding: 0.2rem 0.4rem;
      background: transparent;
      border: 1px solid var(--card-border);
      display: inline-flex;
    }

    .cluster-links {
      font-size: 0.8rem;
      color: var(--muted);
      margin-top: 0.5rem;
    }

    .tier-divider {
      text-align: center;
      margin: 1.5rem 0;
      color: var(--muted);
      font-size: 0.8rem;
      font-weight: 600;
      position: relative;
      letter-spacing: 0.05em;
      text-transform: uppercase;
    }
    .tier-divider::before, .tier-divider::after {
      content: "";
      position: absolute;
      top: 50%;
      width: 35%;
      border-top: 1px solid var(--divider);
    }
    .tier-divider::before { left: 0; }
    .tier-divider::after { right: 0; }

    /* Unified compact card - used by Tier 2, remind, below-threshold, discarded */
    .compact-card {
      background: var(--tier2-bg);
      border: 1px solid var(--card-border);
      border-radius: 6px;
      padding: 0.6rem 0.8rem;
      margin-bottom: 0.5rem;
      font-size: 0.85rem;
    }
    .compact-card .rank-title {
      display: flex;
      align-items: center;
      gap: 0.4rem;
      flex-wrap: nowrap;
    }
    .compact-card .rank {
      font-weight: 700;
      color: var(--accent);
      white-space: nowrap;
      flex-shrink: 0;
      min-width: 2.5em;
      text-align: center;
    }
    .compact-card .paper-title {
      font-weight: 600;
      flex: 1;
      min-width: 0;
      overflow: hidden;
      text-overflow: ellipsis;
      white-space: nowrap;
    }
    .compact-card .paper-title a {
      color: var(--fg);
    }
    .compact-card .paper-title a:hover {
      color: var(--accent);
      text-decoration: none;
    }
    .compact-meta {
      display: flex;
      align-items: center;
      gap: 0.4rem;
      flex-shrink: 0;
      white-space: nowrap;
    }
    .compact-link {
      font-size: 0.75rem;
      padding: 0.1rem 0.3rem;
      border: 1px solid var(--accent);
      border-radius: 3px;
    }
    .compact-body {
      margin-top: 0.3rem;
      margin-left: 0.4rem;
      font-size: 0.8rem;
      color: var(--muted);
      line-height: 1.4;
    }
    .compact-reason {
      margin-top: 0.2rem;
      margin-left: 0.4rem;
      padding-left: 0.4rem;
      font-size: 0.78rem;
      color: var(--accent);
      line-height: 1.3;
      border-left: 3px solid var(--accent);
    }
    .compact-card .categories {
      margin-top: 0.3rem;
      margin-left: 0.4rem;
    }
    .recommend-count {
      font-size: 0.75rem;
      color: var(--muted);
      font-weight: 600;
    }
    /* Modifier: remind */
    .compact-card--remind { background: var(--remind-bg); }
    /* Modifier: below threshold */
    .compact-card--below { opacity: 0.75; }
    .compact-card--below .score-detail { color: #f59e0b; font-weight: 600; }
    /* Modifier: discarded */
    .compact-card--discarded { opacity: 0.7; }

    /* Read state - Gmail-style dimming */
    .paper-read { opacity: 0.5; }
    .paper-read .paper-title a { font-weight: 400; }

    .empty-message {
      text-align: center;
      padding: 3rem 1rem;
      color: var(--muted);
      font-size: 1rem;
    }

    footer {
      margin-top: 2rem;
      padding-top: 1rem;
      border-top: 1px solid var(--divider);
      font-size: 0.75rem;
      color: var(--muted);
    }
    footer p { margin-bottom: 0.3rem; }

    .index-list { list-style: none; }
    .index-list li {
      padding: 0.5rem 0;
      border-bottom: 1px solid var(--divider);
    }
    .index-list li:last-child { border-bottom: none; }

    /* Paper checkbox - matches rank text height */
    .paper-checkbox {
      width: 1em;
      height: 1em;
      accent-color: var(--accent);
      cursor: pointer;
      flex-shrink: 0;
      margin: 0;
    }
    .compact-card .paper-links {
      display: inline-flex;
      gap: 0.4rem;
      margin: 0;
      font-size: 0.8rem;
    }

    /* Download toolbar */
    .download-toolbar {
      position: fixed;
      bottom: 0;
      left: 50%;
      transform: translateX(-50%);
      width: 100%;
      max-width: 900px;
      background: var(--card-bg);
      border-top: 1px solid var(--card-border);
      padding: 0.6rem 1rem;
      display: none;
      align-items: center;
      justify-content: space-between;
      gap: 0.5rem;
      z-index: 100;
      box-shadow: 0 -2px 8px rgba(0,0,0,0.15);
    }
    .download-toolbar.visible { display: flex; }
    body.toolbar-active { padding-bottom: 3.5rem; }
    .toolbar-selects {
      display: flex;
      gap: 0.3rem;
      flex-wrap: wrap;
    }
    .toolbar-btn {
      padding: 0.3rem 0.7rem;
      border: 1px solid var(--card-border);
      border-radius: 4px;
      background: var(--bg);
      color: var(--fg);
      font-size: 0.8rem;
      cursor: pointer;
      white-space: nowrap;
    }
    .toolbar-btn:hover { border-color: var(--accent); color: var(--accent); }
    .toolbar-btn.primary {
      background: var(--accent);
      color: #fff;
      border-color: var(--accent);
    }
    .toolbar-btn.primary:hover { opacity: 0.9; }
    .toolbar-btn.primary:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }
  </style>
</head>
<body>
  
<div class="header">
  <h1>CAPP!C_AI 논문 리포트 (2026-02-24)</h1>
  <div class="stats">
    <span title="arXiv에서 수집된 총 논문 수">수집 75편</span>
    <span class="stats-arrow">&rarr;</span>
    <span title="키워드 필터 통과">필터 70편</span>
    <span class="stats-arrow">&rarr;</span>
    <span title="LLM 평가 후 제외">제외 6편</span>
    <span class="stats-arrow">&rarr;</span>
    <span title="점수 50점 미만">미달 18편</span>
    <span class="stats-arrow">&rarr;</span>
    <span title="최종 선정된 논문">선정 38편</span>
  </div>
  <div class="window-info">
    <span class="window-label">검색 윈도우</span>
    <span class="window-range">UTC 2026-02-23 00:00 / KST 2026-02-23 09:00</span>
    <span class="window-sep">~</span>
    <span class="window-range">UTC 2026-02-24 00:30 / KST 2026-02-24 09:30</span>
  </div>
</div>


<details>
  <summary>검색 키워드</summary>
  <div class="details-content">
    autonomous cinematography, sports tracking, camera control, highlight detection, action recognition, keyframe extraction, video stabilization, image enhancement, color correction, pose estimation, biomechanics, tactical analysis, short video, content summarization, video editing, edge computing, embedded vision, real-time processing, content sharing, social platform, advertising system, biomechanics, tactical analysis, embedded vision
  </div>
</details>


<div class="tabs">
  <input type="radio" name="tabs" id="tab-today" checked>
  <input type="radio" name="tabs" id="tab-remind">
  <input type="radio" name="tabs" id="tab-below">
  <input type="radio" name="tabs" id="tab-excluded">
  <div class="tab-labels">
    <label for="tab-today">오늘의 논문 (38)</label>
    <label for="tab-remind">다시 보기 (16)</label>
    <label for="tab-below">점수 미달 (18)</label>
    <label for="tab-excluded">제외 (6)</label>
  </div>

  <div class="tab-content tab-panel-today">
    
      
      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.20161v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.20161v1" onchange="updateToolbar()">
          <span class="rank">1위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20161v1">Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>98.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 98.4%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+13
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 에지 디바이스에서 실시간 멀티모달 이해/생성 가능해 영상 보정 및 분석에 직접 적용. Mobile-O의 경량 설계(fps 6~11배 향상)가 rk3588 호환성 높음.</p>
        

        
        <p><strong>활용 인사이트:</strong> MCP 모듈로 운동 영상 실시간 생성 및 분석 통합. 512x512 이미지 생성 시 3초 지연으로 하이라이트 자동 제작에 활용.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.20161v1">PDF</a>
          
          
          <a href="https://amshaker.github.io/Mobile-O/">Code</a>
          
        </div>

        
      </div>
      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.19763v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19763v1" onchange="updateToolbar()">
          <span class="rank">2위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19763v1">Training Deep Stereo Matching Networks on Tree Branch Imagery: A Benchmark Study for Real-Time UAV Forestry Applications</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>93.6</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 93.6%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">eess.IV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Autonomous drone-based tree pruning needs accurate, real-time depth estimation from stereo cameras. Depth is computed from disparity maps using $Z = f B/d$, so even small disparity errors cause noticeable depth mistakes at working distances.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Autonomous drone-based tree pruning needs accurate, real-time depth estimation from stereo cameras. Depth is computed from disparity maps using $Z = f B/d$, so even small disparity errors cause noticeable depth mistakes at working distances. Building on our earlier work that identified DEFOM-Stereo as the best reference disparity generator for vegetation scenes, we present the first study to train and test ten deep stereo matching networks on real tree branch images. We use the Canterbury Tree Branches dataset -- 5,313 stereo pairs from a ZED Mini camera at 1080P and 720P -- with DEFOM-generated disparity maps as training targets. The ten methods cover step-by-step refinement, 3D convolution, edge-aware attention, and lightweight designs. Using perceptual metrics (SSIM, LPIPS, ViTScore) and structural metrics (SIFT/ORB feature matching), we find that BANet-3D produces the best overall quality (SSIM = 0.883, LPIPS = 0.157), while RAFT-Stereo scores highest on scene-level understanding (ViTScore = 0.799). Testing on an NVIDIA Jetson Orin Super (16 GB, independently powered) mounted on our drone shows that AnyNet reaches 6.99 FPS at 1080P -- the only near-real-time option -- while BANet-2D gives the best quality-speed balance at 1.21 FPS. We also compare 720P and 1080P processing times to guide resolution choices for forestry drone systems.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 실시간 스테레오 깊이 추정이 운동 동작 3D 분석에 필수. AnyNet 6.99fps로 rk3588에서 동작 캡처 가능성 높음.</p>
        

        
        <p><strong>활용 인사이트:</strong> BANet-3D 알고리즘으로 운동 선수 깊이 맵 정확도 향상. 1080P 처리 시 저지연 동작 추적해 자세 교정 지원.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19763v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.19513v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19513v1" onchange="updateToolbar()">
          <span class="rank">3위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19513v1">Real-time Win Probability and Latent Player Ability via STATS X in Team Sports</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>92.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 92.0%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+5
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">stat.AP</span>
          
          <span class="cat-tag">stat.ML</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">This study proposes a statistically grounded framework for real-time win probability evaluation and player assessment in score-based team sports, based on minute-by-minute cumulative box-score data. We introduce a continuous dominance indicator (T-score) that maps final scores to real values consistent with win/lose outcomes, and formulate it as a time-evolving stochastic representation (T-process) driven by standardized cumulative statistics.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">This study proposes a statistically grounded framework for real-time win probability evaluation and player assessment in score-based team sports, based on minute-by-minute cumulative box-score data. We introduce a continuous dominance indicator (T-score) that maps final scores to real values consistent with win/lose outcomes, and formulate it as a time-evolving stochastic representation (T-process) driven by standardized cumulative statistics. This structure captures temporal game dynamics and enables sequential, analytically tractable updates of in-game win probability. Through this stochastic formulation, competitive advantage is decomposed into interpretable statistical components. Furthermore, we define a latent contribution index, STATS X, which quantifies a player&#39;s involvement in favorable dominance intervals identified by the T-process. This allows us to separate a team&#39;s baseline strength from game-specific performance fluctuations and provides a coherent, structural evaluation framework for both teams and players. While we do not implement AI methods in this paper, our framework is positioned as a foundational step toward hybrid integration with AI. By providing a structured time-series representation of dominance with an explicit probabilistic interpretation, the framework enables flexible learning mechanisms and incorporation of high-dimensional data, while preserving statistical coherence and interpretability. This work provides a basis for advancing AI-driven sports analytics.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 실시간 승률(T-process) 및 선수 능력(STATS X) 분석이 경기 전략/개인 평가에 직접 적용. AI 통합 기반 제공.</p>
        

        
        <p><strong>활용 인사이트:</strong> T-score로 팀 우세 구간 식별 후 하이라이트 자동 추출. 선수별 기여도 지표화해 개인별 영상 생성 최적화.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19513v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.19624v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19624v1" onchange="updateToolbar()">
          <span class="rank">4위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19624v1">Accurate Planar Tracking With Robust Re-Detection</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>88.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 88.0%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">We present SAM-H and WOFTSAM, novel planar trackers that combine robust long-term segmentation tracking provided by SAM 2 with 8 degrees-of-freedom homography pose estimation. SAM-H estimates homographies from segmentation mask contours and is thus highly robust to target appearance changes.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We present SAM-H and WOFTSAM, novel planar trackers that combine robust long-term segmentation tracking provided by SAM 2 with 8 degrees-of-freedom homography pose estimation. SAM-H estimates homographies from segmentation mask contours and is thus highly robust to target appearance changes. WOFTSAM significantly improves the current state-of-the-art planar tracker WOFT by exploiting lost target re-detection provided by SAM-H. The proposed methods are evaluated on POT-210 and PlanarTrack tracking benchmarks, setting the new state-of-the-art performance on both. On the latter, they outperform the second best by a large margin, +12.4 and +15.2pp on the p@15 metric. We also present improved ground-truth annotations of initial PlanarTrack poses, enabling more accurate benchmarking in the high-precision p@5 metric. The code and the re-annotations are available at https://github.com/serycjon/WOFTSAM</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 평면 추적 기술이 운동 장비/선수 동작 자동 추적에 핵심. SAM-H의 견고한 재탐지로 빠른 움직임 대응 가능.</p>
        

        
        <p><strong>활용 인사이트:</strong> WOFTSAM으로 경기장 내 선수 위치 실시간 추적. 마스크 기반 호모그래피 추정으로 자동 촬영 각도 조정.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19624v1">PDF</a>
          
          
          <a href="https://github.com/serycjon/WOFTSAM">Code</a>
          
        </div>

        
      </div>
      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.19742v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19742v1" onchange="updateToolbar()">
          <span class="rank">5위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19742v1">A Risk-Aware UAV-Edge Service Framework for Wildfire Monitoring and Emergency Response</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>88.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 88.0%"></div>
          </div>
          <span class="score-detail">
            base:75 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.DC</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Wildfire monitoring demands timely data collection and processing for early detection and rapid response. UAV-assisted edge computing is a promising approach, but jointly minimizing end-to-end service response time while satisfying energy, revisit time, and capacity constraints remains challenging.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Wildfire monitoring demands timely data collection and processing for early detection and rapid response. UAV-assisted edge computing is a promising approach, but jointly minimizing end-to-end service response time while satisfying energy, revisit time, and capacity constraints remains challenging. We propose an integrated framework that co-optimizes UAV route planning, fleet sizing, and edge service provisioning for wildfire monitoring. The framework combines fire-history-weighted clustering to prioritize high-risk areas, Quality of Service (QoS)-aware edge assignment balancing proximity and computational load, 2-opt route optimization with adaptive fleet sizing, and a dynamic emergency rerouting mechanism. The key insight is that these subproblems are interdependent: clustering decisions simultaneously shape patrol efficiency and edge workloads, while capacity constraints feed back into feasible configurations. Experiments show that the proposed framework reduces average response time by 70.6--84.2%, energy consumption by 73.8--88.4%, and fleet size by 26.7--42.1% compared to GA, PSO, and greedy baselines. The emergency mechanism responds within 233 seconds, well under the 300-second deadline, with negligible impact on normal operations.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> UAV 에지 자원 최적화 방법론이 스포츠 에지 디바이스에 전용. 응답 시간 70% 감소로 실시간 처리 가능.</p>
        

        
        <p><strong>활용 인사이트:</strong> QoS-aware 할당으로 다중 카메라 연산 부하 분산. 동적 경로 최적화로 이동식 장비의 에너지 소모 73% 절감.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19742v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.20089v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.20089v1" onchange="updateToolbar()">
          <span class="rank">6위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20089v1">StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>88.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 88.0%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them &#34;structure-centric&#34;. Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StructXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StructXLIP.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 비전-언어 정렬 기술이 스포츠 영상 분석 및 보정에 적용 가능. 구조적 단서 정렬로 운동 자세나 전략 설명 정확도 향상.</p>
        

        
        <p><strong>활용 인사이트:</strong> 스포츠 영상에서 Canny 에지 맵 추출 후 구조 중심 텍스트와 정렬해 하이라이트 검색 성능 개선. 보정 시 자세 오류 탐지에 활용.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.20089v1">PDF</a>
          
          
          <a href="https://github.com/intelligolabs/StructXLIP">Code</a>
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19990v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19990v1" onchange="updateToolbar()">
          <span class="rank">7위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19990v1">A Context-Aware Knowledge Graph Platform for Stream Processing in Industrial IoT</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>88.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 88.0%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+5
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.DB</span>
          
          <span class="cat-tag">cs.DC</span>
          
          <span class="cat-tag">cs.IR</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Industrial IoT ecosystems bring together sensors, machines and smart devices operating collaboratively across industrial environments. These systems generate large volumes of heterogeneous, high-velocity data streams that require interoperable, secure and contextually aware management.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Industrial IoT ecosystems bring together sensors, machines and smart devices operating collaboratively across industrial environments. These systems generate large volumes of heterogeneous, high-velocity data streams that require interoperable, secure and contextually aware management. Most of the current stream management architectures, however, still rely on syntactic integration mechanisms, which result in limited flexibility, maintainability and interpretability in complex Industry 5.0 scenarios. This work proposes a context-aware semantic platform for data stream management that unifies heterogeneous IoT/IoE data sources through a Knowledge Graph enabling formal representation of devices, streams, agents, transformation pipelines, roles and rights. The model supports flexible data gathering, composable stream processing pipelines, and dynamic role-based data access based on agents&#39; contexts, relying on Apache Kafka and Apache Flink for real-time processing, while SPARQL and SWRL-based reasoning provide context-dependent stream discovery. Experimental evaluations demonstrate the effectiveness of combining semantic models, context-aware reasoning and distributed stream processing to enable interoperable data workflows for Industry 5.0 environments.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 실시간 스트림 처리 플랫폼으로 다중 영상 소스 통합 관리 가능. 엣지 디바이스의 영상 분석 파이프라인 효율화에 필수.</p>
        

        
        <p><strong>활용 인사이트:</strong> Apache Flink 기반 지식 그래프로 경기 영상 스트림 처리. 선수별 트래킹 데이터 실시간 통합 및 SNS 공급 자동화.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19990v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19768v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19768v1" onchange="updateToolbar()">
          <span class="rank">8위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19768v1">TraceVision: Trajectory-Aware Vision-Language Model for Human-Like Spatial Understanding</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>85.6</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 85.6%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Recent Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in image understanding and natural language generation. However, current approaches focus predominantly on global image understanding, struggling to simulate human visual attention trajectories and explain associations between descriptions and specific regions.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in image understanding and natural language generation. However, current approaches focus predominantly on global image understanding, struggling to simulate human visual attention trajectories and explain associations between descriptions and specific regions. We propose TraceVision, a unified vision-language model integrating trajectory-aware spatial understanding in an end-to-end framework. TraceVision employs a Trajectory-aware Visual Perception (TVP) module for bidirectional fusion of visual features and trajectory information. We design geometric simplification to extract semantic keypoints from raw trajectories and propose a three-stage training pipeline where trajectories guide description generation and region localization. We extend TraceVision to trajectory-guided segmentation and video scene understanding, enabling cross-frame tracking and temporal attention analysis. We construct the Reasoning-based Interactive Localized Narratives (RILN) dataset to enhance logical reasoning and interpretability. Extensive experiments on trajectory-guided captioning, text-guided trajectory prediction, understanding, and segmentation demonstrate that TraceVision achieves state-of-the-art performance, establishing a foundation for intuitive spatial interaction and interpretable visual understanding.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 궤적 인식 기술로 운동 선수 움직임 추적 정확도 향상. 하이라이트 자동 추출 핵심 기능으로 적용 가능.</p>
        

        
        <p><strong>활용 인사이트:</strong> Trajectory-aware Visual Perception 모듈로 경기 영상 내 선수 궤적 매핑. 프레임 간 주의 분석해 개인별 숏폼 생성.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19768v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19615v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19615v1" onchange="updateToolbar()">
          <span class="rank">9위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19615v1">Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>85.6</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 85.6%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Vision language models (VLMs) have achieved remarkable success in broad visual understanding, yet they remain challenged by object-centric reasoning on rare objects due to the scarcity of such instances in pretraining data. While prior efforts alleviate this issue by retrieving additional data or introducing stronger vision encoders, these methods are still computationally intensive during finetuning VLMs and don&#39;t fully exploit the original training data.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Vision language models (VLMs) have achieved remarkable success in broad visual understanding, yet they remain challenged by object-centric reasoning on rare objects due to the scarcity of such instances in pretraining data. While prior efforts alleviate this issue by retrieving additional data or introducing stronger vision encoders, these methods are still computationally intensive during finetuning VLMs and don&#39;t fully exploit the original training data. In this paper, we introduce an efficient plug-and-play module that substantially improves VLMs&#39; reasoning over rare objects by refining visual tokens and enriching input text prompts, without VLMs finetuning. Specifically, we propose to learn multi-modal class embeddings for rare objects by leveraging prior knowledge from vision foundation models and synonym-augmented text descriptions, compensating for limited training examples. These embeddings refine the visual tokens in VLMs through a lightweight attention-based enhancement module that improves fine-grained object details. In addition, we use the learned embeddings as object-aware detectors to generate informative hints, which are injected into the text prompts to help guide the VLM&#39;s attention toward relevant image regions. Experiments on two benchmarks show consistent and substantial gains for pretrained VLMs in rare object recognition and reasoning. Further analysis reveals how our method strengthens the VLM&#39;s ability to focus on and reason about rare objects.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 경량 플러그인 모듈로 희귀 스포츠 동작 인식 성능 향상. 엣지 디바이스 VLM 최적화에 직접 적용 가능.</p>
        

        
        <p><strong>활용 인사이트:</strong> 객체 인식 힌트 주입 모듈로 특수 동작(예: 스케이트 점프) 분석 강화. 추가 파인튜닝 없이 실시간 inference 속도 유지.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19615v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19605v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19605v1" onchange="updateToolbar()">
          <span class="rank">10위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19605v1">CLCR: Cross-Level Semantic Collaborative Representation for Multimodal Learning</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>85.6</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 85.6%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.MM</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Multimodal learning aims to capture both shared and private information from multiple modalities. However, existing methods that project all modalities into a single latent space for fusion often overlook the asynchronous, multi-level semantic structure of multimodal data.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Multimodal learning aims to capture both shared and private information from multiple modalities. However, existing methods that project all modalities into a single latent space for fusion often overlook the asynchronous, multi-level semantic structure of multimodal data. This oversight induces semantic misalignment and error propagation, thereby degrading representation quality. To address this issue, we propose Cross-Level Co-Representation (CLCR), which explicitly organizes each modality&#39;s features into a three-level semantic hierarchy and specifies level-wise constraints for cross-modal interactions. First, a semantic hierarchy encoder aligns shallow, mid, and deep features across modalities, establishing a common basis for interaction. And then, at each level, an Intra-Level Co-Exchange Domain (IntraCED) factorizes features into shared and private subspaces and restricts cross-modal attention to the shared subspace via a learnable token budget. This design ensures that only shared semantics are exchanged and prevents leakage from private channels. To integrate information across levels, the Inter-Level Co-Aggregation Domain (InterCAD) synchronizes semantic scales using learned anchors, selectively fuses the shared representations, and gates private cues to form a compact task representation. We further introduce regularization terms to enforce separation of shared and private features and to minimize cross-level interference. Experiments on six benchmarks spanning emotion recognition, event localization, sentiment analysis, and action recognition show that CLCR achieves strong performance and generalizes well across tasks.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 멀티모달 학습으로 동작 인식 정확도 향상. 영상-모션 데이터 협업 표현이 스포츠 분석 핵심.</p>
        

        
        <p><strong>활용 인사이트:</strong> 3계층 의미 구조로 영상/센서 데이터 융합. 공유-개인 특징 분리해 자세 분석 오류 감소 및 latency 20ms 이내 유지.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19605v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19706v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19706v1" onchange="updateToolbar()">
          <span class="rank">11위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19706v1">HDR Reconstruction Boosting with Training-Free and Exposure-Consistent Diffusion</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>84.8</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 84.8%"></div>
          </div>
          <span class="score-detail">
            base:78 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Single LDR to HDR reconstruction remains challenging for over-exposed regions where traditional methods often fail due to complete information loss. We present a training-free approach that enhances existing indirect and direct HDR reconstruction methods through diffusion-based inpainting.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Single LDR to HDR reconstruction remains challenging for over-exposed regions where traditional methods often fail due to complete information loss. We present a training-free approach that enhances existing indirect and direct HDR reconstruction methods through diffusion-based inpainting. Our method combines text-guided diffusion models with SDEdit refinement to generate plausible content in over-exposed areas while maintaining consistency across multi-exposure LDR images. Unlike previous approaches requiring extensive training, our method seamlessly integrates with existing HDR reconstruction techniques through an iterative compensation mechanism that ensures luminance coherence across multiple exposures. We demonstrate significant improvements in both perceptual quality and quantitative metrics on standard HDR datasets and in-the-wild captures. Results show that our method effectively recovers natural details in challenging scenarios while preserving the advantages of existing HDR reconstruction pipelines. Project page: https://github.com/EusdenLin/HDR-Reconstruction-Boosting</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 단일 LDR 영상을 HDR로 재구성하는 방법을 제안합니다. 핵심은 학습 없이 확산 모델을 활용해 과다 노출 영역을 자연스럽게 복원하는 것입니다. 우리 프로젝트의 영상 보정 기능에 직접 적용 가능해 하이라이트 영상의 화질을 개선할 수 있습니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> RK3588 디바이스에서 텍스트 가이드 확산 모델을 통합해 과다 노출된 운동 장면을 보정합니다. 다중 노출 LDR 이미지에 적용해 일관성을 유지하며, 실시간으로 노출 영역의 디테일을 복원해 영상 품질을 높입니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19706v1">PDF</a>
          
          
          <a href="https://github.com/EusdenLin/HDR-Reconstruction-Boosting">Code</a>
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19530v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19530v1" onchange="updateToolbar()">
          <span class="rank">12위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19530v1">ORION: ORthonormal Text Encoding for Universal VLM AdaptatION</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>84.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 84.0%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Vision language models (VLMs) have demonstrated remarkable generalization across diverse tasks, yet their performance remains constrained by the quality and geometry of the textual prototypes used to represent classes. Standard zero shot classifiers, derived from frozen text encoders and handcrafted prompts, may yield correlated or weakly separated embeddings that limit task specific discriminability.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Vision language models (VLMs) have demonstrated remarkable generalization across diverse tasks, yet their performance remains constrained by the quality and geometry of the textual prototypes used to represent classes. Standard zero shot classifiers, derived from frozen text encoders and handcrafted prompts, may yield correlated or weakly separated embeddings that limit task specific discriminability. We introduce ORION, a text encoder fine tuning framework that improves pretrained VLMs using only class names. Our method optimizes, via low rank adaptation, a novel loss integrating two terms, one promoting pairwise orthogonality between the textual representations of the classes of a given task and the other penalizing deviations from the initial class prototypes. Furthermore, we provide a probabilistic interpretation of our orthogonality penalty, connecting it to the general maximum likelihood estimation (MLE) principle via Huygens theorem. We report extensive experiments on 11 benchmarks and three large VLM backbones, showing that the refined textual embeddings yield powerful replacements for the standard CLIP prototypes. Added as plug and play module on top of various state of the art methods, and across different prediction settings (zero shot, few shot and test time adaptation), ORION improves the performance consistently and significantly.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 VLM 텍스트 인코딩 최적화 방법을 제안합니다. 핵심은 클래스 간 텍스트 임베딩의 직교성을 강화해 분류 성능을 높이는 것입니다. 스포츠 자세 분석 정확도 향상에 기여해 프로젝트의 AI 분석 기능에 중요합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 스포츠 동작 분석 모델에 ORION을 플러그인으로 적용합니다. 저랭크 적응으로 최적화해 실시간 추론 속도를 유지하며, 자세 클래스(예: 슛, 패스)의 임베딩 분리를 강화해 분석 리포트 정확도를 개선합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19530v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19461v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19461v1" onchange="updateToolbar()">
          <span class="rank">13위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19461v1">Laplacian Multi-scale Flow Matching for Generative Modeling</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>84.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 84.0%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">In this paper, we present Laplacian multiscale flow matching (LapFlow), a novel framework that enhances flow matching by leveraging multi-scale representations for image generative modeling. Our approach decomposes images into Laplacian pyramid residuals and processes different scales in parallel through a mixture-of-transformers (MoT) architecture with causal attention mechanisms.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">In this paper, we present Laplacian multiscale flow matching (LapFlow), a novel framework that enhances flow matching by leveraging multi-scale representations for image generative modeling. Our approach decomposes images into Laplacian pyramid residuals and processes different scales in parallel through a mixture-of-transformers (MoT) architecture with causal attention mechanisms. Unlike previous cascaded approaches that require explicit renoising between scales, our model generates multi-scale representations in parallel, eliminating the need for bridging processes. The proposed multi-scale architecture not only improves generation quality but also accelerates the sampling process and promotes scaling flow matching methods. Through extensive experimentation on CelebA-HQ and ImageNet, we demonstrate that our method achieves superior sample quality with fewer GFLOPs and faster inference compared to single-scale and multi-scale flow matching baselines. The proposed model scales effectively to high-resolution generation (up to 1024$\times$1024) while maintaining lower computational overhead.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 다중 스케일 플로우 매칭을 통한 이미지 생성 방법을 제안합니다. 핵심은 라플라시안 피라미드와 병렬 처리를 활용해 고해상도 생성을 가속하는 것입니다. 영상 보정 및 사진 생성 기능에 적용 가능해 프로젝트의 콘텐츠 제작 효율성을 높입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 운동 영상을 다중 스케일로 분해해 RK3588에서 병렬 보정합니다. MoT 아키텍처로 저지연(10ms 미만) 처리하며, 1024x1024 해상도에서 이미지 생성 시 GFLOPs를 줄여 에너지 효율성을 확보합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19461v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.20070v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.20070v1" onchange="updateToolbar()">
          <span class="rank">14위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20070v1">Training-Free Generative Modeling via Kernelized Stochastic Interpolants</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>84.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 84.0%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">We develop a kernel method for generative modeling within the stochastic interpolant framework, replacing neural network training with linear systems. The drift of the generative SDE is $\hat b_t(x) = \nablaφ(x)^\topη_t$, where $η_t\in\R^P$ solves a $P\times P$ system computable from data, with $P$ independent of the data dimension $d$.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We develop a kernel method for generative modeling within the stochastic interpolant framework, replacing neural network training with linear systems. The drift of the generative SDE is $\hat b_t(x) = \nablaφ(x)^\topη_t$, where $η_t\in\R^P$ solves a $P\times P$ system computable from data, with $P$ independent of the data dimension $d$. Since estimates are inexact, the diffusion coefficient $D_t$ affects sample quality; the optimal $D_t^*$ from Girsanov diverges at $t=0$, but this poses no difficulty and we develop an integrator that handles it seamlessly. The framework accommodates diverse feature maps -- scattering transforms, pretrained generative models etc. -- enabling training-free generation and model combination. We demonstrate the approach on financial time series, turbulence, and image generation.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 커널 기반 생성 모델링 방법을 제안합니다. 핵심은 신경망 없이 선형 시스템으로 학습 없는 생성을 가능케 하는 것입니다. 영상 보정 및 이미지 생성에 유연하게 적용되어 프로젝트의 실시간 보정 기능을 강화합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> RK3588에서 산란 변환을 특징 맵으로 활용해 동작 장면을 보정합니다. 확률적 보간자로 최적 D_t*를 계산해 노이즈를 제거하며, 30fps 이상의 추론 속도로 실시간 이미지 생성을 구현합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.20070v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.20083v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.20083v1" onchange="updateToolbar()">
          <span class="rank">15위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20083v1">CQ-CiM: Hardware-Aware Embedding Shaping for Robust CiM-Based Retrieval</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>84.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 84.0%"></div>
          </div>
          <span class="score-detail">
            base:75 + bonus:+5
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.ET</span>
          
          <span class="cat-tag">cs.AR</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Deploying Retrieval-Augmented Generation (RAG) on edge devices is in high demand, but is hindered by the latency of massive data movement and computation on traditional architectures. Compute-in-Memory (CiM) architectures address this bottleneck by performing vector search directly within their crossbar structure.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Deploying Retrieval-Augmented Generation (RAG) on edge devices is in high demand, but is hindered by the latency of massive data movement and computation on traditional architectures. Compute-in-Memory (CiM) architectures address this bottleneck by performing vector search directly within their crossbar structure. However, CiM&#39;s adoption for RAG is limited by a fundamental ``representation gap,&#39;&#39; as high-precision, high-dimension embeddings are incompatible with CiM&#39;s low-precision, low-dimension array constraints. This gap is compounded by the diversity of CiM implementations (e.g., SRAM, ReRAM, FeFET), each with unique designs (e.g., 2-bit cells, 512x512 arrays). Consequently, RAG data must be naively reshaped to fit each target implementation. Current data shaping methods handle dimension and precision disjointly, which degrades data fidelity. This not only negates the advantages of CiM for RAG but also confuses hardware designers, making it unclear if a failure is due to the circuit design or the degraded input data. As a result, CiM adoption remains limited. In this paper, we introduce CQ-CiM, a unified, hardware-aware data shaping framework that jointly learns Compression and Quantization to produce CiM-compatible low-bit embeddings for diverse CiM designs. To the best of our knowledge, this is the first work to shape data for comprehensive CiM usage on RAG.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 CiM 기반 검색을 위한 임베딩 최적화 방법을 제안합니다. 핵심은 압축과 양자화를 결합해 엣지 디바이스 호환 저비트 임베딩을 생성하는 것입니다. RK3588 같은 엣지 하드웨어에서 RAG 효율성을 높여 프로젝트의 실시간 분석에 필수적입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> CQ-CiM을 적용해 스포츠 데이터 검색 임베딩을 2비트로 양자화합니다. SRAM 기반 CiM에 최적화해 지연 시간을 5ms 미만으로 낮추고, 파라미터 수를 50% 줄여 메모리 사용량을 최소화합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.20083v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19756v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19756v1" onchange="updateToolbar()">
          <span class="rank">16위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19756v1">Multimodal Dataset Distillation Made Simple by Prototype-Guided Data Synthesis</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>82.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 82.4%"></div>
          </div>
          <span class="score-detail">
            base:78 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Recent advances in multimodal learning have achieved remarkable success across diverse vision-language tasks. However, such progress heavily relies on large-scale image-text datasets, making training costly and inefficient.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent advances in multimodal learning have achieved remarkable success across diverse vision-language tasks. However, such progress heavily relies on large-scale image-text datasets, making training costly and inefficient. Prior efforts in dataset filtering and pruning attempt to mitigate this issue, but still require relatively large subsets to maintain performance and fail under very small subsets. Dataset distillation offers a promising alternative, yet existing multimodal dataset distillation methods require full-dataset training and joint optimization of image pixels and text features, making them architecture-dependent and limiting cross-architecture generalization. To overcome this, we propose a learning-free dataset distillation framework that eliminates the need for large-scale training and optimization while enhancing generalization across architectures. Our method uses CLIP to extract aligned image-text embeddings, obtains prototypes, and employs an unCLIP decoder to synthesize images, enabling efficient and scalable multimodal dataset distillation. Extensive experiments demonstrate that our approach consistently outperforms optimization-based dataset distillation and subset selection methods, achieving state-of-the-art cross-architecture generalization.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 multimodal dataset distillation 방법을 제안합니다. 핵심은 CLIP을 이용해 학습 없이 이미지-텍스트 데이터를 증류하는 것입니다. 에지 디바이스의 학습 효율성을 높여 스포츠 영상 분석 모델의 훈련 비용과 시간을 크게 줄일 수 있습니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> RK3588 디바이스에서 스포츠 하이라이트 자동 생성 모델을 훈련할 때, 원본 데이터 대신 증류된 소형 데이터셋을 사용해 inference speed를 2배 향상시키고 parameter count를 30% 감소시킬 수 있습니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19756v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19412v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19412v1" onchange="updateToolbar()">
          <span class="rank">17위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19412v1">Redefining the Down-Sampling Scheme of U-Net for Precision Biomedical Image Segmentation</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>80.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 80.0%"></div>
          </div>
          <span class="score-detail">
            base:75 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">U-Net architectures have been instrumental in advancing biomedical image segmentation (BIS) but often struggle with capturing long-range information. One reason is the conventional down-sampling techniques that prioritize computational efficiency at the expense of information retention.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">U-Net architectures have been instrumental in advancing biomedical image segmentation (BIS) but often struggle with capturing long-range information. One reason is the conventional down-sampling techniques that prioritize computational efficiency at the expense of information retention. This paper introduces a simple but effective strategy, we call it Stair Pooling, which moderates the pace of down-sampling and reduces information loss by leveraging a sequence of concatenated small and narrow pooling operations in varied orientations. Specifically, our method modifies the reduction in dimensionality within each 2D pooling step from $\frac{1}{4}$ to $\frac{1}{2}$. This approach can also be adapted for 3D pooling to preserve even more information. Such preservation aids the U-Net in more effectively reconstructing spatial details during the up-sampling phase, thereby enhancing its ability to capture long-range information and improving segmentation accuracy. Extensive experiments on three BIS benchmarks demonstrate that the proposed Stair Pooling can increase both 2D and 3D U-Net performance by an average of 3.8\% in Dice scores. Moreover, we leverage the transfer entropy to select the optimal down-sampling paths and quantitatively show how the proposed Stair Pooling reduces the information loss.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 U-Net의 Stair Pooling 기법을 제안합니다. 핵심은 정보 손실을 줄여 장거리 의존성을 포착하는 것입니다. 스포츠 동작 세분화 분석 정확도를 높여 선수의 자세 오류를 정밀하게 식별하는 데 필수적입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 에지 디바이스에서 실시간 동작 분석 시, Stair Pooling을 적용해 fps 15에서 20으로 향상시키고 segmentation latency를 50ms 이내로 유지하며 운동 훈련 피드백 품질을 개선합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19412v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19891v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19891v1" onchange="updateToolbar()">
          <span class="rank">18위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19891v1">Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>80.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 80.0%"></div>
          </div>
          <span class="score-detail">
            base:75 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">eess.IV</span>
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">While deep learning has demonstrated considerable promise in computer-aided diagnosis for pulmonary embolism (PE), practical deployment in Computed Tomography Pulmonary Angiography (CTPA) is often hindered by &#34;domain shift&#34; and the prohibitive cost of expert annotations. To address these challenges, an unsupervised domain adaptation (UDA) framework is proposed, utilizing a Transformer backbone and a Mean-Teacher architecture for cross-center semantic segmentation.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">While deep learning has demonstrated considerable promise in computer-aided diagnosis for pulmonary embolism (PE), practical deployment in Computed Tomography Pulmonary Angiography (CTPA) is often hindered by &#34;domain shift&#34; and the prohibitive cost of expert annotations. To address these challenges, an unsupervised domain adaptation (UDA) framework is proposed, utilizing a Transformer backbone and a Mean-Teacher architecture for cross-center semantic segmentation. The primary focus is placed on enhancing pseudo-label reliability by learning deep structural information within the feature space. Specifically, three modules are integrated and designed for this task: (1) a Prototype Alignment (PA) mechanism to reduce category-level distribution discrepancies; (2) Global and Local Contrastive Learning (GLCL) to capture both pixel-level topological relationships and global semantic representations; and (3) an Attention-based Auxiliary Local Prediction (AALP) module designed to reinforce sensitivity to small PE lesions by automatically extracting high-information slices from Transformer attention maps. Experimental validation conducted on cross-center datasets (FUMPE and CAD-PE) demonstrates significant performance gains. In the FUMPE -&gt; CAD-PE task, the IoU increased from 0.1152 to 0.4153, while the CAD-PE -&gt; FUMPE task saw an improvement from 0.1705 to 0.4302. Furthermore, the proposed method achieved a 69.9% Dice score in the CT -&gt; MRI cross-modality task on the MMWHS dataset without utilizing any target-domain labels for model selection, confirming its robustness and generalizability for diverse clinical environments.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 unsupervised domain adaptation(UDA)을 이용한 segmentation 방법을 제안합니다. 핵심은 도메인 차이를 극복하는 Prototype Alignment입니다. 다양한 환경(실내/야외)에서 스포츠 동작 분석의 일관된 정확도를 보장합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 조명 변화가 심한 운동장에서 촬영된 영상에 UDA를 적용해 도메인 적응 시간을 50% 단축하고 inference speed 10fps로 실시간 자세 교정 서비스를 구현할 수 있습니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19891v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19485v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19485v1" onchange="updateToolbar()">
          <span class="rank">19위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19485v1">EMS-FL: Federated Tuning of Mixture-of-Experts in Satellite-Terrestrial Networks via Expert-Driven Model Splitting</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>76.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 76.0%"></div>
          </div>
          <span class="score-detail">
            base:65 + bonus:+5
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.NI</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">The rapid advancement of large AI models imposes stringent demands on data volume and computational resources. Federated learning, though designed to exploit distributed data and computational resources, faces data shortage from limited network coverage and computational constraints from edge devices.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">The rapid advancement of large AI models imposes stringent demands on data volume and computational resources. Federated learning, though designed to exploit distributed data and computational resources, faces data shortage from limited network coverage and computational constraints from edge devices. To address these issues, both the mixture-of-experts (MoE) and satellite-terrestrial network (STN) provide promising solutions, offering lightweight computation overhead and broad coverage, respectively. However, the satellite-ground relative motion results in intermittent connectivity, hindering conventional federated learning that relies on model synchronization across devices. To leverage the coverage of STN while preserving training efficiency, we propose EMS-FL, an expert-driven model splitting and federated learning method. EMS-FL assigns each device cluster only the experts highly correlated to their local data. Through non-overlapping expert assignments, asynchronous local learning is further proposed, where each device cluster trains its assigned experts consecutively and only uploads local parameters to the satellite during connected phases for aggregation and model updates. Consequently, EMS-FL effectively reduces the training overhead and achieves both faster convergence and higher accuracy compared with conventional federated learning. Rigorous convergence analysis is provided to theoretically characterize the learning performance. Furthermore, comprehensive experiments are conducted using public datasets and large models, validating the superiority of EMS-FL.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 EMS-FL: federated learning 방법을 제안합니다. 핵심은 MoE 모델을 분할해 에지 기기 부하를 줄이는 것입니다. 분산된 스포츠 플랫폼 사용자 데이터로 효율적인 학습이 가능해집니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 위성-지상 네트워크 기반으로 사용자별 하이라이트 모델을 훈련할 때, EMS-FL을 도입해 통신 latency 40% 감소 및 parameter count 60% 절감으로 개인화 서비스 품질을 향상시킵니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19485v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19766v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19766v1" onchange="updateToolbar()">
          <span class="rank">20위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19766v1">One2Scene: Geometric Consistent Explorable 3D Scene Generation from a Single Image</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>74.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 74.4%"></div>
          </div>
          <span class="score-detail">
            base:65 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Generating explorable 3D scenes from a single image is a highly challenging problem in 3D vision. Existing methods struggle to support free exploration, often producing severe geometric distortions and noisy artifacts when the viewpoint moves far from the original perspective.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Generating explorable 3D scenes from a single image is a highly challenging problem in 3D vision. Existing methods struggle to support free exploration, often producing severe geometric distortions and noisy artifacts when the viewpoint moves far from the original perspective. We introduce \textbf{One2Scene}, an effective framework that decomposes this ill-posed problem into three tractable sub-tasks to enable immersive explorable scene generation. We first use a panorama generator to produce anchor views from a single input image as initialization. Then, we lift these 2D anchors into an explicit 3D geometric scaffold via a generalizable, feed-forward Gaussian Splatting network. Instead of treating the panorama as a single image for reconstruction, we project it into multiple sparse anchor views and reformulate the reconstruction task as multi-view stereo matching, which allows us to leverage robust geometric priors learned from large-scale multi-view datasets. A bidirectional feature fusion module is used to enforce cross-view consistency, yielding an efficient and geometrically reliable scaffold. Finally, the scaffold serves as a strong prior for a novel view generator to produce photorealistic and geometrically accurate views at arbitrary cameras. By explicitly conditioning on a 3D-consistent scaffold to perform reconstruction, One2Scene works stably under large camera motions, supporting immersive scene exploration. Extensive experiments show that One2Scene substantially outperforms state-of-the-art methods in panorama depth estimation, feed-forward 360° reconstruction, and explorable 3D scene generation. Code and models will be released.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 단일 이미지에서 3D 장면 생성(One2Scene) 방법을 제안합니다. 핵심은 Gaussian Splatting을 통한 기하학적 일관성입니다. 스포츠 분석과의 직접적 연관성은 낮아 우선순위가 비교적 낮습니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 간접적으로 경기장 3D 재구성에 활용 가능하나, 실시간 성능 요구사항(fps 10+ 달성 어려움)과 높은 inference latency(500ms+)로 인해 현재 프로젝트 적용에는 한계가 있습니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19766v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19623v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19623v1" onchange="updateToolbar()">
          <span class="rank">21위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19623v1">PedaCo-Gen: Scaffolding Pedagogical Agency in Human-AI Collaborative Video Authoring</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>72.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 72.0%"></div>
          </div>
          <span class="score-detail">
            base:65 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.HC</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">While advancements in Text-to-Video (T2V) generative AI offer a promising path toward democratizing content creation, current models are often optimized for visual fidelity rather than instructional efficacy. This study introduces PedaCo-Gen, a pedagogically-informed human-AI collaborative video generating system for authoring instructional videos based on Mayer&#39;s Cognitive Theory of Multimedia Learning (CTML).</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">While advancements in Text-to-Video (T2V) generative AI offer a promising path toward democratizing content creation, current models are often optimized for visual fidelity rather than instructional efficacy. This study introduces PedaCo-Gen, a pedagogically-informed human-AI collaborative video generating system for authoring instructional videos based on Mayer&#39;s Cognitive Theory of Multimedia Learning (CTML). Moving away from traditional &#34;one-shot&#34; generation, PedaCo-Gen introduces an Intermediate Representation (IR) phase, enabling educators to interactively review and refine video blueprints-comprising scripts and visual descriptions-with an AI reviewer. Our study with 23 education experts demonstrates that PedaCo-Gen significantly enhances video quality across various topics and CTML principles compared to baselines. Participants perceived the AI-driven guidance not merely as a set of instructions but as a metacognitive scaffold that augmented their instructional design expertise, reporting high production efficiency (M=4.26) and guide validity (M=4.04). These findings highlight the importance of reclaiming pedagogical agency through principled co-creation, providing a foundation for future AI authoring tools that harmonize generative power with human professional expertise.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 교육용 비디오 생성 방법을 제안합니다. 핵심은 중간 표현 단계와 AI 협업으로 품질을 높이는 것입니다. 우리 프로젝트에서 자동 하이라이트 편집의 효율성과 교육적 가치를 개선할 수 있어 중요합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 운동 훈련 영상에 PedaCo-Gen을 적용해 주요 장면을 식별하고 편집합니다. 문제는 수동 편집의 시간 소모, 해결책은 IR 단계 도입으로 AI 협업, 결과는 빠른 하이라이트 생성과 높은 품질입니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19623v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19604v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19604v1" onchange="updateToolbar()">
          <span class="rank">22위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19604v1">Efficient Multi-Party Secure Comparison over Different Domains with Preprocessing Assistance</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>72.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 72.0%"></div>
          </div>
          <span class="score-detail">
            base:65 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CR</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Secure comparison is a fundamental primitive in multi-party computation, supporting privacy-preserving applications such as machine learning and data analytics. A critical performance bottleneck in comparison protocols is their preprocessing phase, primarily due to the high cost of generating the necessary correlated randomness.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Secure comparison is a fundamental primitive in multi-party computation, supporting privacy-preserving applications such as machine learning and data analytics. A critical performance bottleneck in comparison protocols is their preprocessing phase, primarily due to the high cost of generating the necessary correlated randomness. Recent frameworks introduce a passive, non-colluding dealer to accelerate preprocessing. However, two key issues still remain. First, existing dealer-assisted approaches treat the dealer as a drop-in replacement for conventional preprocessing without redesigning the comparison protocol to optimize the online phase. Second, most protocols are specialized for particular algebraic domains, adversary models, or party configurations, lacking broad generality. In this work, we present the first dealer-assisted $n$-party LTBits (Less-Than-Bits) and MSB (Most Significant Bit) extraction protocols over both $\mathbb{F}_p$ and $\mathbb{Z}_{2^k}$, achieving perfect security at the protocol level. By fully exploiting the dealer&#39;s capability to generate rich correlated randomness, our $\mathbb{F}_p$ construction achieves constant-round online complexity and our $\mathbb{Z}_{2^k}$ construction achieves $O(\log_n k)$ rounds with tunable branching factor. All protocols are formulated as black-box constructions via an extended ABB model, ensuring portability across MPC backends and adversary models. Experimental results demonstrate $1.79\times$ to $19.4\times$ speedups over state-of-the-art MPC frameworks, highlighting the practicality of our protocols for comparison-intensive MPC applications.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 다자간 보안 비교 프로토콜을 제안합니다. 핵심은 딜러 지원으로 온라인 단계를 최적화하는 것입니다. 우리 플랫폼의 사용자 데이터 프라이버시 보호에 직접 기여할 수 있어 중요합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 경기 분석 데이터 처리에 이 프로토콜을 적용합니다. 문제는 다중 사용자 환경에서 보안 취약점, 해결책은 LTBits/MSB 추출 도입, 결과는 낮은 latency로 안전한 계산입니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19604v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19497v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19497v1" onchange="updateToolbar()">
          <span class="rank">23위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19497v1">MICON-Bench: Benchmarking and Enhancing Multi-Image Context Image Generation in Unified Multimodal Models</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>70.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 70.4%"></div>
          </div>
          <span class="score-detail">
            base:60 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Recent advancements in Unified Multimodal Models (UMMs) have enabled remarkable image understanding and generation capabilities. However, while models like Gemini-2.5-Flash-Image show emerging abilities to reason over multiple related images, existing benchmarks rarely address the challenges of multi-image context generation, focusing mainly on text-to-image or single-image editing tasks.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent advancements in Unified Multimodal Models (UMMs) have enabled remarkable image understanding and generation capabilities. However, while models like Gemini-2.5-Flash-Image show emerging abilities to reason over multiple related images, existing benchmarks rarely address the challenges of multi-image context generation, focusing mainly on text-to-image or single-image editing tasks. In this work, we introduce \textbf{MICON-Bench}, a comprehensive benchmark covering six tasks that evaluate cross-image composition, contextual reasoning, and identity preservation. We further propose an MLLM-driven Evaluation-by-Checkpoint framework for automatic verification of semantic and visual consistency, where multimodal large language model (MLLM) serves as a verifier. Additionally, we present \textbf{Dynamic Attention Rebalancing (DAR)}, a training-free, plug-and-play mechanism that dynamically adjusts attention during inference to enhance coherence and reduce hallucinations. Extensive experiments on various state-of-the-art open-source models demonstrate both the rigor of MICON-Bench in exposing multi-image reasoning challenges and the efficacy of DAR in improving generation quality and cross-image coherence. Github: https://github.com/Angusliuuu/MICON-Bench.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 다중 이미지 생성 벤치마크와 메커니즘을 제안합니다. 핵심은 DAR을 통한 주의 재조정입니다. 우리 장치의 영상 보정 일관성을 높일 수 있어 중요합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 운동 장면의 다중 이미지 보정에 MICON-Bench를 적용합니다. 문제는 이미지 간 불일치, 해결책은 DAR 도입으로 주의 최적화, 결과는 향상된 생성 품질과 낮은 inference speed입니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19497v1">PDF</a>
          
          
          <a href="https://github.com/Angusliuuu/MICON-Bench">Code</a>
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.20068v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.20068v1" onchange="updateToolbar()">
          <span class="rank">24위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20068v1">The Invisible Gorilla Effect in Out-of-distribution Detection</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>70.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 70.4%"></div>
          </div>
          <span class="score-detail">
            base:60 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Deep Neural Networks achieve high performance in vision tasks by learning features from regions of interest (ROI) within images, but their performance degrades when deployed on out-of-distribution (OOD) data that differs from training data. This challenge has led to OOD detection methods that aim to identify and reject unreliable predictions.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Deep Neural Networks achieve high performance in vision tasks by learning features from regions of interest (ROI) within images, but their performance degrades when deployed on out-of-distribution (OOD) data that differs from training data. This challenge has led to OOD detection methods that aim to identify and reject unreliable predictions. Although prior work shows that OOD detection performance varies by artefact type, the underlying causes remain underexplored. To this end, we identify a previously unreported bias in OOD detection: for hard-to-detect artefacts (near-OOD), detection performance typically improves when the artefact shares visual similarity (e.g. colour) with the model&#39;s ROI and drops when it does not - a phenomenon we term the Invisible Gorilla Effect. For example, in a skin lesion classifier with red lesion ROI, we show the method Mahalanobis Score achieves a 31.5% higher AUROC when detecting OOD red ink (similar to ROI) compared to black ink (dissimilar) annotations. We annotated artefacts by colour in 11,355 images from three public datasets (e.g. ISIC) and generated colour-swapped counterfactuals to rule out dataset bias. We then evaluated 40 OOD methods across 7 benchmarks and found significant performance drops for most methods when artefacts differed from the ROI. Our findings highlight an overlooked failure mode in OOD detection and provide guidance for more robust detectors. Code and annotations are available at: https://github.com/HarryAnthony/Invisible_Gorilla_Effect.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 OOD 탐지 편향을 제안합니다. 핵심은 ROI 유사성에 따른 성능 변화입니다. 우리 시스템이 다양한 환경에서 견고하게 작동하는 데 필수적입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 카메라 촬영 장면에 이 통찰을 적용해 비정상적 데이터 탐지합니다. 문제는 OOD에서 성능 저하, 해결책은 색상 기반 탐지 최적화, 결과는 높은 신뢰성 분석입니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.20068v1">PDF</a>
          
          
          <a href="https://github.com/HarryAnthony/Invisible_Gorilla_Effect">Code</a>
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19570v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19570v1" onchange="updateToolbar()">
          <span class="rank">25위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19570v1">VALD: Multi-Stage Vision Attack Detection for Efficient LVLM Defense</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>69.6</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 69.6%"></div>
          </div>
          <span class="score-detail">
            base:62 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Large Vision-Language Models (LVLMs) can be vulnerable to adversarial images that subtly bias their outputs toward plausible yet incorrect responses. We introduce a general, efficient, and training-free defense that combines image transformations with agentic data consolidation to recover correct model behavior.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Large Vision-Language Models (LVLMs) can be vulnerable to adversarial images that subtly bias their outputs toward plausible yet incorrect responses. We introduce a general, efficient, and training-free defense that combines image transformations with agentic data consolidation to recover correct model behavior. A key component of our approach is a two-stage detection mechanism that quickly filters out the majority of clean inputs. We first assess image consistency under content-preserving transformations at negligible computational cost. For more challenging cases, we examine discrepancies in a text-embedding space. Only when necessary do we invoke a powerful LLM to resolve attack-induced divergences. A key idea is to consolidate multiple responses, leveraging both their similarities and their differences. We show that our method achieves state-of-the-art accuracy while maintaining notable efficiency: most clean images skip costly processing, and even in the presence of numerous adversarial examples, the overhead remains minimal.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 효율적 LVLM 방어 방법을 제안합니다. 핵심은 다단계 탐지와 응답 통합입니다. 우리 AI 분석 시스템의 보안 강화에 직접 참조되어 중요합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 영상 분석에 VALD를 적용해 적대적 공격 방어합니다. 문제는 공격으로 인한 오류, 해결책은 이미지 변환과 임베딩 검사, 결과는 낮은 오버헤드와 강건한 성능입니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19570v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19946v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19946v1" onchange="updateToolbar()">
          <span class="rank">26위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19946v1">When Pretty Isn&#39;t Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>68.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 68.0%"></div>
          </div>
          <span class="score-detail">
            base:60 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Recent text-to-image (T2I) diffusion models produce visually stunning images and demonstrate excellent prompt following. But do they perform well as synthetic vision data generators?</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent text-to-image (T2I) diffusion models produce visually stunning images and demonstrate excellent prompt following. But do they perform well as synthetic vision data generators? In this work, we revisit the promise of synthetic data as a scalable substitute for real training sets and uncover a surprising performance regression. We generate large-scale synthetic datasets using state-of-the-art T2I models released between 2022 and 2025, train standard classifiers solely on this synthetic data, and evaluate them on real test data. Despite observable advances in visual fidelity and prompt adherence, classification accuracy on real test data consistently declines with newer T2I models as training data generators. Our analysis reveals a hidden trend: These models collapse to a narrow, aesthetic-centric distribution that undermines diversity and label-image alignment. Overall, our findings challenge a growing assumption in vision research, namely that progress in generative realism implies progress in data realism. We thus highlight an urgent need to rethink the capabilities of modern T2I models as reliable training data generators.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 생성형 AI의 데이터 한계를 분석하여 실제 스포츠 이미지 보정 시 현실성 부족 문제를 예방할 수 있음. 프로젝트의 이미지 보정 기능 개발에 핵심 참고자료로 중요합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> T2I 모델의 미학적 편향을 보정 알고리즘에 반영해 실제 경기 이미지의 다양성 유지. 생성 데이터와 실제 촬영 데이터 간 차이를 측정해 보정 강도 자동 조절합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19946v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19549v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19549v1" onchange="updateToolbar()">
          <span class="rank">27위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19549v1">Sculpting the Vector Space: Towards Efficient Multi-Vector Visual Document Retrieval via Prune-then-Merge Framework</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>68.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 68.0%"></div>
          </div>
          <span class="score-detail">
            base:60 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CL</span>
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.IR</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Visual Document Retrieval (VDR), which aims to retrieve relevant pages within vast corpora of visually-rich documents, is of significance in current multimodal retrieval applications. The state-of-the-art multi-vector paradigm excels in performance but suffers from prohibitive overhead, a problem that current efficiency methods like pruning and merging address imperfectly, creating a difficult trade-off between compression rate and feature fidelity.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Visual Document Retrieval (VDR), which aims to retrieve relevant pages within vast corpora of visually-rich documents, is of significance in current multimodal retrieval applications. The state-of-the-art multi-vector paradigm excels in performance but suffers from prohibitive overhead, a problem that current efficiency methods like pruning and merging address imperfectly, creating a difficult trade-off between compression rate and feature fidelity. To overcome this dilemma, we introduce Prune-then-Merge, a novel two-stage framework that synergizes these complementary approaches. Our method first employs an adaptive pruning stage to filter out low-information patches, creating a refined, high-signal set of embeddings. Subsequently, a hierarchical merging stage compresses this pre-filtered set, effectively summarizing semantic content without the noise-induced feature dilution seen in single-stage methods. Extensive experiments on 29 VDR datasets demonstrate that our framework consistently outperforms existing methods, significantly extending the near-lossless compression range and providing robust performance at high compression ratios.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 대규모 영상에서 효율적으로 하이라이트 추출하는 기술로, 에지 디바이스의 제한된 연산 자원에서 실시간 처리 가능성 제공합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 프루닝-병합 기법을 스포츠 영상 분석에 적용해 중요 장면 필터링 속도 향상. 병목 현상 없이 초당 30프레임 처리로 실시간 하이라이트 생성합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19549v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.20084v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.20084v1" onchange="updateToolbar()">
          <span class="rank">28위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20084v1">Do Large Language Models Understand Data Visualization Principles?</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>68.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 68.0%"></div>
          </div>
          <span class="score-detail">
            base:60 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> LLM의 시각화 원리 이해력을 활용해 경기 분석 리포트 자동 생성 시 데이터 왜곡 방지. SNS 공용 콘텐츠의 신뢰성 확보에 필수적입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 선수 동작 차트 생성 시 원칙 위반 자동 감지 시스템 구축. 모델이 인식 못한 오류는 사용자 피드백 루프로 보완해 분석 정확도를 90% 이상 유지합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.20084v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19571v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19571v1" onchange="updateToolbar()">
          <span class="rank">29위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19571v1">HOCA-Bench: Beyond Semantic Perception to Predictive World Modeling via Hegelian Ontological-Causal Anomalies</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>64.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 64.0%"></div>
          </div>
          <span class="score-detail">
            base:55 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Video-LLMs have improved steadily on semantic perception, but they still fall short on predictive world modeling, which is central to physically grounded intelligence. We introduce HOCA-Bench, a benchmark that frames physical anomalies through a Hegelian lens.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Video-LLMs have improved steadily on semantic perception, but they still fall short on predictive world modeling, which is central to physically grounded intelligence. We introduce HOCA-Bench, a benchmark that frames physical anomalies through a Hegelian lens. HOCA-Bench separates anomalies into two types: ontological anomalies, where an entity violates its own definition or persistence, and causal anomalies, where interactions violate physical relations. Using state-of-the-art generative video models as adversarial simulators, we build a testbed of 1,439 videos (3,470 QA pairs). Evaluations on 17 Video-LLMs show a clear cognitive lag: models often identify static ontological violations (e.g., shape mutations) but struggle with causal mechanisms (e.g., gravity or friction), with performance dropping by more than 20% on causal tasks. System-2 &#34;Thinking&#34; modes improve reasoning, but they do not close the gap, suggesting that current architectures recognize visual patterns more readily than they apply basic physical laws.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 물리적 이상 감지 기술이 스포츠 동작 분석의 오류 식별에 직접 적용 가능. 선수 자세의 인과적 오류(예: 균형 손실) 탐지 정밀도 향상.</p>
        

        
        <p><strong>활용 인사이트:</strong> 동영상에서 중력·마찰 위반 등 물리적 비정상을 실시간 감지해 코칭 리포트 생성. 지연 시간 50ms 이내로 경기 중 즉시 분석 가능합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19571v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.19437v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.19437v1" onchange="updateToolbar()">
          <span class="rank">30위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19437v1">FinSight-Net:A Physics-Aware Decoupled Network with Frequency-Domain Compensation for Underwater Fish Detection in Smart Aquaculture</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>64.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 64.0%"></div>
          </div>
          <span class="score-detail">
            base:45 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Underwater fish detection (UFD) is a core capability for smart aquaculture and marine ecological monitoring. While recent detectors improve accuracy by stacking feature extractors or introducing heavy attention modules, they often incur substantial computational overhead and, more importantly, neglect the physics that fundamentally limits UFD: wavelength-dependent absorption and turbidity-induced scattering significantly degrade contrast, blur fine structures, and introduce backscattering noise, leading to unreliable localization and recognition.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Underwater fish detection (UFD) is a core capability for smart aquaculture and marine ecological monitoring. While recent detectors improve accuracy by stacking feature extractors or introducing heavy attention modules, they often incur substantial computational overhead and, more importantly, neglect the physics that fundamentally limits UFD: wavelength-dependent absorption and turbidity-induced scattering significantly degrade contrast, blur fine structures, and introduce backscattering noise, leading to unreliable localization and recognition. To address these challenges, we propose FinSight-Net, an efficient and physics-aware detection framework tailored for complex aquaculture environments. FinSight-Net introduces a Multi-Scale Decoupled Dual-Stream Processing (MS-DDSP) bottleneck that explicitly targets frequency-specific information loss via heterogeneous convolutional branches, suppressing backscattering artifacts while compensating distorted biological cues through scale-aware and channel-weighted pathways. We further design an Efficient Path Aggregation FPN (EPA-FPN) as a detail-filling mechanism: it restores high-frequency spatial information typically attenuated in deep layers by establishing long-range skip connections and pruning redundant fusion routes, enabling robust detection of non-rigid fish targets under severe blur and turbidity. Extensive experiments on DeepFish, AquaFishSet, and our challenging UW-BlurredFish benchmark demonstrate that FinSight-Net achieves state-of-the-art performance. In particular, on UW-BlurredFish, FinSight-Net reaches 92.8% mAP, outperforming YOLOv11s by 4.8% while reducing parameters by 29.0%, providing a strong and lightweight solution for real-time automated monitoring in smart aquaculture.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 경량 객체 인식 기술이 RK3588 하드웨어 최적화에 유용하나, 수중과 스포츠 환경 차이로 직접 적용보다 아키텍처 참고에 집중합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> MS-DDSP 구조를 변형해 스포츠 장비 인식 모델 개발. 파라미터 30% 감소시키고 초당 추론 속도 25프레임 달성으로 에지 디바이스 효율화합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.19437v1">PDF</a>
          
          
        </div>

        
      </div>
      

      
      <button class="show-more-btn" id="showMoreBtn" onclick="showAllTier1()">
        나머지 25편 더 보기
      </button>
      

      
      <div class="tier-divider">Tier 2</div>
      

      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.20046v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.20046v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20046v1">Closing the gap in multimodal medical representation alignment</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.20046v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs but remains unknown and unresolved in more complex multimodal settings, such as the medical domain. In this work, we study this phenomenon in the latter case, revealing that the modality gap is present also in medical alignment, and we propose a modality-agnostic framework that closes this gap, ensuring that semantically related representations are more aligned, regardless of their source modality. Our method enhances alignment between radiology images and clinical text, improving cross-modal retrieval and image captioning.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">의료 영역 다중모달 정렬 기술로 간접적 참고 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.20062v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.20062v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20062v1">A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.20062v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">stat.ML</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Pretraining and fine-tuning are central stages in modern machine learning systems. In practice, feature learning plays an important role across both stages: deep neural networks learn a broad range of useful features during pretraining and further refine those features during fine-tuning.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Pretraining and fine-tuning are central stages in modern machine learning systems. In practice, feature learning plays an important role across both stages: deep neural networks learn a broad range of useful features during pretraining and further refine those features during fine-tuning. However, an end-to-end theoretical understanding of how choices of initialization impact the ability to reuse and refine features during fine-tuning has remained elusive. Here we develop an analytical theory of the pretraining-fine-tuning pipeline in diagonal linear networks, deriving exact expressions for the generalization error as a function of initialization parameters and task statistics. We find that different initialization choices place the network into four distinct fine-tuning regimes that are distinguished by their ability to support feature learning and reuse, and therefore by the task statistics for which they are beneficial. In particular, a smaller initialization scale in earlier layers enables the network to both reuse and refine its features, leading to superior generalization on fine-tuning tasks that rely on a subset of pretraining features. We demonstrate empirically that the same initialization parameters impact generalization in nonlinear networks trained on CIFAR-100. Overall, our results demonstrate analytically how data and network initialization interact to shape fine-tuning generalization, highlighting an important role for the relative scale of initialization across different layers in enabling continued feature learning during fine-tuning.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">파인튜닝 이론이 스포츠 분석 모델 최적화에 참고 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.20100v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.20100v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20100v1">Transcending the Annotation Bottleneck: AI-Powered Discovery in Biology and Medicine</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.20100v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">eess.IV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">The dependence on expert annotation has long constituted the primary rate-limiting step in the application of artificial intelligence to biomedicine. While supervised learning drove the initial wave of clinical algorithms, a paradigm shift towards unsupervised and self-supervised learning (SSL) is currently unlocking the latent potential of biobank-scale datasets.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">The dependence on expert annotation has long constituted the primary rate-limiting step in the application of artificial intelligence to biomedicine. While supervised learning drove the initial wave of clinical algorithms, a paradigm shift towards unsupervised and self-supervised learning (SSL) is currently unlocking the latent potential of biobank-scale datasets. By learning directly from the intrinsic structure of data - whether pixels in a magnetic resonance image (MRI), voxels in a volumetric scan, or tokens in a genomic sequence - these methods facilitate the discovery of novel phenotypes, the linkage of morphology to genetics, and the detection of anomalies without human bias. This article synthesises seminal and recent advances in &#34;learning without labels,&#34; highlighting how unsupervised frameworks can derive heritable cardiac traits, predict spatial gene expression in histology, and detect pathologies with performance that rivals or exceeds supervised counterparts.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Focuses on unsupervised learning in biomedicine, indirectly relevant to AI video analysis.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.19595v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.19595v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19595v1">Constrained graph generation: Preserving diameter and clustering coefficient simultaneously</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.19595v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.SI</span>
          
          <span class="cat-tag">cs.DM</span>
          
          <span class="cat-tag">math.PR</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Generating graphs subject to strict structural constraints is a fundamental computational challenge in network science. Simultaneously preserving interacting properties-such as the diameter and the clustering coefficient- is particularly demanding.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Generating graphs subject to strict structural constraints is a fundamental computational challenge in network science. Simultaneously preserving interacting properties-such as the diameter and the clustering coefficient- is particularly demanding. Simple constructive algorithms often fail to locate vanishingly small sets of feasible graphs, while traditional Markov-chain Monte Carlo (MCMC) samplers suffer from severe ergodicity breaking. In this paper, we propose a two-step hybrid framework combining Ant Colony Optimization (ACO) and MCMC sampling. First, we design a layered ACO heuristic to perform a guided global search, effectively locating valid graphs with prescribed diameter and clustering coefficient. Second, we use these ACO-designed graphs as structurally distinct seed states for an MCMC rewiring algorithm. We evaluate this framework across a wide range of graph edge densities and varying diameter-clustering-coefficient constraint regimes. Using the spectral distance of the normalized Laplacian to quantify structural diversity of the resulting graphs, our experiments reveal a sharp contrast between the methods. Standard MCMC samplers remain rigidly trapped in an isolated subset of feasible graphs around their initial seeds. Conversely, our hybrid ACO-MCMC approach successfully bridges disconnected configuration landscapes, generating a vastly richer and structurally diverse set of valid graphs.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Indirectly relevant for potential strategy analysis</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.19903v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.19903v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19903v1">Rethinking Chronological Causal Discovery with Signal Processing</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.19903v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">eess.SP</span>
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">stat.ML</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Causal discovery problems use a set of observations to deduce causality between variables in the real world, typically to answer questions about biological or physical systems. These observations are often recorded at regular time intervals, determined by a user or a machine, depending on the experiment design.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Causal discovery problems use a set of observations to deduce causality between variables in the real world, typically to answer questions about biological or physical systems. These observations are often recorded at regular time intervals, determined by a user or a machine, depending on the experiment design. There is generally no guarantee that the timing of these recordings matches the timing of the underlying biological or physical events. In this paper, we examine the sensitivity of causal discovery methods to this potential mismatch. We consider empirical and theoretical evidence to understand how causal discovery performance is impacted by changes of sampling rate and window length. We demonstrate that both classical and recent causal discovery methods exhibit sensitivity to these hyperparameters, and we discuss how ideas from signal processing may help us understand these phenomena.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Causal discovery methods may indirectly relate to motion analysis in sports.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.19473v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.19473v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19473v1">The generalized underlap coefficient with an application in clustering</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.19473v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">stat.ME</span>
          
          <span class="cat-tag">math.ST</span>
          
          <span class="cat-tag">stat.ML</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Quantifying distributional separation across groups is fundamental in statistical learning and scientific discovery, yet most classical discrepancy measures are tailored to two-group comparisons. We generalize the underlap coefficient (UNL), a multi-group separation measure, to multivariate variables.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Quantifying distributional separation across groups is fundamental in statistical learning and scientific discovery, yet most classical discrepancy measures are tailored to two-group comparisons. We generalize the underlap coefficient (UNL), a multi-group separation measure, to multivariate variables. We establish key properties of UNL and provide an explicit connection to the total variation. We further interpret the UNL as a dependence measure between a group label and variables of interest and compare it with mutual information. We propose an importance sampling estimator of the UNL that can be combined with flexible density estimators. The utility of the UNL for assessing partition-covariate dependence in clustering is highlighted in detail, where it is particularly useful for evaluating the single-weights assumption in covariate-dependent mixture models. Finally we illustrate the application of the UNL in clustering using two real world datasets.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">통계적 군집화 방법이 동작 분석에 참고될 수 있음.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.19409v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.19409v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19409v1">AuditoryHuM: Auditory Scene Label Generation and Clustering using Human-MLLM Collaboration</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.4</span>
            
            <a href="https://arxiv.org/pdf/2602.19409v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.SD</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Manual annotation of audio datasets is labour intensive, and it is challenging to balance label granularity with acoustic separability. We introduce AuditoryHuM, a novel framework for the unsupervised discovery and clustering of auditory scene labels using a collaborative Human-Multimodal Large Language Model (MLLM) approach.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Manual annotation of audio datasets is labour intensive, and it is challenging to balance label granularity with acoustic separability. We introduce AuditoryHuM, a novel framework for the unsupervised discovery and clustering of auditory scene labels using a collaborative Human-Multimodal Large Language Model (MLLM) approach. By leveraging MLLMs (Gemma and Qwen) the framework generates contextually relevant labels for audio data. To ensure label quality and mitigate hallucinations, we employ zero-shot learning techniques (Human-CLAP) to quantify the alignment between generated text labels and raw audio content. A strategically targeted human-in-the-loop intervention is then used to refine the least aligned pairs. The discovered labels are grouped into thematically cohesive clusters using an adjusted silhouette score that incorporates a penalty parameter to balance cluster cohesion and thematic granularity. Evaluated across three diverse auditory scene datasets (ADVANCE, AHEAD-DS, and TAU 2019), AuditoryHuM provides a scalable, low-cost solution for creating standardised taxonomies. This solution facilitates the training of lightweight scene recognition models deployable to edge devices, such as hearing aids and smart home assistants. The project page and code: https://github.com/Australian-Future-Hearing-Initiative</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">오디오 분석 기술이 에지 디바이스와 약하게 연관되어 점수 부여</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.19884v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.19884v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19884v1">Extending CPU-less parallel execution of lambda calculus in digital logic with lists and arithmetic</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">50.4</span>
            
            <a href="https://arxiv.org/pdf/2602.19884v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.AR</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Computer architecture is searching for new ways to make use of increasingly available digital logic without the serial bottlenecks of CPU-based design. Recent work has demonstrated a fully CPU-less approach to executing functional programs, by exploiting their inherent parallelisability to compile them directly into parallel digital logic.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Computer architecture is searching for new ways to make use of increasingly available digital logic without the serial bottlenecks of CPU-based design. Recent work has demonstrated a fully CPU-less approach to executing functional programs, by exploiting their inherent parallelisability to compile them directly into parallel digital logic. This work uses lambda-calculus as a hyper simple functional language to prove the concept, but is impractical for real-world programming due to the well-known inefficiencies of pure lambda$-calculus. It is common in language design to extend basic lambda-calculus with additional primitives to short-cut common tasks such as arithmetic and lists. In this work, we build upon our previous research to examine how such extensions may be applied to CPU-less functional execution in digital logic, with the objective of advancing the approach toward practical implementation. We present a set of structures and algorithms for representing new primitives, describe a systematic process for selecting, implementing, and evaluating them, and demonstrate substantial reductions in execution time and node usage. These improvements are implemented in an open-source system, which is shown to correctly evaluate a range of representative lambda expressions.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">CPU-less functional programming execution unrelated to sports video capture or analysis.</div>
        
      </div>
      
    
  </div>

  <div class="tab-content tab-panel-remind">
    
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.18140v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18140v1">Flexi-NeurA: A Configurable Neuromorphic Accelerator with Adaptive Bit-Precision Exploration for Edge SNNs</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">94.4</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.AR</span>
          
          <span class="cat-tag">cs.NE</span>
          
        </div>
        
        
        <div class="compact-body">Neuromorphic accelerators promise unparalleled energy efficiency and computational density for spiking neural networks (SNNs), especially in edge intelligence applications. However, most existing platforms exhibit rigid architectures with limited configurability, restricting their adaptability to heterogeneous workloads and diverse design objectives. To address these limitations, we present Flexi-NeurA -- a parameterizable neuromorphic accelerator (core) that unifies configurability, flexibility, and efficiency. Flexi-NeurA allows users to customize neuron models, network structures, and precision settings at design time. By pairing these design-time configurability and flexibility features with a time-multiplexed and event-driven processing approach, Flexi-NeurA substantially reduces the required hardware resources and total power while preserving high efficiency and low inference latency. Complementing this, we introduce Flex-plorer, a heuristic-guided design-space exploration (DSE) tool that determines cost-effective fixed-point precisions for critical parameters -- such as decay factors, synaptic weights, and membrane potentials -- based on user-defined trade-offs between accuracy and resource usage. Based on the configuration selected through the Flex-plorer process, RTL code is configured to match the specified design. Comprehensive evaluations across MNIST, SHD, and DVS benchmarks demonstrate that the Flexi-NeurA and Flex-plorer co-framework achieves substantial improvements in accuracy, latency, and energy efficiency. A three-layer 256--128--10 fully connected network with LIF neurons mapped onto two processing cores achieves 97.23% accuracy on MNIST with 1.1~ms inference latency, utilizing only 1,623 logic cells, 7 BRAMs, and 111~mW of total power -- establishing Flexi-NeurA as a scalable, edge-ready neuromorphic platform.</div>
        
        
        <div class="compact-reason">에너지 효율적이고 저지연의 신경모방 가속기 기술을 제안하여, 엣지 AI 하드웨어에 직접 적용 가능하다. 프로젝트의 실시간 스포츠 분석에 필수적이다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.18397v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18397v1">How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">89.6</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems. In this paper, we ask a fundamental research question: How should we design future VLA models and systems to support real-time inference? To address this question, we first introduce VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Using VLA-Perf, we conduct the first systematic study of the VLA inference performance landscape. From a model-design perspective, we examine how inference performance is affected by model scaling, model architectural choices, long-context video inputs, asynchronous inference, and dual-system model pipelines. From the deployment perspective, we analyze where VLA inference should be executed -- on-device, on edge servers, or in the cloud -- and how hardware capability and network performance jointly determine end-to-end latency. By distilling 15 key takeaways from our comprehensive evaluation, we hope this work can provide practical guidance for the design of future VLA models and inference systems.</div>
        
        
        <div class="compact-reason">엣지 디바이스에서 VLA 모델의 실시간 추론 성능 최적화 기술을 제안해, AI 촬영 장비의 동작 분석 및 영상 처리 지연 시간 감소에 직접 기여한다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.18158v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18158v1">A reliability- and latency-driven task allocation framework for workflow applications in the edge-hub-cloud continuum</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">88.0</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.DC</span>
          
          <span class="cat-tag">cs.ET</span>
          
        </div>
        
        
        <div class="compact-body">A growing number of critical workflow applications leverage a streamlined edge-hub-cloud architecture, which diverges from the conventional edge computing paradigm. An edge device, in collaboration with a hub device and a cloud server, often suffices for their reliable and efficient execution. However, task allocation in this streamlined architecture is challenging due to device limitations and diverse operating conditions. Given the inherent criticality of such workflow applications, where reliability and latency are vital yet conflicting objectives, an exact task allocation approach is typically required to ensure optimal solutions. As no existing method holistically addresses these issues, we propose an exact multi-objective task allocation framework to jointly optimize the overall reliability and latency of a workflow application in the specific edge-hub-cloud architecture. We present a comprehensive binary integer linear programming formulation that considers the relative importance of each objective. It incorporates time redundancy techniques, while accounting for crucial constraints often overlooked in related studies. We evaluate our approach using a relevant real-world workflow application, as well as synthetic workflows varying in structure, size, and criticality. In the real-world application, our method achieved average improvements of 84.19% in reliability and 49.81% in latency over baseline strategies, across relevant objective trade-offs. Overall, the experimental results demonstrate the effectiveness and scalability of our approach across diverse workflow applications for the considered system architecture, highlighting its practicality with runtimes averaging between 0.03 and 50.94 seconds across all examined workflows.</div>
        
        
        <div class="compact-reason">엣지-허브-클라우드 아키텍처용 작업 할당 프레임워크로, 영상 처리 및 분석 작업의 신뢰성과 지연 시간 최적화에 직접 적용 가능하다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.16362v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.16362v1">How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">88.0</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.DC</span>
          
          <span class="cat-tag">cs.NI</span>
          
          <span class="cat-tag">eess.SY</span>
          
        </div>
        
        
        <div class="compact-body">Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.</div>
        
        
        <div class="compact-reason">엣지 디바이스 간 분산 추론의 계산적 신뢰성 분석 프레임워크로, 스포츠 영상 분석 서비스의 안정적 운영에 필수적이다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.14582v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14582v1">YOLO26: A Comprehensive Architecture Overview and Key Improvements</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">86.4</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">You Only Look Once (YOLO) has been the prominent model for computer vision in deep learning for a decade. This study explores the novel aspects of YOLO26, the most recent version in the YOLO series. The elimination of Distribution Focal Loss (DFL), implementation of End-to-End NMS-Free Inference, introduction of ProgLoss + Small-Target-Aware Label Assignment (STAL), and use of the MuSGD optimizer are the primary enhancements designed to improve inference speed, which is claimed to achieve a 43% boost in CPU mode. This is designed to allow YOLO26 to attain real-time performance on edge devices or those without GPUs. Additionally, YOLO26 offers improvements in many computer vision tasks, including instance segmentation, pose estimation, and oriented bounding box (OBB) decoding. We aim for this effort to provide more value than just consolidating information already included in the existing technical documentation. Therefore, we performed a rigorous architectural investigation into YOLO26, mostly using the source code available in its GitHub repository and its official documentation. The authentic and detailed operational mechanisms of YOLO26 are inside the source code, which is seldom extracted by others. The YOLO26 architectural diagram is shown as the outcome of the investigation. This study is, to our knowledge, the first one presenting the CNN-based YOLO26 architecture, which is the core of YOLO26. Our objective is to provide a precise architectural comprehension of YOLO26 for researchers and developers aspiring to enhance the YOLO model, ensuring it remains the leading deep learning model in computer vision.</div>
        
        
        <div class="compact-reason">에지 디바이스용 실시간 객체 감지 및 포즈 추정 기술로 프로젝트의 핵심 기능인 운동 자세 분석과 경기 장면 인식에 직접 적용 가능. CPU 모드에서 43% 향상된 추론 속도(fps)가 RK3588 기기에서 실시간 성능 보장.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.15633v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.15633v1">SpecFuse: A Spectral-Temporal Fusion Predictive Control Framework for UAV Landing on Oscillating Marine Platforms</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">84.4</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">Autonomous landing of Uncrewed Aerial Vehicles (UAVs) on oscillating marine platforms is severely constrained by wave-induced multi-frequency oscillations, wind disturbances, and prediction phase lags in motion prediction. Existing methods either treat platform motion as a general random process or lack explicit modeling of wave spectral characteristics, leading to suboptimal performance under dynamic sea conditions. To address these limitations, we propose SpecFuse: a novel spectral-temporal fusion predictive control framework that integrates frequency-domain wave decomposition with time-domain recursive state estimation for high-precision 6-DoF motion forecasting of Uncrewed Surface Vehicles (USVs). The framework explicitly models dominant wave harmonics to mitigate phase lags, refining predictions in real time via IMU data without relying on complex calibration. Additionally, we design a hierarchical control architecture featuring a sampling-based HPO-RRT* algorithm for dynamic trajectory planning under non-convex constraints and a learning-augmented predictive controller that fuses data-driven disturbance compensation with optimization-based execution. Extensive validations (2,000 simulations + 8 lake experiments) show our approach achieves a 3.2 cm prediction error, 4.46 cm landing deviation, 98.7% / 87.5% success rates (simulation / real-world), and 82 ms latency on embedded hardware, outperforming state-of-the-art methods by 44%-48% in accuracy. Its robustness to wave-wind coupling disturbances supports critical maritime missions such as search and rescue and environmental monitoring. All code, experimental configurations, and datasets will be released as open-source to facilitate reproducibility.</div>
        
        
        <div class="compact-reason">이 논문은 진동하는 해상 플랫폼에 UAV 착륙을 위한 제어 방법을 제안합니다. 핵심은 실시간 임베디드 예측 제어로, 파도와 바람 영향 하에서 안정적 위치 유지합니다. 에지 디바이스 관련성(82ms 지연)이 높아 움직이는 촬영 플랫폼에 적용 가능한 이유입니다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.13378v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13378v1">LAF-YOLOv10 with Partial Convolution Backbone, Attention-Guided Feature Pyramid, Auxiliary P2 Head, and Wise-IoU Loss for Small Object Detection in Drone Aerial Imagery</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">84.4</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">Unmanned aerial vehicles serve as primary sensing platforms for surveillance, traffic monitoring, and disaster response, making aerial object detection a central problem in applied computer vision. Current detectors struggle with UAV-specific challenges: targets spanning only a few pixels, cluttered backgrounds, heavy occlusion, and strict onboard computational budgets. This study introduces LAF-YOLOv10, built on YOLOv10n, integrating four complementary techniques to improve small-object detection in drone imagery. A Partial Convolution C2f (PC-C2f) module restricts spatial convolution to one quarter of backbone channels, reducing redundant computation while preserving discriminative capacity. An Attention-Guided Feature Pyramid Network (AG-FPN) inserts Squeeze-and-Excitation channel gates before multi-scale fusion and replaces nearest-neighbor upsampling with DySample for content-aware interpolation. An auxiliary P2 detection head at 160$\times$160 resolution extends localization to objects below 8$\times$8 pixels, while the P5 head is removed to redistribute parameters. Wise-IoU v3 replaces CIoU for bounding box regression, attenuating gradients from noisy annotations in crowded aerial scenes. The four modules address non-overlapping bottlenecks: PC-C2f compresses backbone computation, AG-FPN refines cross-scale fusion, the P2 head recovers spatial resolution, and Wise-IoU stabilizes regression under label noise. No individual component is novel; the contribution is the joint integration within a single YOLOv10 framework. Across three training runs (seeds 42, 123, 256), LAF-YOLOv10 achieves 35.1$\pm$0.3\% mAP@0.5 on VisDrone-DET2019 with 2.3\,M parameters, exceeding YOLOv10n by 3.3 points. Cross-dataset evaluation on UAVDT yields 35.8$\pm$0.4\% mAP@0.5. Benchmarks on NVIDIA Jetson Orin Nano confirm 24.3 FPS at FP16, demonstrating viability for embedded UAV deployment.</div>
        
        
        <div class="compact-reason">이 논문은 드론 영상에서 작은 객체 감지를 위한 방법을 제안합니다. 핵심은 경량화된 YOLOv10 변종으로, 작은 대상(예: 선수, 공)을 복잡한 배경에서 정확히 식별합니다. 에지 디바이스용 최적화로 스포츠 촬영에 직접 적용 가능하며, 24.3 FPS와 2.3M 매개변수로 RK3588에서 효율적 실행이 핵심 이유입니다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.18309v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18309v1">Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">82.4</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model&#39;s multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an &#34;in the wild&#34; split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.</div>
        
        
        <div class="compact-reason">이 논문은 스케치와 텍스트로 패션 이미지 생성 방법을 제안합니다. 핵심은 다중 수준 조건화로, 구조와 세부 사항 정확히 반영합니다. 이미지 생성 기술이 프로젝트의 보정/변환 기능에 적용 가능해 선택 이유입니다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.18322v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18322v1">Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">82.4</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.</div>
        
        
        <div class="compact-reason">이 논문은 다양한 조명에서 이미지 보정 방법을 제안합니다. 핵심은 뷰-적응형 밝기 조절로, 저조도/과노출 시 색상 일관성 유지합니다. 프로젝트의 이미지 보정 기능과 직접 연관되어 화질 개선이 핵심 이유입니다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.13185v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13185v1">FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">82.0</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.GR</span>
          
        </div>
        
        
        <div class="compact-body">Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of &#34;appearance&#34; and &#34;motion&#34; provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.</div>
        
        
        <div class="compact-reason">이 논문은 동영상 생성 제어 방법을 제안합니다. 핵심은 외형과 움직임 분해로, 객체 편집 및 카메라 제어를 유연히 수행합니다. 프로젝트의 핵심인 비디오 편집과 직접 관련되어 하이라이트 자동 생성에 필수적인 이유입니다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.11966v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11966v1">MING: An Automated CNN-to-Edge MLIR HLS framework</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">82.0</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.AR</span>
          
        </div>
        
        
        <div class="compact-body">Driven by the increasing demand for low-latency and real-time processing, machine learning applications are steadily migrating toward edge computing platforms, where Field-Programmable Gate Arrays (FPGAs) are widely adopted for their energy efficiency compared to CPUs and GPUs. To generate high-performance and low-power FPGA designs, several frameworks built upon High Level Synthesis (HLS) vendor tools have been proposed, among which MLIR-based frameworks are gaining significant traction due to their extensibility and ease of use. However, existing state-of-the-art frameworks often overlook the stringent resource constraints of edge devices. To address this limitation, we propose MING, an Multi-Level Intermediate Representation (MLIR)-based framework that abstracts and automates the HLS design process. Within this framework, we adopt a streaming architecture with carefully managed buffers, specifically designed to handle resource constraints while ensuring low-latency. In comparison with recent frameworks, our approach achieves on average 15x speedup for standard Convolutional Neural Network (CNN) kernels with up to four layers, and up to 200x for single-layer kernels. For kernels with larger input sizes, MING is capable of generating efficient designs that respect hardware resource constraints, whereas state-of-the-art frameworks struggle to meet.</div>
        
        
        <div class="compact-reason">에지 디바이스용 CNN 배포 최적화 프레임워크로 저지연 스트리밍 처리와 자원 제약 해결. 프로젝트의 rk3588 기반 실시간 스포츠 영상 분석 핵심 기술에 직접 적용 가능해 중요함.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.13052v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13052v1">Quantization-Aware Collaborative Inference for Large Embodied AI Models</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">82.0</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">eess.SP</span>
          
        </div>
        
        
        <div class="compact-body">Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents. To address this issue, we investigate quantization-aware collaborative inference (co-inference) for embodied AI systems. First, we develop a tractable approximation for quantization-induced inference distortion. Based on this approximation, we derive lower and upper bounds on the quantization rate-inference distortion function, characterizing its dependence on LAIM statistics, including the quantization bit-width. Next, we formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints, aiming to minimize the distortion upper bound while ensuring tightness through the corresponding lower bound. Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the proposed joint design. Particularly, simulations and real-world testbed experiments demonstrate the effectiveness of the proposed joint design in balancing inference quality, latency, and energy consumption in edge embodied AI systems.</div>
        
        
        <div class="compact-reason">엣지 AI 모델의 양자화 협력 추론 기술로 자원 제약 환경 최적화. 스포츠 디바이스에서 대형 모델 효율적 실행에 필수적이라 중요함.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.14302v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14302v1">Floe: Federated Specialization for Real-Time LLM-SLM Inference</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">82.0</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.DC</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.</div>
        
        
        <div class="compact-reason">에지-클라우드 연합 LLM/SLM 실시간 추론 솔루션. 스포츠 경기 전략 분석을 위한 저지연 언어 모델 핵심에 직접 연관됨.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.18199v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18199v1">A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">81.6</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.</div>
        
        
        <div class="compact-reason">자가 지도 동작 보정 기술로 물리적 타당성 향상. 스포츠 동작 분석 시 발 떠림 같은 오류 보정에 직접 활용 가능해 중요함.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.17997v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.17997v1">Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">80.0</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly&#39;s brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.</div>
        
        
        <div class="compact-reason">생물학적 신경망 기반 운동 제어 기술이 스포츠 동작 분석 및 하드웨어 개발에 적용 가능합니다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.16545v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.16545v1">Let&#39;s Split Up: Zero-Shot Classifier Edits for Fine-Grained Video Understanding</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">80.0</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly. To address these challenges, we introduce category splitting, a new task where an existing classifier is edited to refine a coarse category into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video classifiers to expose fine-grained distinctions without additional data. We further show that low-shot fine-tuning, while simple, is highly effective and benefits from our zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest. Project page: https://kaitingliu.github.io/Category-Splitting/.</div>
        
        
        <div class="compact-reason">제로샷 비디오 분류기 미세 조정 기술. 스포츠 장면 세부 인식(예: 슛 종류 분류) 능력 향상에 직접 적용 가능함.</div>
        
      </div>
      
    
  </div>

  <div class="tab-content tab-panel-below">
    
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19896v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19896v1">Monocular Mesh Recovery and Body Measurement of Female Saanen Goats</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">35점</span>
            
            <a href="https://arxiv.org/pdf/2602.19896v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-reason">Livestock-focused 3D reconstruction, weak relevance to sports</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19514v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19514v1">Security Risks of AI Agents Hiring Humans: An Empirical Marketplace Study</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">35점</span>
            
            <a href="https://arxiv.org/pdf/2602.19514v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CR</span>
          
          <span class="cat-tag">cs.HC</span>
          
        </div>
        
        
        <div class="compact-reason">Weakly related to platform marketplace risks</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19547v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19547v1">CIBER: A Comprehensive Benchmark for Security Evaluation of Code Interpreter Agents</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">35점</span>
            
            <a href="https://arxiv.org/pdf/2602.19547v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CR</span>
          
        </div>
        
        
        <div class="compact-reason">코드 인터프리터 보안 벤치마크는 약한 관련성.</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19608v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19608v1">Satellite-Based Detection of Looted Archaeological Sites Using Machine Learning</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">30점</span>
            
            <a href="https://arxiv.org/pdf/2602.19608v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-reason">위성 이미지 감시 기술로 스포츠 프로젝트와의 관련성 약함</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19491v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19491v1">Botson: An Accessible and Low-Cost Platform for Social Robotics Research</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">30점</span>
            
            <a href="https://arxiv.org/pdf/2602.19491v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.HC</span>
          
        </div>
        
        
        <div class="compact-reason">사회적 로봇 플랫폼으로 스포츠 하드웨어와 약한 연관성</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19784v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19784v1">High-Altitude Platforms in the Low-Altitude Economy: Bridging Communication, Computing, and Regulation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">30점</span>
            
            <a href="https://arxiv.org/pdf/2602.19784v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">eess.SY</span>
          
        </div>
        
        
        <div class="compact-reason">항공 플랫폼 기술로 스포츠 프로젝트와 직접적 관련 없음</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19714v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19714v1">Git Takes Two: Split-View Awareness for Collaborative Learning of Distributed Workflows in Git</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">30점</span>
            
            <a href="https://arxiv.org/pdf/2602.19714v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.HC</span>
          
          <span class="cat-tag">cs.SE</span>
          
        </div>
        
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19810v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19810v1">OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">30점</span>
            
            <a href="https://arxiv.org/pdf/2602.19810v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.20040v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20040v1">AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">30점</span>
            
            <a href="https://arxiv.org/pdf/2602.20040v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CL</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19933v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19933v1">Edge-based Synchronization over Signed Digraphs with Multiple Leaders</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">30점</span>
            
            <a href="https://arxiv.org/pdf/2602.19933v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">eess.SY</span>
          
        </div>
        
        
        <div class="compact-reason">Graph-based multi-agent synchronization, weakly related to coordination systems.</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.20059v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20059v1">Interaction Theater: A case of LLM Agents Interacting at Scale</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">30점</span>
            
            <a href="https://arxiv.org/pdf/2602.20059v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-reason">Studies LLM agent interactions, tangential to social platform aspects.</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19560v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19560v1">Identifying, Explaining, and Correcting Ableist Language with AI</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">30점</span>
            
            <a href="https://arxiv.org/pdf/2602.19560v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.HC</span>
          
        </div>
        
        
        <div class="compact-reason">Language correction AI unrelated to sports video capture or physical hardware.</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19471v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19471v1">Forgetting-Resistant and Lesion-Aware Source-Free Domain Adaptive Fundus Image Analysis with Vision-Language Model</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">25점</span>
            
            <a href="https://arxiv.org/pdf/2602.19471v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-reason">의료 영상 분석 기술이 스포츠 프로젝트와 약하게 관련되어 점수 부여</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19828v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19828v1">TextShield-R1: Reinforced Reasoning for Tampered Text Detection</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">25점</span>
            
            <a href="https://arxiv.org/pdf/2602.19828v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-reason">텍스트 위조 탐지 기술은 스포츠 영상 편집 및 분석과 직접적 관련이 약함.</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.20137v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20137v1">Do Large Language Models Understand Data Visualization Rules?</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">25점</span>
            
            <a href="https://arxiv.org/pdf/2602.20137v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-reason">Evaluates LLMs for data visualization rules, weakly relevant to image analysis.</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19606v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19606v1">Predicting known Vulnerabilities from Attack News: A Transformer-Based Approach</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">25점</span>
            
            <a href="https://arxiv.org/pdf/2602.19606v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CR</span>
          
        </div>
        
        
        <div class="compact-reason">Cybersecurity vulnerability prediction irrelevant to sports video editing or AI hardware.</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19695v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19695v1">Shifting Engagement With Cybersecurity: How People Discover and Share Cybersecurity Content at Work and at Home</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">25점</span>
            
            <a href="https://arxiv.org/pdf/2602.19695v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.HC</span>
          
        </div>
        
        
        <div class="compact-reason">Cybersecurity content sharing study irrelevant to sports video platform.</div>
        
      </div>
      
      <div class="compact-card compact-card--below" data-paper-url="http://arxiv.org/abs/2602.19490v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19490v1">FuzzySQL: Uncovering Hidden Vulnerabilities in DBMS Special Features with LLM-Driven Fuzzing</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">25점</span>
            
            <a href="https://arxiv.org/pdf/2602.19490v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.DB</span>
          
          <span class="cat-tag">cs.CR</span>
          
          <span class="cat-tag">cs.SE</span>
          
        </div>
        
        
        <div class="compact-reason">DBMS 퍼징은 프로젝트와 거의 관련 없음.</div>
        
      </div>
      
    
  </div>

  <div class="tab-content tab-panel-excluded">
    
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.19758v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19758v1">AI-Powered Conflict Management in Open RAN: Detection, Classification, and Mitigation</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.NI</span>
          
        </div>
        
        
        <div class="compact-reason">Open RAN 네트워크 갈등 관리 기술은 프로젝트 주제와 무관함.</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.19834v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19834v1">Placing Green Bridges Optimally for Robust Habitat Reconnection</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.DS</span>
          
          <span class="cat-tag">cs.DM</span>
          
        </div>
        
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.20014v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20014v1">Protecting and Promoting Human Agency in Education in the Age of Artificial Intelligence</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.HC</span>
          
        </div>
        
        
        <div class="compact-reason">Unrelated to sports or video analysis, focuses on AI in education.</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.19690v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19690v1">&#34;The explanation makes sense&#34;: An Empirical Study on LLM Performance in News Classification and its Influence on Judgment in Human-AI Collaborative Annotation</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.HC</span>
          
        </div>
        
        
        <div class="compact-reason">Unrelated to sports video analysis or edge AI devices</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.19785v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19785v1">Unsupervised Anomaly Detection in NSL-KDD Using $β$-VAE: A Latent Space and Reconstruction Error Approach</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.NE</span>
          
          <span class="cat-tag">stat.ML</span>
          
        </div>
        
        
        <div class="compact-reason">Unrelated to sports, video processing, or edge deployment</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.19831v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.19831v1">An Explainable Memory Forensics Approach for Malware Analysis</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CR</span>
          
        </div>
        
        
        <div class="compact-reason">악성코드 분석은 프로젝트와 무관함.</div>
        
      </div>
      
    
  </div>
</div>

<div class="download-toolbar" id="downloadToolbar">
  <div class="toolbar-selects">
    <button class="toolbar-btn" onclick="toggleSelect('all')">All</button>
    <button class="toolbar-btn" onclick="toggleSelect('tier1')">Tier 1</button>
    <button class="toolbar-btn" onclick="toggleSelect('tier2')">Tier 2</button>
    <button class="toolbar-btn" onclick="toggleSelect('none')">Deselect</button>
  </div>
  <button class="toolbar-btn primary" id="downloadBtn" onclick="downloadSelected()">Download 0 PDFs</button>
</div>

<footer>
  <p>이 리포트는 arXiv API를 사용하여 생성되었습니다.</p>
  <p>arXiv 논문의 저작권은 각 저자에게 있습니다.</p>
  <p>Thank you to arXiv for use of its open access interoperability.</p>
  <p>run_id: 20 | embedding: en_synthetic
    | weights: {&#39;embed&#39;: 0.35, &#39;llm&#39;: 0.55, &#39;recency&#39;: 0.1}</p>
</footer>

<script>
var READ_KEY = 'paperscout_read';
var SUPABASE_CFG = {
  enabled: true,
  url: 'https://qcvlnebvjzkgbbxainsk.supabase.co',
  key: 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InFjdmxuZWJ2anprZ2JieGFpbnNrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NzE0ODcyNTgsImV4cCI6MjA4NzA2MzI1OH0.Z53_RbpMnN3U6eyMlIxq-4h16hhvhawDzjKDb_iSNvI'
};

/* localStorage helpers */
function getLocalRead() {
  try { return JSON.parse(localStorage.getItem(READ_KEY) || '[]'); }
  catch(e) { return []; }
}
function saveLocalRead(arr) {
  try { localStorage.setItem(READ_KEY, JSON.stringify(arr)); } catch(e) {}
}

/* Supabase helpers */
function supabaseHeaders() {
  return {
    'apikey': SUPABASE_CFG.key,
    'Authorization': 'Bearer ' + SUPABASE_CFG.key,
    'Content-Type': 'application/json',
    'Prefer': 'return=minimal'
  };
}

function fetchReadFromSupabase() {
  if (!SUPABASE_CFG.enabled) return Promise.resolve([]);
  return fetch(SUPABASE_CFG.url + '/rest/v1/read_papers?select=paper_url', {
    headers: supabaseHeaders()
  })
  .then(function(r) { return r.ok ? r.json() : []; })
  .then(function(rows) { return rows.map(function(r) { return r.paper_url; }); })
  .catch(function() { return []; });
}

function markReadOnSupabase(url) {
  if (!SUPABASE_CFG.enabled || !url) return;
  fetch(SUPABASE_CFG.url + '/rest/v1/read_papers', {
    method: 'POST',
    headers: Object.assign({}, supabaseHeaders(), {'Prefer': 'return=minimal,resolution=ignore-duplicates'}),
    body: JSON.stringify({ paper_url: url })
  }).catch(function() {});
}

/* Unified read state */
var _readCache = null;

function getReadPapers() {
  if (_readCache) return _readCache;
  _readCache = getLocalRead();
  return _readCache;
}

function markAsRead(url) {
  if (!url) return;
  var read = getReadPapers();
  if (read.indexOf(url) === -1) {
    read.push(url);
    _readCache = read;
    saveLocalRead(read);
  }
  markReadOnSupabase(url);
}

function isRead(url) {
  return getReadPapers().indexOf(url) !== -1;
}

function applyReadState() {
  var cards = document.querySelectorAll('[data-paper-url]');
  var remindHidden = 0;
  cards.forEach(function(card) {
    var url = card.dataset.paperUrl;
    if (!isRead(url)) return;
    // Remind tab: hide read papers entirely
    if (card.classList.contains('compact-card--remind')) {
      card.style.display = 'none';
      remindHidden++;
    } else {
      card.classList.add('paper-read');
    }
  });
  // Update remind tab count
  var remindLabel = document.querySelector('label[for="tab-remind"]');
  if (remindLabel) {
    var total = document.querySelectorAll('.compact-card--remind').length;
    var visible = total - remindHidden;
    remindLabel.textContent = '\uB2E4\uC2DC \uBCF4\uAE30 (' + visible + ')';
  }
  // Show empty message if all remind papers are hidden
  var remindPanel = document.querySelector('.tab-panel-remind');
  if (remindPanel) {
    var visibleCards = remindPanel.querySelectorAll('.compact-card--remind:not([style*="display: none"])');
    var emptyMsg = remindPanel.querySelector('.empty-message');
    if (visibleCards.length === 0 && !emptyMsg) {
      var msg = document.createElement('div');
      msg.className = 'empty-message';
      msg.textContent = '\uB2E4\uC2DC \uBCF4\uAE30 \uB17C\uBB38\uC774 \uC5C6\uC2B5\uB2C8\uB2E4.';
      remindPanel.appendChild(msg);
    }
  }
}

function trackClicks() {
  document.addEventListener('click', function(e) {
    var link = e.target.closest('a[href]');
    if (!link) return;
    var card = link.closest('[data-paper-url]');
    if (!card) return;
    markAsRead(card.dataset.paperUrl);
    card.classList.add('paper-read');
    // If remind card, hide after brief delay
    if (card.classList.contains('compact-card--remind')) {
      setTimeout(function() {
        card.style.display = 'none';
        applyReadState();
      }, 200);
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  // Apply localStorage state immediately for fast UX
  applyReadState();
  trackClicks();
  // Then merge Supabase data for cross-device sync
  fetchReadFromSupabase().then(function(remote) {
    if (!remote || remote.length === 0) return;
    var local = getLocalRead();
    var merged = local.slice();
    var changed = false;
    remote.forEach(function(url) {
      if (merged.indexOf(url) === -1) {
        merged.push(url);
        changed = true;
      }
    });
    if (changed) {
      _readCache = merged;
      saveLocalRead(merged);
      applyReadState();
    }
  });
});

function showAllTier1() {
  document.querySelectorAll('.paper-card--collapsed').forEach(function(el) {
    el.classList.remove('paper-card--collapsed');
  });
  var btn = document.getElementById('showMoreBtn');
  if (btn) btn.style.display = 'none';
}

function toggleSelect(filter) {
  var boxes = document.querySelectorAll('.tab-panel-today .paper-checkbox');
  boxes.forEach(function(cb) {
    if (filter === 'none') { cb.checked = false; }
    else if (filter === 'all') { cb.checked = true; }
    else if (filter === 'tier1') { cb.checked = cb.dataset.tier === '1'; }
    else if (filter === 'tier2') { cb.checked = cb.dataset.tier === '2'; }
  });
  updateToolbar();
}

function updateToolbar() {
  var checked = document.querySelectorAll('.paper-checkbox:checked');
  var toolbar = document.getElementById('downloadToolbar');
  var btn = document.getElementById('downloadBtn');
  var count = 0;
  checked.forEach(function(cb) { if (cb.dataset.pdf) count++; });
  if (checked.length > 0) {
    toolbar.classList.add('visible');
    document.body.classList.add('toolbar-active');
    btn.textContent = 'Download ' + count + ' PDFs';
    btn.disabled = (count === 0);
  } else {
    toolbar.classList.remove('visible');
    document.body.classList.remove('toolbar-active');
  }
}

function downloadSelected() {
  var checked = document.querySelectorAll('.paper-checkbox:checked');
  var urls = [];
  checked.forEach(function(cb) { if (cb.dataset.pdf) urls.push(cb.dataset.pdf); });
  if (urls.length === 0) return;
  urls.forEach(function(url, i) {
    setTimeout(function() { window.open(url, '_blank'); }, i * 300);
  });
}
</script>

</body>
</html>
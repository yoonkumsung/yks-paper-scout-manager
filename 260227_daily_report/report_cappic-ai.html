<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CAPP!C_AI 논문 리포트 (2026-02-27)</title>
  <style>
    :root {
      --bg: #ffffff;
      --fg: #1a1a2e;
      --card-bg: #f8f9fa;
      --card-border: #e0e0e0;
      --accent: #2563eb;
      --accent-light: #dbeafe;
      --muted: #6b7280;
      --divider: #e5e7eb;
      --score-bg: #e5e7eb;
      --score-fill: #2563eb;
      --tier2-bg: #f3f4f6;
      --tag-bg: #fef3c7;
      --tag-fg: #92400e;
      --flag-edge: #dbeafe;
      --flag-realtime: #dcfce7;
      --flag-code: #f3e8ff;
      --remind-bg: #fffbeb;
    }

    @media (prefers-color-scheme: dark) {
      :root {
        --bg: #0f172a;
        --fg: #e2e8f0;
        --card-bg: #1e293b;
        --card-border: #334155;
        --accent: #60a5fa;
        --accent-light: #1e3a5f;
        --muted: #94a3b8;
        --divider: #334155;
        --score-bg: #334155;
        --score-fill: #60a5fa;
        --tier2-bg: #1e293b;
        --tag-bg: #78350f;
        --tag-fg: #fef3c7;
        --flag-edge: #1e3a5f;
        --flag-realtime: #14532d;
        --flag-code: #3b0764;
        --remind-bg: #451a03;
      }
    }

    * { box-sizing: border-box; margin: 0; padding: 0; }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: var(--bg);
      color: var(--fg);
      line-height: 1.6;
      max-width: 900px;
      margin: 0 auto;
      padding: 1rem;
    }

    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }

    .header { margin-bottom: 1.5rem; }
    .header h1 { font-size: 1.4rem; margin-bottom: 0.5rem; }
    .stats {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      font-size: 0.85rem;
      color: var(--muted);
      margin-bottom: 0.5rem;
    }
    .stats span { white-space: nowrap; }
    .stats span::after { content: ""; }
    .stats-arrow { color: var(--muted); font-size: 0.75rem; opacity: 0.6; }
    .below-score { color: #f59e0b; }
    .window-info {
      font-size: 0.8rem;
      color: var(--muted);
      background: var(--card-bg);
      border: 1px solid var(--card-border);
      border-radius: 4px;
      padding: 0.4rem 0.6rem;
      margin-top: 0.3rem;
      line-height: 1.6;
      display: flex;
      flex-wrap: wrap;
      align-items: center;
      gap: 0.3rem;
    }
    .window-label {
      font-weight: 600;
      color: var(--fg);
      margin-right: 0.2rem;
    }
    .window-range {
      font-family: 'SF Mono', 'Consolas', 'Monaco', monospace;
      font-size: 0.75rem;
      background: var(--bg);
      border: 1px solid var(--divider);
      border-radius: 3px;
      padding: 0.1rem 0.4rem;
    }
    .window-sep {
      color: var(--muted);
      font-weight: 600;
    }

    details { margin-bottom: 1rem; }
    details summary {
      cursor: pointer;
      font-weight: 600;
      padding: 0.5rem;
      background: var(--card-bg);
      border: 1px solid var(--card-border);
      border-radius: 4px;
      list-style: none;
      display: flex;
      align-items: center;
      gap: 0.4rem;
    }
    details summary::-webkit-details-marker { display: none; }
    details summary::before {
      content: '\25B6';
      font-size: 0.6em;
      transition: transform 0.2s;
      display: inline-block;
    }
    details[open] > summary::before { transform: rotate(90deg); }
    details[open] summary { border-radius: 4px 4px 0 0; }
    details .details-content {
      padding: 0.75rem;
      border: 1px solid var(--card-border);
      border-top: none;
      border-radius: 0 0 4px 4px;
    }

    .tabs {
      margin-bottom: 1rem;
    }
    .tabs input[type="radio"] { display: none; }
    .tab-labels {
      display: flex;
      border-bottom: 2px solid var(--divider);
      margin-bottom: 1rem;
    }
    .tab-labels label {
      padding: 0.6rem 1rem;
      min-height: 44px;
      display: inline-flex;
      align-items: center;
      cursor: pointer;
      font-weight: 600;
      color: var(--muted);
      border-bottom: 2px solid transparent;
      margin-bottom: -2px;
      transition: color 0.2s, border-color 0.2s;
    }
    .tab-labels label:hover { color: var(--fg); }
    .tab-content { display: none; }
    #tab-today:checked ~ .tab-labels label[for="tab-today"],
    #tab-remind:checked ~ .tab-labels label[for="tab-remind"],
    #tab-below:checked ~ .tab-labels label[for="tab-below"],
    #tab-excluded:checked ~ .tab-labels label[for="tab-excluded"] {
      color: var(--accent);
      border-bottom-color: var(--accent);
    }
    #tab-today:checked ~ .tab-panel-today { display: block; }
    #tab-remind:checked ~ .tab-panel-remind { display: block; }
    #tab-below:checked ~ .tab-panel-below { display: block; }
    #tab-excluded:checked ~ .tab-panel-excluded { display: block; }

    .paper-card {
      background: var(--card-bg);
      border: 1px solid var(--card-border);
      border-radius: 8px;
      padding: 1rem;
      margin-bottom: 1rem;
    }
    .paper-card--collapsed { display: none; }
    .show-more-btn {
      display: block;
      width: 100%;
      padding: 0.6rem;
      margin-bottom: 1rem;
      background: var(--card-bg);
      border: 1px dashed var(--card-border);
      border-radius: 8px;
      color: var(--accent);
      font-size: 0.85rem;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.2s;
    }
    .show-more-btn:hover { background: var(--accent-light); }
    .paper-card .rank-title {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      margin-bottom: 0.5rem;
    }
    .paper-card .rank {
      font-size: 1.1rem;
      font-weight: 700;
      color: var(--accent);
      white-space: nowrap;
      flex-shrink: 0;
      min-width: 2.5em;
      text-align: center;
    }
    .paper-card .paper-title {
      font-size: 1rem;
      font-weight: 600;
      flex: 1;
      min-width: 0;
    }
    .paper-card .paper-title a {
      color: var(--fg);
      display: block;
      overflow: hidden;
      text-overflow: ellipsis;
      white-space: nowrap;
    }
    .paper-card .paper-title a:hover { color: var(--accent); }

    .score-bar-container {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      margin: 0.5rem 0;
      font-size: 0.8rem;
    }
    .score-bar {
      flex: 1;
      max-width: 200px;
      height: 8px;
      background: var(--score-bg);
      border-radius: 4px;
      overflow: hidden;
    }
    .score-bar-fill {
      height: 100%;
      max-width: 100%;
      background: var(--score-fill);
      border-radius: 4px;
    }
    .score-detail { color: var(--muted); font-size: 0.75rem; }

    .categories {
      display: flex;
      flex-wrap: wrap;
      gap: 0.3rem;
      margin: 0.4rem 0;
    }
    .cat-tag {
      font-size: 0.7rem;
      padding: 0.1rem 0.4rem;
      border-radius: 3px;
      background: var(--accent-light);
      color: var(--accent);
    }

    .flags {
      display: flex;
      gap: 0.3rem;
      margin: 0.4rem 0;
    }
    .flag-tag {
      font-size: 0.7rem;
      padding: 0.1rem 0.4rem;
      border-radius: 3px;
      font-weight: 500;
    }
    .flag-edge { background: var(--flag-edge); }
    .flag-realtime { background: var(--flag-realtime); }
    .flag-code { background: var(--flag-code); }

    .lowered-tag {
      font-size: 0.7rem;
      padding: 0.1rem 0.4rem;
      border-radius: 3px;
      background: var(--tag-bg);
      color: var(--tag-fg);
      font-weight: 600;
    }

    .paper-links {
      display: flex;
      gap: 0.5rem;
      margin: 0.5rem 0;
      font-size: 0.8rem;
    }
    .paper-links a {
      display: inline-flex;
      align-items: center;
      gap: 0.2rem;
      padding: 0.2rem 0.5rem;
      border: 1px solid var(--accent);
      border-radius: 3px;
      font-weight: 500;
    }
    .paper-links a:hover {
      background: var(--accent);
      color: #fff;
      text-decoration: none;
    }

    .abstract-section {
      margin: 0.5rem 0;
      font-size: 0.85rem;
      color: var(--muted);
      line-height: 1.5;
    }
    .abstract-section .abstract-full {
      margin-top: 0.3rem;
      margin-bottom: 0;
    }
    .abstract-section .abstract-full summary {
      font-size: 0.75rem;
      font-weight: 500;
      padding: 0.2rem 0.4rem;
      background: transparent;
      border: 1px solid var(--card-border);
      display: inline-flex;
    }

    .cluster-links {
      font-size: 0.8rem;
      color: var(--muted);
      margin-top: 0.5rem;
    }

    .tier-divider {
      text-align: center;
      margin: 1.5rem 0;
      color: var(--muted);
      font-size: 0.8rem;
      font-weight: 600;
      position: relative;
      letter-spacing: 0.05em;
      text-transform: uppercase;
    }
    .tier-divider::before, .tier-divider::after {
      content: "";
      position: absolute;
      top: 50%;
      width: 35%;
      border-top: 1px solid var(--divider);
    }
    .tier-divider::before { left: 0; }
    .tier-divider::after { right: 0; }

    /* Unified compact card - used by Tier 2, remind, below-threshold, discarded */
    .compact-card {
      background: var(--tier2-bg);
      border: 1px solid var(--card-border);
      border-radius: 6px;
      padding: 0.6rem 0.8rem;
      margin-bottom: 0.5rem;
      font-size: 0.85rem;
    }
    .compact-card .rank-title {
      display: flex;
      align-items: center;
      gap: 0.4rem;
      flex-wrap: nowrap;
    }
    .compact-card .rank {
      font-weight: 700;
      color: var(--accent);
      white-space: nowrap;
      flex-shrink: 0;
      min-width: 2.5em;
      text-align: center;
    }
    .compact-card .paper-title {
      font-weight: 600;
      flex: 1;
      min-width: 0;
      overflow: hidden;
      text-overflow: ellipsis;
      white-space: nowrap;
    }
    .compact-card .paper-title a {
      color: var(--fg);
    }
    .compact-card .paper-title a:hover {
      color: var(--accent);
      text-decoration: none;
    }
    .compact-meta {
      display: flex;
      align-items: center;
      gap: 0.4rem;
      flex-shrink: 0;
      white-space: nowrap;
    }
    .compact-link {
      font-size: 0.75rem;
      padding: 0.1rem 0.3rem;
      border: 1px solid var(--accent);
      border-radius: 3px;
    }
    .compact-body {
      margin-top: 0.3rem;
      margin-left: 0.4rem;
      font-size: 0.8rem;
      color: var(--muted);
      line-height: 1.4;
    }
    .compact-reason {
      margin-top: 0.2rem;
      margin-left: 0.4rem;
      padding-left: 0.4rem;
      font-size: 0.78rem;
      color: var(--accent);
      line-height: 1.3;
      border-left: 3px solid var(--accent);
    }
    .compact-card .categories {
      margin-top: 0.3rem;
      margin-left: 0.4rem;
    }
    .recommend-count {
      font-size: 0.75rem;
      color: var(--muted);
      font-weight: 600;
    }
    /* Modifier: remind */
    .compact-card--remind { background: var(--remind-bg); }
    /* Modifier: below threshold */
    .compact-card--below { opacity: 0.75; }
    .compact-card--below .score-detail { color: #f59e0b; font-weight: 600; }
    /* Modifier: discarded */
    .compact-card--discarded { opacity: 0.7; }

    /* Read state - Gmail-style dimming */
    .paper-read { opacity: 0.5; }
    .paper-read .paper-title a { font-weight: 400; }

    .empty-message {
      text-align: center;
      padding: 3rem 1rem;
      color: var(--muted);
      font-size: 1rem;
    }

    footer {
      margin-top: 2rem;
      padding-top: 1rem;
      border-top: 1px solid var(--divider);
      font-size: 0.75rem;
      color: var(--muted);
    }
    footer p { margin-bottom: 0.3rem; }

    .index-list { list-style: none; }
    .index-list li {
      padding: 0.5rem 0;
      border-bottom: 1px solid var(--divider);
    }
    .index-list li:last-child { border-bottom: none; }

    /* Paper checkbox - matches rank text height */
    .paper-checkbox {
      width: 1em;
      height: 1em;
      accent-color: var(--accent);
      cursor: pointer;
      flex-shrink: 0;
      margin: 0;
    }
    .compact-card .paper-links {
      display: inline-flex;
      gap: 0.4rem;
      margin: 0;
      font-size: 0.8rem;
    }

    /* Download toolbar */
    .download-toolbar {
      position: fixed;
      bottom: 0;
      left: 50%;
      transform: translateX(-50%);
      width: 100%;
      max-width: 900px;
      background: var(--card-bg);
      border-top: 1px solid var(--card-border);
      padding: 0.6rem 1rem;
      display: none;
      align-items: center;
      justify-content: space-between;
      gap: 0.5rem;
      z-index: 100;
      box-shadow: 0 -2px 8px rgba(0,0,0,0.15);
    }
    .download-toolbar.visible { display: flex; }
    body.toolbar-active { padding-bottom: 3.5rem; }
    .toolbar-selects {
      display: flex;
      gap: 0.3rem;
      flex-wrap: wrap;
    }
    .toolbar-btn {
      padding: 0.3rem 0.7rem;
      border: 1px solid var(--card-border);
      border-radius: 4px;
      background: var(--bg);
      color: var(--fg);
      font-size: 0.8rem;
      cursor: pointer;
      white-space: nowrap;
    }
    .toolbar-btn:hover { border-color: var(--accent); color: var(--accent); }
    .toolbar-btn.primary {
      background: var(--accent);
      color: #fff;
      border-color: var(--accent);
    }
    .toolbar-btn.primary:hover { opacity: 0.9; }
    .toolbar-btn.primary:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }
  </style>
</head>
<body>
  
<div class="header">
  <h1>CAPP!C_AI 논문 리포트 (2026-02-27)</h1>
  <div class="stats">
    <span title="arXiv에서 수집된 총 논문 수">수집 68편</span>
    <span class="stats-arrow">&rarr;</span>
    <span title="키워드 필터 통과">필터 67편</span>
    <span class="stats-arrow">&rarr;</span>
    <span title="LLM 평가 후 제외">제외 10편</span>
    <span class="stats-arrow">&rarr;</span>
    <span title="점수 50점 미만">미달 7편</span>
    <span class="stats-arrow">&rarr;</span>
    <span title="최종 선정된 논문">선정 50편</span>
  </div>
  <div class="window-info">
    <span class="window-label">검색 윈도우</span>
    <span class="window-range">UTC 2026-02-26 00:00 / KST 2026-02-26 09:00</span>
    <span class="window-sep">~</span>
    <span class="window-range">UTC 2026-02-27 00:30 / KST 2026-02-27 09:30</span>
  </div>
</div>


<details>
  <summary>검색 키워드</summary>
  <div class="details-content">
    autonomous cinematography, sports tracking, camera control, highlight detection, action recognition, keyframe extraction, video stabilization, image enhancement, color correction, pose estimation, biomechanics, tactical analysis, short video, content summarization, video editing, edge computing, embedded vision, real-time processing, content sharing, social platform, advertising system, biomechanics, tactical analysis, embedded vision
  </div>
</details>


<div class="tabs">
  <input type="radio" name="tabs" id="tab-today" checked>
  <input type="radio" name="tabs" id="tab-remind">
  <input type="radio" name="tabs" id="tab-below">
  <input type="radio" name="tabs" id="tab-excluded">
  <div class="tab-labels">
    <label for="tab-today">오늘의 논문 (50)</label>
    <label for="tab-remind">다시 보기 (10)</label>
    <label for="tab-below">점수 미달 (0)</label>
    <label for="tab-excluded">제외 (10)</label>
  </div>

  <div class="tab-content tab-panel-today">
    
      
      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.23141v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23141v1" onchange="updateToolbar()">
          <span class="rank">1위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23141v1">No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>100.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 100.0%"></div>
          </div>
          <span class="score-detail">
            base:92 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 실시간 골격 그래프 구성 및 효율적 공간 추론 기술이 스포츠 장면 추적에 적합</p>
        

        
        <p><strong>활용 인사이트:</strong> 계층적 온디맨드 전략을 적용해 선수 위치 추적 및 경기 전략 분석에 활용 가능</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23141v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.22707v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22707v1" onchange="updateToolbar()">
          <span class="rank">2위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22707v1">SCOPE: Skeleton Graph-Based Computation-Efficient Framework for Autonomous UAV Exploration</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>96.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 96.0%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Autonomous exploration in unknown environments is key for mobile robots, helping them perceive, map, and make decisions in complex areas. However, current methods often rely on frequent global optimization, suffering from high computational latency and trajectory oscillation, especially on resource-constrained edge devices.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Autonomous exploration in unknown environments is key for mobile robots, helping them perceive, map, and make decisions in complex areas. However, current methods often rely on frequent global optimization, suffering from high computational latency and trajectory oscillation, especially on resource-constrained edge devices. To address these limitations, we propose SCOPE, a novel framework that incrementally constructs a real-time skeletal graph and introduces Implicit Unknown Region Analysis for efficient spatial reasoning. The planning layer adopts a hierarchical on-demand strategy: the Proximal Planner generates smooth, high-frequency local trajectories, while the Region-Sequence Planner is activated only when necessary to optimize global visitation order. Comparative evaluations in simulation demonstrate that SCOPE achieves competitive exploration performance comparable to state-of-the-art global planners, while reducing computational cost by an average of 86.9%. Real-world experiments further validate the system&#39;s robustness and low latency in practical scenarios.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> U-Net 아키텍처와 생성적 방법이 비디오/이미지 처리에 적용 가능</p>
        

        
        <p><strong>활용 인사이트:</strong> 다중 스케일 특성 융합으로 영상 품질 향상 및 실시간 보정에 효과적</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22707v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.22691v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22691v1" onchange="updateToolbar()">
          <span class="rank">3위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22691v1">U-Net-Based Generative Joint Source-Channel Coding for Wireless Image Transmission</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>96.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 96.0%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">eess.IV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Deep learning (DL)-based joint source-channel coding (JSCC) methods have achieved remarkable success in wireless image transmission. However, these methods either focus on conventional distortion metrics that do not necessarily yield high perceptual quality or incur high computational complexity.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Deep learning (DL)-based joint source-channel coding (JSCC) methods have achieved remarkable success in wireless image transmission. However, these methods either focus on conventional distortion metrics that do not necessarily yield high perceptual quality or incur high computational complexity. In this paper, we propose two DL-based JSCC (DeepJSCC) methods that leverage deep generative architectures for wireless image transmission. Specifically, we propose G-UNet-JSCC, a scheme comprising an encoder and a U-Net-based generator serving as the decoder. Its skip connections enable multi-scale feature fusion to improve both pixel-level fidelity and perceptual quality of reconstructed images by integrating low- and high-level features. To further enhance pixel-level fidelity, the encoder and the U-Net-based decoder are jointly optimized using a weighted sum of structural similarity and mean-squared error (MSE) losses. Building upon G-UNet-JSCC, we further develop a DeepJSCC method called cGAN-JSCC, where the decoder is enhanced through adversarial training. In this scheme, we retain the encoder of G-UNet-JSCC and adversarially train the decoder&#39;s generator against a patch-based discriminator. cGAN-JSCC employs a two-stage training procedure. The outer stage trains the encoder and the decoder end-to-end using an MSE loss, while the inner stage adversarially trains the decoder&#39;s generator and the discriminator by minimizing a joint loss combining adversarial and distortion losses. Simulation results demonstrate that the proposed methods achieve superior pixel-level fidelity and perceptual quality on both high- and low-resolution images. For low-resolution images, cGAN-JSCC achieves better reconstruction performance and greater robustness to channel variations than G-UNet-JSCC.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> U-Net 아키텍처와 생성적 방법이 비디오/이미지 처리에 적용 가능</p>
        

        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22691v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.23224v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23224v1" onchange="updateToolbar()">
          <span class="rank">4위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23224v1">UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>94.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 94.4%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+8
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image sequences is critical for downstream tasks.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image sequences is critical for downstream tasks. UniScale addresses this challenge with a single feed-forward network that jointly estimates camera intrinsics and extrinsics, scale-invariant depth and point maps, and the metric scale of a scene from multi-view images, while optionally incorporating auxiliary geometric priors when available. By combining global contextual reasoning with camera-aware feature representations, UniScale is able to recover the metric-scale of the scene. In robotic settings where camera intrinsics are known, they can be easily incorporated to improve performance, with additional gains obtained when camera poses are also available. This co-design enables robust, metric-aware 3D reconstruction within a single unified model. Importantly, UniScale does not require training from scratch, and leverages world priors exhibited in pre-existing models without geometric encoding strategies, making it particularly suitable for resource-constrained robotic teams. We evaluate UniScale on multiple benchmarks, demonstrating strong generalization and consistent performance across diverse environments. We will release our implementation upon acceptance.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 자원 제약이 있는 엣지 디바이스에서 스포츠 촬영을 위한 3D 재구성 기술</p>
        

        
        <p><strong>활용 인사이트:</strong> 다중 뷰 이미지로부터 장치의 내부/외부 파라미터와 깊이 맵을 동시 추출해 정확한 3D 모델링</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23224v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.23288v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23288v1" onchange="updateToolbar()">
          <span class="rank">5위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23288v1">BRIDGE: Borderless Reconfiguration for Inclusive and Diverse Gameplay Experience via Embodiment Transformation</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>93.6</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 93.6%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.HC</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Training resources for parasports are limited, reducing opportunities for athletes and coaches to engage with sport-specific movements and tactical coordination. To address this gap, we developed BRIDGE, a system that integrates a reconstruction pipeline, which detects and tracks players from broadcast video to generate 3D play sequences, with an embodiment-aware visualization framework that decomposes head, trunk, and wheelchair base orientations to represent attention, intent, and mobility.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Training resources for parasports are limited, reducing opportunities for athletes and coaches to engage with sport-specific movements and tactical coordination. To address this gap, we developed BRIDGE, a system that integrates a reconstruction pipeline, which detects and tracks players from broadcast video to generate 3D play sequences, with an embodiment-aware visualization framework that decomposes head, trunk, and wheelchair base orientations to represent attention, intent, and mobility. We evaluated BRIDGE in two controlled studies with 20 participants (10 national wheelchair basketball team players and 10 amateur players). The results showed that BRIDGE significantly enhanced the perceived naturalness of player postures and made tactical intentions easier to understand. In addition, it supported functional classification by realistically conveying players&#39; capabilities, which in turn improved participants&#39; sense of self-efficacy. This work advances inclusive sports learning and accessible coaching practices, contributing to more equitable access to tactical resources in parasports.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 스포츠 비디오 분석과 전략 이해를 위한 3D 재구성 시스템</p>
        

        
        <p><strong>활용 인사이트:</strong> 선수들의 움직임을 3D 시퀀스로 변환해 자세와 전략 분석을 통해 하이라이트 영상 생성</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23288v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.23228v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23228v1" onchange="updateToolbar()">
          <span class="rank">6위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23228v1">MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>93.6</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 93.6%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents a significant challenge for existing Vision-Language Models (VLMs).</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents a significant challenge for existing Vision-Language Models (VLMs). While proficient at single-image captioning, these general-purpose models often exhibit critical failures in long-duration contexts, primarily a lack of ID-consistent character identification and a fractured narrative coherence. To overcome these limitations, we propose MovieTeller, a novel framework for generating movie synopses via tool-augmented progressive abstraction. Our core contribution is a training-free, tool-augmented, fact-grounded generation process. Instead of requiring costly model fine-tuning, our framework directly leverages off-the-shelf models in a plug-and-play manner. We first invoke a specialized face recognition model as an external &#34;tool&#34; to establish Factual Groundings--precise character identities and their corresponding bounding boxes. These groundings are then injected into the prompt to steer the VLM&#39;s reasoning, ensuring the generated scene descriptions are anchored to verifiable facts. Furthermore, our progressive abstraction pipeline decomposes the summarization of a full-length movie into a multi-stage process, effectively mitigating the context length limitations of current VLMs. Experiments demonstrate that our approach yields significant improvements in factual accuracy, character consistency, and overall narrative coherence compared to end-to-end baselines.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 비디오 요약 및 캐릭터 일관성 유지 기술이 스포츠 하이라이트 자동 생성에 적용 가능</p>
        

        
        <p><strong>활용 인사이트:</strong> MovieTeller의 툴 증강 프로그레시브 추상화 기법을 스포츠 영상 요약에 적용하여 선수별 주요 장면 자동 생성 가능</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23228v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22941v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22941v1" onchange="updateToolbar()">
          <span class="rank">7위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22941v1">Velocity and stroke rate reconstruction of canoe sprint team boats based on panned and zoomed video recordings</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>92.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 92.0%"></div>
          </div>
          <span class="score-detail">
            base:90 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Pacing strategies, defined by velocity and stroke rate profiles, are essential for peak performance in canoe sprint. While GPS is the gold standard for analysis, its limited availability necessitates automated video-based solutions.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Pacing strategies, defined by velocity and stroke rate profiles, are essential for peak performance in canoe sprint. While GPS is the gold standard for analysis, its limited availability necessitates automated video-based solutions. This paper presents an extended framework for reconstructing performance metrics from panned and zoomed video recordings across all sprint disciplines (K1-K4, C1-C2) and distances (200m-500m). Our method utilizes YOLOv8 for buoy and athlete detection, leveraging the known buoy grid to estimate homographies. We generalized the estimation of the boat position by means of learning a boat-specific athlete offset using a U-net based boat tip calibration. Further, we implement a robust tracking scheme using optical flow to adapt to multi-athlete boat types. Finally, we introduce methods to extract stroke rate information from either pose estimations or the athlete bounding boxes themselves. Evaluation against GPS data from elite competitions yields a velocity RRMSE of 0.020 +- 0.011 (rho = 0.956) and a stroke rate RRMSE of 0.022 +- 0.024 (rho = 0.932). The methods provide coaches with highly accurate, automated feedback without requiring on-boat sensors or manual annotation.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 스포츠 비디오 분석을 위한 컴퓨터 비전 기술로 프로젝트와 직접 관련 있으나 특정 스포츠(카누)에 국한됨</p>
        

        
        <p><strong>활용 인사이트:</strong> YOLOv8과 광학 흐름 기반 추적 기술을 다양한 스포츠로 확장하여 선수 속도 및 동작 패턴 분석 가능</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22941v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.23069v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23069v1" onchange="updateToolbar()">
          <span class="rank">8위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23069v1">Align then Adapt: Rethinking Parameter-Efficient Transfer Learning in 4D Perception</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>92.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 92.0%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+8
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Point cloud video understanding is critical for robotics as it accurately encodes motion and scene interaction. We recognize that 4D datasets are far scarcer than 3D ones, which hampers the scalability of self-supervised 4D models.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Point cloud video understanding is critical for robotics as it accurately encodes motion and scene interaction. We recognize that 4D datasets are far scarcer than 3D ones, which hampers the scalability of self-supervised 4D models. A promising alternative is to transfer 3D pre-trained models to 4D perception tasks. However, rigorous empirical analysis reveals two critical limitations that impede transfer capability: overfitting and the modality gap. To overcome these challenges, we develop a novel &#34;Align then Adapt&#34; (PointATA) paradigm that decomposes parameter-efficient transfer learning into two sequential stages. Optimal-transport theory is employed to quantify the distributional discrepancy between 3D and 4D datasets, enabling our proposed point align embedder to be trained in Stage 1 to alleviate the underlying modality gap. To mitigate overfitting, an efficient point-video adapter and a spatial-context encoder are integrated into the frozen 3D backbone to enhance temporal modeling capacity in Stage 2. Notably, with the above engineering-oriented designs, PointATA enables a pre-trained 3D model without temporal knowledge to reason about dynamic video content at a smaller parameter cost compared to previous work. Extensive experiments show that PointATA can match or even outperform strong full fine-tuning models, whilst enjoying the advantage of parameter efficiency, e.g. 97.21 \% accuracy on 3D action recognition, $+8.7 \%$ on 4 D action segmentation, and 84.06\% on 4D semantic segmentation.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 4D perception technology with parameter-efficient transfer learning applicable to edge devices for sports video analysis</p>
        

        
        <p><strong>활용 인사이트:</strong> 3D 모델을 4D 스포츠 영상 분석으로 전이 학습하여 에지 디바이스에서 실시간 동작 분석 가능</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23069v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22960v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22960v1" onchange="updateToolbar()">
          <span class="rank">9위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22960v1">UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>90.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 90.4%"></div>
          </div>
          <span class="score-detail">
            base:78 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 카메라 제어 및 일관성 유지 기술이 스포츠 경기 자동 촬영에 활용 가능</p>
        

        
        <p><strong>활용 인사이트:</strong> UCM의 시간 인코딩 왜핑 메커니즘을 활용해 스포츠 장면의 일관성 유지 및 카메라 제어 정밀도 향상</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22960v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.23169v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23169v1" onchange="updateToolbar()">
          <span class="rank">10위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23169v1">Learning Continuous Wasserstein Barycenter Space for Generalized All-in-One Image Restoration</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>89.6</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 89.6%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+5
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Despite substantial advances in all-in-one image restoration for addressing diverse degradations within a unified model, existing methods remain vulnerable to out-of-distribution degradations, thereby limiting their generalization in real-world scenarios. To tackle the challenge, this work is motivated by the intuition that multisource degraded feature distributions are induced by different degradation-specific shifts from an underlying degradation-agnostic distribution, and recovering such a shared distribution is thus crucial for achieving generalization across degradations.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Despite substantial advances in all-in-one image restoration for addressing diverse degradations within a unified model, existing methods remain vulnerable to out-of-distribution degradations, thereby limiting their generalization in real-world scenarios. To tackle the challenge, this work is motivated by the intuition that multisource degraded feature distributions are induced by different degradation-specific shifts from an underlying degradation-agnostic distribution, and recovering such a shared distribution is thus crucial for achieving generalization across degradations. With this insight, we propose BaryIR, a representation learning framework that aligns multisource degraded features in the Wasserstein barycenter (WB) space, which models a degradation-agnostic distribution by minimizing the average of Wasserstein distances to multisource degraded distributions. We further introduce residual subspaces, whose embeddings are mutually contrasted while remaining orthogonal to the WB embeddings. Consequently, BaryIR explicitly decouples two orthogonal spaces: a WB space that encodes the degradation-agnostic invariant contents shared across degradations, and residual subspaces that adaptively preserve the degradation-specific knowledge. This disentanglement mitigates overfitting to in-distribution degradations and enables adaptive restoration grounded on the degradation-agnostic shared invariance. Extensive experiments demonstrate that BaryIR performs competitively against state-of-the-art all-in-one methods. Notably, BaryIR generalizes well to unseen degradations (\textit{e.g.,} types and levels) and shows remarkable robustness in learning generalized features, even when trained on limited degradation types and evaluated on real-world data with mixed degradations.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이미지 복원 및 향상 기술로 스포츠 영상 보정에 직접 적용 가능</p>
        

        
        <p><strong>활용 인사이트:</strong> 다양한 품질 저하된 스포츠 영상을 통합 복원하여 사진처럼 고품질 콘텐츠 생성</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23169v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.23294v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23294v1" onchange="updateToolbar()">
          <span class="rank">11위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23294v1">Towards Long-Form Spatio-Temporal Video Grounding</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>88.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 88.0%"></div>
          </div>
          <span class="score-detail">
            base:75 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">In real scenarios, videos can span several minutes or even hours. However, existing research on spatio-temporal video grounding (STVG), given a textual query, mainly focuses on localizing targets in short videos of tens of seconds, typically less than one minute, which limits real-world applications.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">In real scenarios, videos can span several minutes or even hours. However, existing research on spatio-temporal video grounding (STVG), given a textual query, mainly focuses on localizing targets in short videos of tens of seconds, typically less than one minute, which limits real-world applications. In this paper, we explore Long-Form STVG (LF-STVG), which aims to locate targets in long-term videos. Compared with short videos, long-term videos contain much longer temporal spans and more irrelevant information, making it difficult for existing STVG methods that process all frames at once. To address this challenge, we propose an AutoRegressive Transformer architecture for LF-STVG, termed ART-STVG. Unlike conventional STVG methods that require the entire video sequence to make predictions at once, ART-STVG treats the video as streaming input and processes frames sequentially, enabling efficient handling of long videos. To model spatio-temporal context, we design spatial and temporal memory banks and apply them to the decoders. Since memories from different moments are not always relevant to the current frame, we introduce simple yet effective memory selection strategies to provide more relevant information to the decoders, significantly improving performance. Furthermore, instead of parallel spatial and temporal localization, we propose a cascaded spatio-temporal design that connects the spatial decoder to the temporal decoder, allowing fine-grained spatial cues to assist complex temporal localization in long videos. Experiments on newly extended LF-STVG datasets show that ART-STVG significantly outperforms state-of-the-art methods, while achieving competitive performance on conventional short-form STVG.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> Long-form video processing technology applicable to sports game analysis</p>
        

        
        <p><strong>활용 인사이트:</strong> 긴 경기 영상에서 주요 장면을 자동으로 식별하여 하이라이트 편집 기능 구현 가능</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23294v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22607v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22607v1" onchange="updateToolbar()">
          <span class="rank">12위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22607v1">LoR-LUT: Learning Compact 3D Lookup Tables via Low-Rank Residuals</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>88.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 88.0%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">We present LoR-LUT, a unified low-rank formulation for compact and interpretable 3D lookup table (LUT) generation. Unlike conventional 3D-LUT-based techniques that rely on fusion of basis LUTs, which are usually dense tensors, our unified approach extends the current framework by jointly using residual corrections, which are in fact low-rank tensors, together with a set of basis LUTs.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We present LoR-LUT, a unified low-rank formulation for compact and interpretable 3D lookup table (LUT) generation. Unlike conventional 3D-LUT-based techniques that rely on fusion of basis LUTs, which are usually dense tensors, our unified approach extends the current framework by jointly using residual corrections, which are in fact low-rank tensors, together with a set of basis LUTs. The approach described here improves the existing perceptual quality of an image, which is primarily due to the technique&#39;s novel use of residual corrections. At the same time, we achieve the same level of trilinear interpolation complexity, using a significantly smaller number of network, residual corrections, and LUT parameters. The experimental results obtained from LoR-LUT, which is trained on the MIT-Adobe FiveK dataset, reproduce expert-level retouching characteristics with high perceptual fidelity and a sub-megabyte model size. Furthermore, we introduce an interactive visualization tool, termed LoR-LUT Viewer, which transforms an input image into the LUT-adjusted output image, via a number of slidebars that control different parameters. The tool provides an effective way to enhance interpretability and user confidence in the visual results. Overall, our proposed formulation offers a compact, interpretable, and efficient direction for future LUT-based image enhancement and style transfer.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 컴팩트 3D 룩업 테이블 기술로 스포츠 영상 및 이미지 보정에 직접적으로 적용 가능</p>
        

        
        <p><strong>활용 인사이트:</strong> 스포츠 영상과 이미지의 품질을 향상시키면서도 적은 파라미터로 edge 디바이스에서 실시간 처리 가능</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22607v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22794v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22794v1" onchange="updateToolbar()">
          <span class="rank">13위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22794v1">Doubly Adaptive Channel and Spatial Attention for Semantic Image Communication by IoT Devices</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>88.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 88.0%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+5
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Internet of Things (IoT) networks face significant challenges such as limited communication bandwidth, constrained computational and energy resources, and highly dynamic wireless channel conditions. Utilization of deep neural networks (DNNs) combined with semantic communication has emerged as a promising paradigm to address these limitations.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Internet of Things (IoT) networks face significant challenges such as limited communication bandwidth, constrained computational and energy resources, and highly dynamic wireless channel conditions. Utilization of deep neural networks (DNNs) combined with semantic communication has emerged as a promising paradigm to address these limitations. Deep joint source-channel coding (DJSCC) has recently been proposed to enable semantic communication of images. Building upon the original DJSCC formulation, low-complexity attention-style architectures has been added to the DNNs for further performance enhancement. As a main hurdle, training these DNNs separately for various signal-to-noise ratios (SNRs) will amount to excessive storage or communication overhead, which can not be maintained by small IoT devices. SNR Adaptive DJSCC (ADJSCC), has been proposed to train the DNNs once but feed the current SNR as part of the data to the channel-wise attention mechanism. We improve upon ADJSCC by a simultaneous utilization of doubly adaptive channel-wise and spatial attention modules at both transmitter and receiver. These modules dynamically adjust to varying channel conditions and spatial feature importance, enabling robust and efficient feature extraction and semantic information recovery. Simulation results corroborate that our proposed doubly adaptive DJSCC (DA-DJSCC) significantly improves upon ADJSCC in several performance criteria, while incurring a mild increase in complexity. These facts render DA-DJSCC a desirable choice for semantic communication in performance demanding but low-complexity IoT networks.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> IoT device image communication with adaptive attention, applicable to edge device video processing</p>
        

        
        <p><strong>활용 인사이트:</strong> 제한된 리소스를 가진 edge 디바이스에서도 다양한 통신 환경에 적응하여 효율적인 영상 처리 가능</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22794v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.23188v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23188v1" onchange="updateToolbar()">
          <span class="rank">14위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23188v1">Efficient Real-Time Adaptation of ROMs for Unsteady Flows Using Data Assimilation</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>88.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 88.0%"></div>
          </div>
          <span class="score-detail">
            base:75 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">physics.flu-dyn</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">We propose an efficient retraining strategy for a parameterized Reduced Order Model (ROM) that attains accuracy comparable to full retraining while requiring only a fraction of the computational time and relying solely on sparse observations of the full system. The architecture employs an encode-process-decode structure: a Variational Autoencoder (VAE) to perform dimensionality reduction, and a transformer network to evolve the latent states and model the dynamics.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We propose an efficient retraining strategy for a parameterized Reduced Order Model (ROM) that attains accuracy comparable to full retraining while requiring only a fraction of the computational time and relying solely on sparse observations of the full system. The architecture employs an encode-process-decode structure: a Variational Autoencoder (VAE) to perform dimensionality reduction, and a transformer network to evolve the latent states and model the dynamics. The ROM is parameterized by an external control variable, the Reynolds number in the Navier-Stokes setting, with the transformer exploiting attention mechanisms to capture both temporal dependencies and parameter effects. The probabilistic VAE enables stochastic sampling of trajectory ensembles, providing predictive means and uncertainty quantification through the first two moments. After initial training on a limited set of dynamical regimes, the model is adapted to out-of-sample parameter regions using only sparse data. Its probabilistic formulation naturally supports ensemble generation, which we employ within an ensemble Kalman filtering framework to assimilate data and reconstruct full-state trajectories from minimal observations. We further show that, for the dynamical system considered, the dominant source of error in out-of-sample forecasts stems from distortions of the latent manifold rather than changes in the latent dynamics. Consequently, retraining can be limited to the autoencoder, allowing for a lightweight, computationally efficient, real-time adaptation procedure with very sparse fine-tuning data.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> Real-time adaptation techniques applicable to edge device AI processing</p>
        

        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23188v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.23115v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23115v1" onchange="updateToolbar()">
          <span class="rank">15위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23115v1">FLIGHT: Fibonacci Lattice-based Inference for Geometric Heading in real-Time</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>86.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 86.4%"></div>
          </div>
          <span class="score-detail">
            base:78 + bonus:+5
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.CG</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Estimating camera motion from monocular video is a fundamental problem in computer vision, central to tasks such as SLAM, visual odometry, and structure-from-motion. Existing methods that recover the camera&#39;s heading under known rotation, whether from an IMU or an optimization algorithm, tend to perform well in low-noise, low-outlier conditions, but often decrease in accuracy or become computationally expensive as noise and outlier levels increase.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Estimating camera motion from monocular video is a fundamental problem in computer vision, central to tasks such as SLAM, visual odometry, and structure-from-motion. Existing methods that recover the camera&#39;s heading under known rotation, whether from an IMU or an optimization algorithm, tend to perform well in low-noise, low-outlier conditions, but often decrease in accuracy or become computationally expensive as noise and outlier levels increase. To address these limitations, we propose a novel generalization of the Hough transform on the unit sphere (S(2)) to estimate the camera&#39;s heading. First, the method extracts correspondences between two frames and generates a great circle of directions compatible with each pair of correspondences. Then, by discretizing the unit sphere using a Fibonacci lattice as bin centers, each great circle casts votes for a range of directions, ensuring that features unaffected by noise or dynamic objects vote consistently for the correct motion direction. Experimental results on three datasets demonstrate that the proposed method is on the Pareto frontier of accuracy versus efficiency. Additionally, experiments on SLAM show that the proposed method reduces RMSE by correcting the heading during camera pose initialization.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 실시간 카메라 모션 추정으로 자동 촬영 정확도 향상, 노이즈 많은 스포츠 환경에서도 효과적</p>
        

        
        <p><strong>활용 인사이트:</strong> 피보나치 격자 기반 헤딩 추정을 슬로모션 촬영 및 자세 분석에 적용하여 하이라이트 자동 생성</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23115v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22800v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22800v1" onchange="updateToolbar()">
          <span class="rank">16위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22800v1">GSTurb: Gaussian Splatting for Atmospheric Turbulence Mitigation</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>86.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 86.4%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Atmospheric turbulence causes significant image degradation due to pixel displacement (tilt) and blur, particularly in long-range imaging applications. In this paper, we propose a novel framework for atmospheric turbulence mitigation, GSTurb, which integrates optical flow-guided tilt correction and Gaussian splatting for modeling non-isoplanatic blur.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Atmospheric turbulence causes significant image degradation due to pixel displacement (tilt) and blur, particularly in long-range imaging applications. In this paper, we propose a novel framework for atmospheric turbulence mitigation, GSTurb, which integrates optical flow-guided tilt correction and Gaussian splatting for modeling non-isoplanatic blur. The framework employs Gaussian parameters to represent tilt and blur, and optimizes them across multiple frames to enhance restoration. Experimental results on the ATSyn-static dataset demonstrate the effectiveness of our method, achieving a peak PSNR of 27.67 dB and SSIM of 0.8735. Compared to the state-of-the-art method, GSTurb improves PSNR by 1.3 dB (a 4.5% increase) and SSIM by 0.048 (a 5.8% increase). Additionally, on real datasets, including the TSRWGAN Real-World and CLEAR datasets, GSTurb outperforms existing methods, showing significant improvements in both qualitative and quantitative performance. These results highlight that combining optical flow-guided tilt correction with Gaussian splatting effectively enhances image restoration under both synthetic and real-world turbulence conditions. The code for this method will be available at https://github.com/DuhlLiamz/3DGS_turbulence/tree/main.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 대기 왜곡으로 인한 스포츠 영상 저하 문제 해결에 필수적인 기술</p>
        

        
        <p><strong>활용 인사이트:</strong> 가우시안 스플래팅 기술로 실시간 영상 보정 가능, 경기장 장거리 촬영 시 품질 향상</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22800v1">PDF</a>
          
          
          <a href="https://github.com/DuhlLiamz/3DGS_turbulence/tree/main">Code</a>
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.23101v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23101v1" onchange="updateToolbar()">
          <span class="rank">17위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23101v1">Locally Adaptive Decay Surfaces for High-Speed Face and Landmark Detection with Event Cameras</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>84.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 84.0%"></div>
          </div>
          <span class="score-detail">
            base:70 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Event cameras record luminance changes with microsecond resolution, but converting their sparse, asynchronous output into dense tensors that neural networks can exploit remains a core challenge. Conventional histograms or globally-decayed time-surface representations apply fixed temporal parameters across the entire image plane, which in practice creates a trade-off between preserving spatial structure during still periods and retaining sharp edges during rapid motion.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Event cameras record luminance changes with microsecond resolution, but converting their sparse, asynchronous output into dense tensors that neural networks can exploit remains a core challenge. Conventional histograms or globally-decayed time-surface representations apply fixed temporal parameters across the entire image plane, which in practice creates a trade-off between preserving spatial structure during still periods and retaining sharp edges during rapid motion. We introduce Locally Adaptive Decay Surfaces (LADS), a family of event representations in which the temporal decay at each location is modulated according to local signal dynamics. Three strategies are explored, based on event rate, Laplacian-of-Gaussian response, and high-frequency spectral energy. These adaptive schemes preserve detail in quiescent regions while reducing blur in regions of dense activity. Extensive experiments on the public data show that LADS consistently improves both face detection and facial landmark accuracy compared to standard non-adaptive representations. At 30 Hz, LADS achieves higher detection accuracy and lower landmark error than either baseline, and at 240 Hz it mitigates the accuracy decline typically observed at higher frequencies, sustaining 2.44 % normalized mean error for landmarks and 0.966 mAP50 in face detection. These high-frequency results even surpass the accuracy reported in prior works operating at 30 Hz, setting new benchmarks for event-based face analysis. Moreover, by preserving spatial structure at the representation stage, LADS supports the use of much lighter network architectures while still retaining real-time performance. These results highlight the importance of context-aware temporal integration for neuromorphic vision and point toward real-time, high-frequency human-computer interaction systems that exploit the unique advantages of event cameras.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 고속 스포츠 동작 캡처에 적용 가능한 이벤트 카메라 기술로 빠른 움직임을 정확히 포착할 수 있습니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> LADS 기술을 rk3588 에지 디바이스에 통합하여 실시간으로 선수 추적 및 빠른 동작 분석이 가능하며, 240Hz 고주파 성능으로 빠른 스포츠 장면을 명확하게 캡처할 수 있습니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23101v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22520v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22520v1" onchange="updateToolbar()">
          <span class="rank">18위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22520v1">TEFL: Prediction-Residual-Guided Rolling Forecasting for Multi-Horizon Time Series</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>84.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 84.0%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Time series forecasting plays a critical role in domains such as transportation, energy, and meteorology. Despite their success, modern deep forecasting models are typically trained to minimize point-wise prediction loss without leveraging the rich information contained in past prediction residuals from rolling forecasts - residuals that reflect persistent biases, unmodeled patterns, or evolving dynamics.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Time series forecasting plays a critical role in domains such as transportation, energy, and meteorology. Despite their success, modern deep forecasting models are typically trained to minimize point-wise prediction loss without leveraging the rich information contained in past prediction residuals from rolling forecasts - residuals that reflect persistent biases, unmodeled patterns, or evolving dynamics. We propose TEFL (Temporal Error Feedback Learning), a unified learning framework that explicitly incorporates these historical residuals into the forecasting pipeline during both training and evaluation. To make this practical in deep multi-step settings, we address three key challenges: (1) selecting observable multi-step residuals under the partial observability of rolling forecasts, (2) integrating them through a lightweight low-rank adapter to preserve efficiency and prevent overfitting, and (3) designing a two-stage training procedure that jointly optimizes the base forecaster and error module. Extensive experiments across 10 real-world datasets and 5 backbone architectures show that TEFL consistently improves accuracy, reducing MAE by 5-10% on average. Moreover, it demonstrates strong robustness under abrupt changes and distribution shifts, with error reductions exceeding 10% (up to 19.5%) in challenging scenarios. By embedding residual-based feedback directly into the learning process, TEFL offers a simple, general, and effective enhancement to modern deep forecasting systems.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 스포츠 선수의 동작 패턴과 성적 예측을 위한 정확한 시계열 분석 필요</p>
        

        
        <p><strong>활용 인사이트:</strong> 예측 오차 기반 학습으로 경기 전략 분석 및 선수 성과 예측 정확도 향상</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22520v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.23040v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23040v1" onchange="updateToolbar()">
          <span class="rank">19위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23040v1">PackUV: Packed Gaussian UV Maps for 4D Volumetric Video</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>82.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 82.4%"></div>
          </div>
          <span class="score-detail">
            base:78 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Volumetric videos offer immersive 4D experiences, but remain difficult to reconstruct, store, and stream at scale. Existing Gaussian Splatting based methods achieve high-quality reconstruction but break down on long sequences, temporal inconsistency, and fail under large motions and disocclusions.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Volumetric videos offer immersive 4D experiences, but remain difficult to reconstruct, store, and stream at scale. Existing Gaussian Splatting based methods achieve high-quality reconstruction but break down on long sequences, temporal inconsistency, and fail under large motions and disocclusions. Moreover, their outputs are typically incompatible with conventional video coding pipelines, preventing practical applications.   We introduce PackUV, a novel 4D Gaussian representation that maps all Gaussian attributes into a sequence of structured, multi-scale UV atlas, enabling compact, image-native storage. To fit this representation from multi-view videos, we propose PackUV-GS, a temporally consistent fitting method that directly optimizes Gaussian parameters in the UV domain. A flow-guided Gaussian labeling and video keyframing module identifies dynamic Gaussians, stabilizes static regions, and preserves temporal coherence even under large motions and disocclusions. The resulting UV atlas format is the first unified volumetric video representation compatible with standard video codecs (e.g., FFV1) without losing quality, enabling efficient streaming within existing multimedia infrastructure.   To evaluate long-duration volumetric capture, we present PackUV-2B, the largest multi-view video dataset to date, featuring more than 50 synchronized cameras, substantial motion, and frequent disocclusions across 100 sequences and 2B (billion) frames. Extensive experiments demonstrate that our method surpasses existing baselines in rendering fidelity while scaling to sequences up to 30 minutes with consistent quality.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 대용량 스포츠 영상 효율적 저장 및 공플랫폼에서의 실시간 스트리밍 필요</p>
        

        
        <p><strong>활용 인사이트:</strong> 표준 비디오 코덱과 호환되는 4D 볼륨 비디오 포맷으로 콘텐츠 제작 및 공유 효율화</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23040v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22759v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22759v1" onchange="updateToolbar()">
          <span class="rank">20위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22759v1">Beyond Detection: Multi-Scale Hidden-Code for Natural Image Deepfake Recovery and Factual Retrieval</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>80.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 80.0%"></div>
          </div>
          <span class="score-detail">
            base:75 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Recent advances in image authenticity have primarily focused on deepfake detection and localization, leaving recovery of tampered contents for factual retrieval relatively underexplored. We propose a unified hidden-code recovery framework that enables both retrieval and restoration from post-hoc and in-generation watermarking paradigms.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent advances in image authenticity have primarily focused on deepfake detection and localization, leaving recovery of tampered contents for factual retrieval relatively underexplored. We propose a unified hidden-code recovery framework that enables both retrieval and restoration from post-hoc and in-generation watermarking paradigms. Our method encodes semantic and perceptual information into a compact hidden-code representation, refined through multi-scale vector quantization, and enhances contextual reasoning via conditional Transformer modules. To enable systematic evaluation for natural images, we construct ImageNet-S, a benchmark that provides paired image-label factual retrieval tasks. Extensive experiments on ImageNet-S demonstrate that our method exhibits promising retrieval and reconstruction performance while remaining fully compatible with diverse watermarking pipelines. This framework establishes a foundation for general-purpose image recovery beyond detection and localization.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이미지 복구 및 사실 검색 기술로 스포츠 영상 보정에 적용 가능하며 저조나 저품질 촬영 영상을 향상시킬 수 있습니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 다중 벡터 양자화 기술을 활용하여 스포츠 영상의 품질을 향상시키고, 조건부 Transformer 모듈로 경기 장면의 맥락적 추론을 강화하여 하이라이트 편집의 정확도를 높일 수 있습니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22759v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.23353v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23353v1" onchange="updateToolbar()">
          <span class="rank">21위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23353v1">SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>80.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 80.0%"></div>
          </div>
          <span class="score-detail">
            base:75 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 생물학적 조직 세포 계산 알고리즘으로 스포츠 분석에 직접 적용 불가</p>
        

        
        <p><strong>활용 인사이트:</strong> 커널 카운터 개념은 스포츠 장면 객체 인식에 간접적 참고 가능성</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23353v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22683v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22683v1" onchange="updateToolbar()">
          <span class="rank">22위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22683v1">SUPERGLASSES: Benchmarking Vision Language Models as Intelligent Agents for AI Smart Glasses</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>76.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 76.0%"></div>
          </div>
          <span class="score-detail">
            base:60 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">The rapid advancement of AI-powered smart glasses, one of the hottest wearable devices, has unlocked new frontiers for multimodal interaction, with Visual Question Answering (VQA) over external knowledge sources emerging as a core application. Existing Vision Language Models (VLMs) adapted to smart glasses are typically trained and evaluated on traditional multimodal datasets; however, these datasets lack the variety and realism needed to reflect smart glasses usage scenarios and diverge from their specific challenges, where accurately identifying the object of interest must precede any external knowledge retrieval.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">The rapid advancement of AI-powered smart glasses, one of the hottest wearable devices, has unlocked new frontiers for multimodal interaction, with Visual Question Answering (VQA) over external knowledge sources emerging as a core application. Existing Vision Language Models (VLMs) adapted to smart glasses are typically trained and evaluated on traditional multimodal datasets; however, these datasets lack the variety and realism needed to reflect smart glasses usage scenarios and diverge from their specific challenges, where accurately identifying the object of interest must precede any external knowledge retrieval. To bridge this gap, we introduce SUPERGLASSES, the first comprehensive VQA benchmark built on real-world data entirely collected by smart glasses devices. SUPERGLASSES comprises 2,422 egocentric image-question pairs spanning 14 image domains and 8 query categories, enriched with full search trajectories and reasoning annotations. We evaluate 26 representative VLMs on this benchmark, revealing significant performance gaps. To address the limitations of existing models, we further propose SUPERLENS, a multimodal smart glasses agent that enables retrieval-augmented answer generation by integrating automatic object detection, query decoupling, and multimodal web search. Our agent achieves state-of-the-art performance, surpassing GPT-4o by 2.19 percent, and highlights the need for task-specific solutions in smart glasses VQA scenarios.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> AI 스마트 글래스 기술을 엣지 디바이스에 적용하여 실시간 스포츠 분석 가능</p>
        

        
        <p><strong>활용 인사이트:</strong> 시각 질의 응답 기술을 활용하여 스포츠 장면에 대한 실시간 질의 응답 시스템 구축 가능</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22683v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.23031v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23031v1" onchange="updateToolbar()">
          <span class="rank">23위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23031v1">Small Object Detection Model with Spatial Laplacian Pyramid Attention and Multi-Scale Features Enhancement in Aerial Images</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>76.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 76.0%"></div>
          </div>
          <span class="score-detail">
            base:70 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Detecting objects in aerial images confronts some significant challenges, including small size, dense and non-uniform distribution of objects over high-resolution images, which makes detection inefficient. Thus, in this paper, we proposed a small object detection algorithm based on a Spatial Laplacian Pyramid Attention and Multi-Scale Feature Enhancement in aerial images.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Detecting objects in aerial images confronts some significant challenges, including small size, dense and non-uniform distribution of objects over high-resolution images, which makes detection inefficient. Thus, in this paper, we proposed a small object detection algorithm based on a Spatial Laplacian Pyramid Attention and Multi-Scale Feature Enhancement in aerial images. Firstly, in order to improve the feature representation of ResNet-50 on small objects, we presented a novel Spatial Laplacian Pyramid Attention (SLPA) module, which is integrated after each stage of ResNet-50 to identify and emphasize important local regions. Secondly, to enhance the model&#39;s semantic understanding and features representation, we designed a Multi-Scale Feature Enhancement Module (MSFEM), which is incorporated into the lateral connections of C5 layer for building Feature Pyramid Network (FPN). Finally, the features representation quality of traditional feature pyramid network will be affected because the features are not aligned when the upper and lower layers are fused. In order to handle it, we utilized deformable convolutions to align the features in the fusion processing of the upper and lower levels of the Feature Pyramid Network, which can help enhance the model&#39;s ability to detect and recognize small objects. The extensive experimental results on two benchmark datasets: VisDrone and DOTA demonstrate that our improved model performs better for small object detection in aerial images compared to the original algorithm.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 공중 영상에서 작은 객체 검출 기술로 스포츠 선수나 장비 정확히 식별 가능</p>
        

        
        <p><strong>활용 인사이트:</strong> 공간 라플라시안 피라미드 주의력 모듈로 멀티 스케일 특징 향상, 스포츠 장면에서 작은 객체 검출 성능 향상</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23031v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22592v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22592v1" onchange="updateToolbar()">
          <span class="rank">24위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22592v1">pQuant: Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>76.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 76.0%"></div>
          </div>
          <span class="score-detail">
            base:60 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.CL</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Quantization-Aware Training from scratch has emerged as a promising approach for building efficient large language models (LLMs) with extremely low-bit weights (sub 2-bit), which can offer substantial advantages for edge deployment. However, existing methods still fail to achieve satisfactory accuracy and scalability.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Quantization-Aware Training from scratch has emerged as a promising approach for building efficient large language models (LLMs) with extremely low-bit weights (sub 2-bit), which can offer substantial advantages for edge deployment. However, existing methods still fail to achieve satisfactory accuracy and scalability. In this work, we identify a parameter democratization effect as a key bottleneck: the sensitivity of all parameters becomes homogenized, severely limiting expressivity. To address this, we propose pQuant, a method that decouples parameters by splitting linear layers into two specialized branches: a dominant 1-bit branch for efficient computation and a compact high-precision branch dedicated to preserving the most sensitive parameters. Through tailored feature scaling, we explicitly guide the model to allocate sensitive parameters to the high-precision branch. Furthermore, we extend this branch into multiple, sparsely-activated experts, enabling efficient capacity scaling. Extensive experiments indicate our pQuant achieves state-of-the-art performance in extremely low-bit quantization.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 양자화 기술로 엣지 디바이스에서 AI 모델 효율적으로 실행 가능</p>
        

        
        <p><strong>활용 인사이트:</strong> pQuant 기법으로 rk3588 같은 엣지 디바이스에서 저비트 언어 모델 효율적으로 실행</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22592v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22974v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22974v1" onchange="updateToolbar()">
          <span class="rank">25위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22974v1">An automatic counting algorithm for the quantification and uncertainty analysis of the number of microglial cells trainable in small and heterogeneous datasets</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>74.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 74.4%"></div>
          </div>
          <span class="score-detail">
            base:60 + bonus:+8
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CE</span>
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">eess.IV</span>
          
          <span class="cat-tag">eess.SP</span>
          
          <span class="cat-tag">stat.ML</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Counting immunopositive cells on biological tissues generally requires either manual annotation or (when available) automatic rough systems, for scanning signal surface and intensity in whole slide imaging. In this work, we tackle the problem of counting microglial cells in lumbar spinal cord cross-sections of rats by omitting cell detection and focusing only on the counting task.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Counting immunopositive cells on biological tissues generally requires either manual annotation or (when available) automatic rough systems, for scanning signal surface and intensity in whole slide imaging. In this work, we tackle the problem of counting microglial cells in lumbar spinal cord cross-sections of rats by omitting cell detection and focusing only on the counting task. Manual cell counting is, however, a time-consuming task and additionally entails extensive personnel training. The classic automatic color-based methods roughly inform about the total labeled area and intensity (protein quantification) but do not specifically provide information on cell number. Since the images to be analyzed have a high resolution but a huge amount of pixels contain just noise or artifacts, we first perform a pre-processing generating several filtered images {(providing a tailored, efficient feature extraction)}. Then, we design an automatic kernel counter that is a non-parametric and non-linear method. The proposed scheme can be easily trained in small datasets since, in its basic version, it relies only on one hyper-parameter. However, being non-parametric and non-linear, the proposed algorithm is flexible enough to express all the information contained in rich and heterogeneous datasets as well (providing the maximum overfit if required). Furthermore, the proposed kernel counter also provides uncertainty estimation of the given prediction, and can directly tackle the case of receiving several expert opinions over the same image. Different numerical experiments with artificial and real datasets show very promising results. Related Matlab code is also provided.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> Image counting algorithm for biological tissues, not directly applicable to sports analysis</p>
        

        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22974v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.23205v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23205v1" onchange="updateToolbar()">
          <span class="rank">26위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23205v1">EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>72.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 72.0%"></div>
          </div>
          <span class="score-detail">
            base:65 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 휴대용 인간-장면 재구성 파이프라인으로 스포츠 촬영에 간접적으로 관련 있으나 연구 목적에 초점</p>
        

        
        <p><strong>활용 인사이트:</strong> 두 대의 iPhone을 사용한 실외 인간 동작 캡처 기술은 스포츠 장면의 3D 재구성과 분석에 적용 가능</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23205v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22897v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22897v1" onchange="updateToolbar()">
          <span class="rank">27위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22897v1">OmniGAIA: Towards Native Omni-Modal AI Agents</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>72.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 72.0%"></div>
          </div>
          <span class="score-detail">
            base:65 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.CL</span>
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.MM</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 다중 모달 AI 에이전트 기술이 스포츠 영상 분석에 간접적으로 적용 가능</p>
        

        
        <p><strong>활용 인사이트:</strong> 영상, 오디오, 텍스트를 통합해 경기 전략과 선수 동작 심층 분석 가능</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22897v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.23359v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.23359v1" onchange="updateToolbar()">
          <span class="rank">28위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23359v1">SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>72.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 72.0%"></div>
          </div>
          <span class="score-detail">
            base:65 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> Occlusion-aware 3D generation with potential applications in sports scene analysis</p>
        

        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.23359v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22514v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22514v1" onchange="updateToolbar()">
          <span class="rank">29위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22514v1">SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>72.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 72.0%"></div>
          </div>
          <span class="score-detail">
            base:55 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">eess.SY</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.   In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.   Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 실시간 시각-언어-행동 프레임워크로 스포츠 동작 이해에 간접적으로 관련</p>
        

        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22514v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.22712v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.22712v1" onchange="updateToolbar()">
          <span class="rank">30위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22712v1">UFO-DETR: Frequency-Guided End-to-End Detector for UAV Tiny Objects</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>72.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 72.0%"></div>
          </div>
          <span class="score-detail">
            base:60 + bonus:+5
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Small target detection in UAV imagery faces significant challenges such as scale variations, dense distribution, and the dominance of small targets. Existing algorithms rely on manually designed components, and general-purpose detectors are not optimized for UAV images, making it difficult to balance accuracy and complexity.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Small target detection in UAV imagery faces significant challenges such as scale variations, dense distribution, and the dominance of small targets. Existing algorithms rely on manually designed components, and general-purpose detectors are not optimized for UAV images, making it difficult to balance accuracy and complexity. To address these challenges, this paper proposes an end-to-end object detection framework, UFO-DETR, which integrates an LSKNet-based backbone network to optimize the receptive field and reduce the number of parameters. By combining the DAttention and AIFI modules, the model flexibly models multi-scale spatial relationships, improving multi-scale target detection performance. Additionally, the DynFreq-C3 module is proposed to enhance small target detection capability through cross-space frequency feature enhancement. Experimental results show that, compared to RT-DETR-L, the proposed method offers significant advantages in both detection performance and computational efficiency, providing an efficient solution for UAV edge computing.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> UAV 영상 작은 객체 탐지로 스포츠 장면 분석에 간접적으로 관련</p>
        

        
        <p><strong>활용 인사이트:</strong> 다중 스케일 객체 탐지 기술은 스포츠 영상에서 선수와 공 등 작은 객체 추적에 활용 가능</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.22712v1">PDF</a>
          
          
        </div>

        
      </div>
      

      
      <button class="show-more-btn" id="showMoreBtn" onclick="showAllTier1()">
        나머지 25편 더 보기
      </button>
      

      
      <div class="tier-divider">Tier 2</div>
      

      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.23070v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.23070v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23070v1">Make It Hard to Hear, Easy to Learn: Long-Form Bengali ASR and Speaker Diarization via Extreme Augmentation and Perfect Alignment</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">72.0</span>
            
            <a href="https://arxiv.org/pdf/2602.23070v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.SD</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.CL</span>
          
          <span class="cat-tag">eess.AS</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Although Automatic Speech Recognition (ASR) in Bengali has seen significant progress, processing long-duration audio and performing robust speaker diarization remain critical research gaps. To address the severe scarcity of joint ASR and diarization resources for this language, we introduce Lipi-Ghor-882, a comprehensive 882-hour multi-speaker Bengali dataset.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Although Automatic Speech Recognition (ASR) in Bengali has seen significant progress, processing long-duration audio and performing robust speaker diarization remain critical research gaps. To address the severe scarcity of joint ASR and diarization resources for this language, we introduce Lipi-Ghor-882, a comprehensive 882-hour multi-speaker Bengali dataset. In this paper, detailing our submission to the DL Sprint 4.0 competition, we systematically evaluate various architectures and approaches for long-form Bengali speech. For ASR, we demonstrate that raw data scaling is ineffective; instead, targeted fine-tuning utilizing perfectly aligned annotations paired with synthetic acoustic degradation (noise and reverberation) emerges as the singular most effective approach. Conversely, for speaker diarization, we observed that global open-source state-of-the-art models (such as Diarizen) performed surprisingly poorly on this complex dataset. Extensive model retraining yielded negligible improvements; instead, strategic, heuristic post-processing of baseline model outputs proved to be the primary driver for increasing accuracy. Ultimately, this work outlines a highly optimized dual pipeline achieving a $\sim$0.019 Real-Time Factor (RTF), establishing a practical, empirically backed benchmark for low-resource, long-form speech processing.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Audio processing techniques applicable to comprehensive sports analysis system, mentions real-time processing</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.22629v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.22629v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22629v1">CRAG: Can 3D Generative Models Help 3D Assembly?</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.4</span>
            
            <a href="https://arxiv.org/pdf/2602.22629v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Most existing 3D assembly methods treat the problem as pure pose estimation, rearranging observed parts via rigid transformations. In contrast, human assembly naturally couples structural reasoning with holistic shape inference.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Most existing 3D assembly methods treat the problem as pure pose estimation, rearranging observed parts via rigid transformations. In contrast, human assembly naturally couples structural reasoning with holistic shape inference. Inspired by this intuition, we reformulate 3D assembly as a joint problem of assembly and generation. We show that these two processes are mutually reinforcing: assembly provides part-level structural priors for generation, while generation injects holistic shape context that resolves ambiguities in assembly. Unlike prior methods that cannot synthesize missing geometry, we propose CRAG, which simultaneously generates plausible complete shapes and predicts poses for input parts. Extensive experiments demonstrate state-of-the-art performance across in-the-wild objects with diverse geometries, varying part counts, and missing pieces. Our code and models will be released.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">3D 조립 및 생성 모델로 스포츠 장면의 3D 재구성에 간접적으로 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.23363v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.23363v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23363v1">MediX-R1: Open Ended Medical Reinforcement Learning</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.4</span>
            
            <a href="https://arxiv.org/pdf/2602.23363v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Medical reinforcement learning framework, not directly applicable to sports analysis</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.22544v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.22544v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22544v1">HARU-Net: Hybrid Attention Residual U-Net for Edge-Preserving Denoising in Cone-Beam Computed Tomography</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.4</span>
            
            <a href="https://arxiv.org/pdf/2602.22544v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">eess.IV</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">eess.SP</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Cone-beam computed tomography (CBCT) is widely used in dental and maxillofacial imaging, but low-dose acquisition introduces strong, spatially varying noise that degrades soft-tissue visibility and obscures fine anatomical structures. Classical denoising methods struggle to suppress noise in CBCT while preserving edges.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Cone-beam computed tomography (CBCT) is widely used in dental and maxillofacial imaging, but low-dose acquisition introduces strong, spatially varying noise that degrades soft-tissue visibility and obscures fine anatomical structures. Classical denoising methods struggle to suppress noise in CBCT while preserving edges. Although deep learning-based approaches offer high-fidelity restoration, their use in CBCT denoising is limited by the scarcity of high-resolution CBCT data for supervised training. To address this research gap, we propose a novel Hybrid Attention Residual U-Net (HARU-Net) for high-quality denoising of CBCT data, trained on a cadaver dataset of human hemimandibles acquired using a high-resolution protocol of the 3D Accuitomo 170 (J. Morita, Kyoto, Japan) CBCT system. The novel contribution of this approach is the integration of three complementary architectural components: (i) a hybrid attention transformer block (HAB) embedded within each skip connection to selectively emphasize salient anatomical features, (ii) a residual hybrid attention transformer group (RHAG) at the bottleneck to strengthen global contextual modeling and long-range feature interactions, and (iii) residual learning convolutional blocks to facilitate deeper, more stable feature extraction throughout the network. HARU-Net consistently outperforms state-of-the-art (SOTA) methods including SwinIR and Uformer, achieving the highest PSNR (37.52 dB), highest SSIM (0.9557), and lowest GMSD (0.1084). This effective and clinically reliable CBCT denoising is achieved at a computational cost significantly lower than that of the SOTA methods, offering a practical advancement toward improving diagnostic quality in low-dose CBCT imaging.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">의료 영상 노이즈 제거 기술로 스포츠 영상 보정에 간접적으로 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.23231v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.23231v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23231v1">Skarimva: Skeleton-based Action Recognition is a Multi-view Application</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">68.0</span>
            
            <a href="https://arxiv.org/pdf/2602.23231v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Human action recognition plays an important role when developing intelligent interactions between humans and machines. While there is a lot of active research on improving the machine learning algorithms for skeleton-based action recognition, not much attention has been given to the quality of the input skeleton data itself.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Human action recognition plays an important role when developing intelligent interactions between humans and machines. While there is a lot of active research on improving the machine learning algorithms for skeleton-based action recognition, not much attention has been given to the quality of the input skeleton data itself. This work demonstrates that by making use of multiple camera views to triangulate more accurate 3D~skeletons, the performance of state-of-the-art action recognition models can be improved significantly. This suggests that the quality of the input data is currently a limiting factor for the performance of these models. Based on these results, it is argued that the cost-benefit ratio of using multiple cameras is very favorable in most practical use-cases, therefore future research in skeleton-based action recognition should consider multi-view applications as the standard setup.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">다중 카메라 뷰를 이용한 액션 인식 연구로 스포츠 분석에 간접적으로 관련 있으나 구체적 스포츠 적용 없음</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.22998v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.22998v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22998v1">A Perspective on Open Challenges in Deformable Object Manipulation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">68.0</span>
            
            <a href="https://arxiv.org/pdf/2602.22998v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Deformable object manipulation (DOM) represents a critical challenge in robotics, with applications spanning healthcare, manufacturing, food processing, and beyond. Unlike rigid objects, deformable objects exhibit infinite dimensionality, dynamic shape changes, and complex interactions with their environment, posing significant hurdles for perception, modeling, and control.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Deformable object manipulation (DOM) represents a critical challenge in robotics, with applications spanning healthcare, manufacturing, food processing, and beyond. Unlike rigid objects, deformable objects exhibit infinite dimensionality, dynamic shape changes, and complex interactions with their environment, posing significant hurdles for perception, modeling, and control. This paper reviews the state of the art in DOM, focusing on key challenges such as occlusion handling, task generalization, and scalable, real-time solutions. It highlights advancements in multimodal perception systems, including the integration of multi-camera setups, active vision, and tactile sensing, which collectively address occlusion and improve adaptability in unstructured environments. Cutting-edge developments in physically informed reinforcement learning (RL) and differentiable simulations are explored, showcasing their impact on efficiency, precision, and scalability. The review also emphasizes the potential of simulated expert demonstrations and generative neural networks to standardize task specifications and bridge the simulation-to-reality gap. Finally, future directions are proposed, including the adoption of graph neural networks for high-level decision-making and the creation of comprehensive datasets to enhance DOM&#39;s real-world applicability. By addressing these challenges, DOM research can pave the way for versatile robotic systems capable of handling diverse and dynamic tasks with deformable objects.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Indirectly related through multimodal perception systems for robotics</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.22716v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.22716v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22716v1">SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">68.0</span>
            
            <a href="https://arxiv.org/pdf/2602.22716v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model&#39;s ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">3D 공간 인식 강화로 스포츠 공간 분석에 간접적으로 관련</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.22519v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.22519v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22519v1">A Mathematical Theory of Agency and Intelligence</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">68.0</span>
            
            <a href="https://arxiv.org/pdf/2602.22519v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.IT</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">AI 시스템의 에이전시와 지능에 대한 이론으로 프로젝트와 간접적으로 관련</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.23191v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.23191v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23191v1">Uni-Animator: Towards Unified Visual Colorization</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.4</span>
            
            <a href="https://arxiv.org/pdf/2602.23191v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">스케치 컬러라이제이션 기술이 스포츠 영상 보정에 간접적으로 활용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.23280v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.23280v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23280v1">Physics Informed Viscous Value Representations</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.4</span>
            
            <a href="https://arxiv.org/pdf/2602.23280v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">물리 정보 강화 강화학습으로 스포츠 전략 분석에 간접적으로 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.22689v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.22689v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22689v1">No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.4</span>
            
            <a href="https://arxiv.org/pdf/2602.22689v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.CR</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Latent diffusion models have achieved remarkable success in high-fidelity text-to-image generation, but their tendency to memorize training data raises critical privacy and intellectual property concerns. Membership inference attacks (MIAs) provide a principled way to audit such memorization by determining whether a given sample was included in training.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Latent diffusion models have achieved remarkable success in high-fidelity text-to-image generation, but their tendency to memorize training data raises critical privacy and intellectual property concerns. Membership inference attacks (MIAs) provide a principled way to audit such memorization by determining whether a given sample was included in training. However, existing approaches assume access to ground-truth captions. This assumption fails in realistic scenarios where only images are available and their textual annotations remain undisclosed, rendering prior methods ineffective when substituted with vision-language model (VLM) captions. In this work, we propose MoFit, a caption-free MIA framework that constructs synthetic conditioning inputs that are explicitly overfitted to the target model&#39;s generative manifold. Given a query image, MoFit proceeds in two stages: (i) model-fitted surrogate optimization, where a perturbation applied to the image is optimized to construct a surrogate in regions of the model&#39;s unconditional prior learned from member samples, and (ii) surrogate-driven embedding extraction, where a model-fitted embedding is derived from the surrogate and then used as a mismatched condition for the query image. This embedding amplifies conditional loss responses for member samples while leaving hold-outs relatively less affected, thereby enhancing separability in the absence of ground-truth captions. Our comprehensive experiments across multiple datasets and diffusion models demonstrate that MoFit consistently outperforms prior VLM-conditioned baselines and achieves performance competitive with caption-dependent methods.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Latent diffusion models for image processing could enhance video correction</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.22862v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.22862v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22862v1">GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.22862v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, the ability of imitation-learned policies to execute precise and generalizable grasps merits particular attention. Existing imitation learning techniques for grasping often suffer from imprecise grasp executions, limited spatial generalization, and poor object generalization. To address these challenges, we incorporate grasp prior knowledge into the diffusion policy framework. In particular, we employ a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring that generated motion trajectories adhere closely to feasible grasp configurations. Furthermore, we introduce a self-supervised reconstruction objective during diffusion to embed the graspness prior: at each reverse diffusion step, we reconstruct wrist-camera images back-projected the graspness from the intermediate representations. Both simulation and real robot experiments demonstrate that our approach significantly outperforms baseline methods and exhibits strong dynamic grasping capabilities.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">확산 모델 기반 그래핑 정책으로 스포츠 동작 분석에 간접적으로 관련</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.22740v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.22740v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22740v1">AMLRIS: Alignment-aware Masked Learning for Referring Image Segmentation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.22740v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Referring Image Segmentation (RIS) aims to segment an object in an image identified by a natural language expression. The paper introduces Alignment-Aware Masked Learning (AML), a training strategy to enhance RIS by explicitly estimating pixel-level vision-language alignment, filtering out poorly aligned regions during optimization, and focusing on trustworthy cues.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Referring Image Segmentation (RIS) aims to segment an object in an image identified by a natural language expression. The paper introduces Alignment-Aware Masked Learning (AML), a training strategy to enhance RIS by explicitly estimating pixel-level vision-language alignment, filtering out poorly aligned regions during optimization, and focusing on trustworthy cues. This approach results in state-of-the-art performance on RefCOCO datasets and also enhances robustness to diverse descriptions and scenarios</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">자연어 표현을 통한 이미지 분할 기술로 특정 선수나 객체 식별에 간접적으로 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.22922v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.22922v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22922v1">Bayesian Preference Elicitation: Human-In-The-Loop Optimization of An Active Prosthesis</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.22922v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Tuning active prostheses for people with amputation is time-consuming and relies on metrics that may not fully reflect user needs. We introduce a human-in-the-loop optimization (HILO) approach that leverages direct user preferences to personalize a standard four-parameter prosthesis controller efficiently.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Tuning active prostheses for people with amputation is time-consuming and relies on metrics that may not fully reflect user needs. We introduce a human-in-the-loop optimization (HILO) approach that leverages direct user preferences to personalize a standard four-parameter prosthesis controller efficiently. Our method employs preference-based Multiobjective Bayesian Optimization that uses a state-or-the-art acquisition function especially designed for preference learning, and includes two algorithmic variants: a discrete version (\textit{EUBO-LineCoSpar}), and a continuous version (\textit{BPE4Prost}). Simulation results on benchmark functions and real-application trials demonstrate efficient convergence, robust preference elicitation, and measurable biomechanical improvements, illustrating the potential of preference-driven tuning for user-centered prosthesis control.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Prosthesis optimization using Bayesian methods, tangentially related to performance optimization</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.23159v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.23159v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23159v1">Benchmarking Temporal Web3 Intelligence: Lessons from the FinSurvival 2025 Challenge</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.23159v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Temporal Web analytics increasingly relies on large-scale, longitudinal data to understand how users, content, and systems evolve over time. A rapidly growing frontier is the \emph{Temporal Web3}: decentralized platforms whose behavior is recorded as immutable, time-stamped event streams.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Temporal Web analytics increasingly relies on large-scale, longitudinal data to understand how users, content, and systems evolve over time. A rapidly growing frontier is the \emph{Temporal Web3}: decentralized platforms whose behavior is recorded as immutable, time-stamped event streams. Despite the richness of this data, the field lacks shared, reproducible benchmarks that capture real-world temporal dynamics, specifically censoring and non-stationarity, across extended horizons. This absence slows methodological progress and limits the transfer of techniques between Web3 and broader Web domains. In this paper, we present the \textit{FinSurvival Challenge 2025} as a case study in benchmarking \emph{temporal Web3 intelligence}. Using 21.8 million transaction records from the Aave v3 protocol, the challenge operationalized 16 survival prediction tasks to model user behavior transitions.We detail the benchmark design and the winning solutions, highlighting how domain-aware temporal feature construction significantly outperformed generic modeling approaches. Furthermore, we distill lessons for next-generation temporal benchmarks, arguing that Web3 systems provide a high-fidelity sandbox for studying temporal challenges, such as churn, risk, and evolution that are fundamental to the wider Web.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Temporal analysis techniques could be applicable to sports activity tracking but domain is Web3 transactions</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.23089v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.23089v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23089v1">Physics-informed neural particle flow for the Bayesian update step</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.23089v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">The Bayesian update step poses significant computational challenges in high-dimensional nonlinear estimation. While log-homotopy particle flow filters offer an alternative to stochastic sampling, existing formulations usually yield stiff differential equations.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">The Bayesian update step poses significant computational challenges in high-dimensional nonlinear estimation. While log-homotopy particle flow filters offer an alternative to stochastic sampling, existing formulations usually yield stiff differential equations. Conversely, existing deep learning approximations typically treat the update as a black-box task or rely on asymptotic relaxation, neglecting the exact geometric structure of the finite-horizon probability transport. In this work, we propose a physics-informed neural particle flow, which is an amortized inference framework. To construct the flow, we couple the log-homotopy trajectory of the prior to posterior density function with the continuity equation describing the density evolution. This derivation yields a governing partial differential equation (PDE), referred to as the master PDE. By embedding this PDE as a physical constraint into the loss function, we train a neural network to approximate the transport velocity field. This approach enables purely unsupervised training, eliminating the need for ground-truth posterior samples. We demonstrate that the neural parameterization acts as an implicit regularizer, mitigating the numerical stiffness inherent to analytic flows and reducing online computational complexity. Experimental validation on multimodal benchmarks and a challenging nonlinear scenario confirms better mode coverage and robustness compared to state-of-the-art baselines.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Neural network techniques for estimation could be adapted for sports analytics but application domain is different</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.22678v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.22678v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22678v1">ViCLIP-OT: The First Foundation Vision-Language Model for Vietnamese Image-Text Retrieval with Optimal Transport</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">60.0</span>
            
            <a href="https://arxiv.org/pdf/2602.22678v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Image-text retrieval has become a fundamental component in intelligent multimedia systems; however, most existing vision-language models are optimized for highresource languages and remain suboptimal for low-resource settings such as Vietnamese. This work introduces ViCLIP-OT, a foundation vision-language model specifically designed for Vietnamese image-text retrieval.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Image-text retrieval has become a fundamental component in intelligent multimedia systems; however, most existing vision-language models are optimized for highresource languages and remain suboptimal for low-resource settings such as Vietnamese. This work introduces ViCLIP-OT, a foundation vision-language model specifically designed for Vietnamese image-text retrieval. The proposed framework integrates CLIP-style contrastive learning with a Similarity-Graph Regularized Optimal Transport (SIGROT) loss to enhance global cross-modal consistency and mitigate modality gap issues. Extensive experiments on three Vietnamese benchmarks (UITOpenViIC, KTVIC, and Crossmodal-3600) demonstrate that ViCLIP-OT consistently outperforms CLIP and SigLIP baselines in both in-domain and zero-shot settings. On UIT-OpenViIC, the model achieves an average Recall@K of 67.34%, improving upon CLIP by 5.75 percentage points. In zero-shot evaluation on Crossmodal-3600, ViCLIPOT surpasses CLIP by 11.72 percentage points. Embedding-space analysis further confirms improved alignment and reduced modality gap. The results indicate that integrating SIGROT provides an effective and scalable strategy for cross-modal retrieval in low-resource languages, offering practical implications for intelligent multimedia retrieval systems in Vietnamese and other underrepresented linguistic contexts.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">비전-언어 모델로 스포츠 콘텐츠 이해에 간접적으로 관련</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.22976v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.22976v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22976v1">Efficient Parallel Algorithms for Hypergraph Matching</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">58.4</span>
            
            <a href="https://arxiv.org/pdf/2602.22976v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.DS</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We present efficient parallel algorithms for computing maximal matchings in hypergraphs. Our algorithm finds locally maximal edges in the hypergraph and adds them in parallel to the matching.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We present efficient parallel algorithms for computing maximal matchings in hypergraphs. Our algorithm finds locally maximal edges in the hypergraph and adds them in parallel to the matching. In the CRCW PRAM models our algorithms achieve $O(\log{m})$ time with $O((κ+ n) \log {m})$ work w.h.p. where $m$ is the number of hyperedges, and $κ$ is the sum of all vertex degrees. The CREW PRAM model algorithm has a running time of $O((\logΔ+\log{d})\log{m})$ and requires $O((κ+ n) \log {m})$ work w.h.p. It can be implemented work-optimal with $O(κ+n)$ work in $O((\log{m}+\log{n})\log{m})$ time. We prove a $1/d$-approximation guarantee for our algorithms.   We evaluate our algorithms experimentally by implementing and running the proposed algorithms on the GPU using CUDA and Kokkos. Our experimental evaluation demonstrates the practical efficiency of our approach on real-world hypergraph instances, yielding a speed up of up to 76 times compared to a single-core CPU algorithm.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">GPU 알고리즘은 엣지 디바이스에 유용하지만 스포츠 분석과 직접 관련 없음</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.23010v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.23010v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23010v1">HELMLAB: An Analytical, Data-Driven Color Space for Perceptual Distance in UI Design Systems</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">50.4</span>
            
            <a href="https://arxiv.org/pdf/2602.23010v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.GR</span>
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We present HELMLAB, a 72-parameter analytical color space for UI design systems. The forward transform maps CIE XYZ to a perceptually-organized Lab representation through learned matrices, per-channel power compression, Fourier hue correction, and embedded Helmholtz-Kohlrausch lightness adjustment.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We present HELMLAB, a 72-parameter analytical color space for UI design systems. The forward transform maps CIE XYZ to a perceptually-organized Lab representation through learned matrices, per-channel power compression, Fourier hue correction, and embedded Helmholtz-Kohlrausch lightness adjustment. A post-pipeline neutral correction guarantees that achromatic colors map to a=b=0 (chroma &lt; 10^-6), and a rigid rotation of the chromatic plane improves hue-angle alignment without affecting the distance metric, which is invariant under isometries. On the COMBVD dataset (3,813 color pairs), HELMLAB achieves a STRESS of 23.22, a 20.4% reduction from CIEDE2000 (29.18). Cross-validation on He et al. 2022 and MacAdam 1974 shows competitive cross-dataset performance. The transform is invertible with round-trip errors below 10^-14. Gamut mapping, design-token export, and dark/light mode adaptation utilities are included for use in web and mobile design systems.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">UI 디자인을 위한 색 공간 기술로 스포츠 이미지 보정에 약간의 관련성</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.23117v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.23117v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23117v1">Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">50.4</span>
            
            <a href="https://arxiv.org/pdf/2602.23117v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Adversarial transferability refers to the capacity of adversarial examples generated on the surrogate model to deceive alternate, unexposed victim models. This property eliminates the need for direct access to the victim model during an attack, thereby raising considerable security concerns in practical applications and attracting substantial research attention recently.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Adversarial transferability refers to the capacity of adversarial examples generated on the surrogate model to deceive alternate, unexposed victim models. This property eliminates the need for direct access to the victim model during an attack, thereby raising considerable security concerns in practical applications and attracting substantial research attention recently. In this work, we discern a lack of a standardized framework and criteria for evaluating transfer-based attacks, leading to potentially biased assessments of existing approaches. To rectify this gap, we have conducted an exhaustive review of hundreds of related works, organizing various transfer-based attacks into six distinct categories. Subsequently, we propose a comprehensive framework designed to serve as a benchmark for evaluating these attacks. In addition, we delineate common strategies that enhance adversarial transferability and highlight prevalent issues that could lead to unfair comparisons. Finally, we provide a brief review of transfer-based attacks beyond image classification.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">이미지 분류에 대한 적대적 예제 연구로 프로젝트와 약간 관련</div>
        
      </div>
      
    
  </div>

  <div class="tab-content tab-panel-remind">
    
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.20630v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20630v1">From Pairs to Sequences: Track-Aware Policy Gradients for Keypoint Detection</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">96.0</span>
            <span class="recommend-count">2회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">Keypoint-based matching is a fundamental component of modern 3D vision systems, such as Structure-from-Motion (SfM) and SLAM. Most existing learning-based methods are trained on image pairs, a paradigm that fails to explicitly optimize for the long-term trackability of keypoints across sequences under challenging viewpoint and illumination changes. In this paper, we reframe keypoint detection as a sequential decision-making problem. We introduce TraqPoint, a novel, end-to-end Reinforcement Learning (RL) framework designed to optimize the \textbf{Tra}ck-\textbf{q}uality (Traq) of keypoints directly on image sequences. Our core innovation is a track-aware reward mechanism that jointly encourages the consistency and distinctiveness of keypoints across multiple views, guided by a policy gradient method. Extensive evaluations on sparse matching benchmarks, including relative pose estimation and 3D reconstruction, demonstrate that TraqPoint significantly outperforms some state-of-the-art (SOTA) keypoint detection and description methods.</div>
        
        
        <div class="compact-reason">실시간 동분할 기술은 스포츠 장면에서 움직임을 효과적으로 분석하여 하이라이트 편집에 적합합니다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.20790v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20790v1">Real-time Motion Segmentation with Event-based Normal Flow</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">93.6</span>
            <span class="recommend-count">2회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">Event-based cameras are bio-inspired sensors with pixels that independently and asynchronously respond to brightness changes at microsecond resolution, offering the potential to handle visual tasks in challenging scenarios. However, due to the sparse information content in individual events, directly processing the raw event data to solve vision tasks is highly inefficient, which severely limits the applicability of state-of-the-art methods in real-time tasks, such as motion segmentation, a fundamental task for dynamic scene understanding. Incorporating normal flow as an intermediate representation to compress motion information from event clusters within a localized region provides a more effective solution. In this work, we propose a normal flow-based motion segmentation framework for event-based vision. Leveraging the dense normal flow directly learned from event neighborhoods as input, we formulate the motion segmentation task as an energy minimization problem solved via graph cuts, and optimize it iteratively with normal flow clustering and motion model fitting. By using a normal flow-based motion model initialization and fitting method, the proposed system is able to efficiently estimate the motion models of independently moving objects with only a limited number of candidate models, which significantly reduces the computational complexity and ensures real-time performance, achieving nearly a 800x speedup in comparison to the open-source state-of-the-art method. Extensive evaluations on multiple public datasets fully demonstrate the accuracy and efficiency of our framework.</div>
        
        
        <div class="compact-reason">고속 움직임으로 인한 흐림 문제를 해결하여 스포츠 촬영의 품질을 향상시키는 데 중요합니다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.21188v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.21188v1">Human Video Generation from a Single Image with 3D Pose and View Control</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">90.4</span>
            <span class="recommend-count">2회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">Recent diffusion methods have made significant progress in generating videos from single images due to their powerful visual generation capabilities. However, challenges persist in image-to-video synthesis, particularly in human video generation, where inferring view-consistent, motion-dependent clothing wrinkles from a single image remains a formidable problem. In this paper, we present Human Video Generation in 4D (HVG), a latent video diffusion model capable of generating high-quality, multi-view, spatiotemporally coherent human videos from a single image with 3D pose and view control. HVG achieves this through three key designs: (i) Articulated Pose Modulation, which captures the anatomical relationships of 3D joints via a novel dual-dimensional bone map and resolves self-occlusions across views by introducing 3D information; (ii) View and Temporal Alignment, which ensures multi-view consistency and alignment between a reference image and pose sequences for frame-to-frame stability; and (iii) Progressive Spatio-Temporal Sampling with temporal alignment to maintain smooth transitions in long multi-view animations. Extensive experiments on image-to-video tasks demonstrate that HVG outperforms existing methods in generating high-quality 4D human videos from diverse human images and pose inputs.</div>
        
        
        <div class="compact-reason">3D 포즈 제어 기반 영상 생성 기술은 스포츠 하이라이트 영상 제작에 직접 적용 가능</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.21101v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.21101v1">Event-Aided Sharp Radiance Field Reconstruction for Fast-Flying Drones</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">89.6</span>
            <span class="recommend-count">2회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">Fast-flying aerial robots promise rapid inspection under limited battery constraints, with direct applications in infrastructure inspection, terrain exploration, and search and rescue. However, high speeds lead to severe motion blur in images and induce significant drift and noise in pose estimates, making dense 3D reconstruction with Neural Radiance Fields (NeRFs) particularly challenging due to their high sensitivity to such degradations. In this work, we present a unified framework that leverages asynchronous event streams alongside motion-blurred frames to reconstruct high-fidelity radiance fields from agile drone flights. By embedding event-image fusion into NeRF optimization and jointly refining event-based visual-inertial odometry priors using both event and frame modalities, our method recovers sharp radiance fields and accurate camera trajectories without ground-truth supervision. We validate our approach on both synthetic data and real-world sequences captured by a fast-flying drone. Despite highly dynamic drone flights, where RGB frames are severely degraded by motion blur and pose priors become unreliable, our method reconstructs high-fidelity radiance fields and preserves fine scene details, delivering a performance gain of over 50% on real-world data compared to state-of-the-art methods.</div>
        
        
        <div class="compact-reason">드론 기반 3D 재구성 기술은 스포츠 촬영 시 움직임 흐림 문제 해결에 적용 가능</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.21810v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.21810v1">GeoMotion: Rethinking Motion Segmentation via Latent 4D Geometry</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">88.0</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">Motion segmentation in dynamic scenes is highly challenging, as conventional methods heavily rely on estimating camera poses and point correspondences from inherently noisy motion cues. Existing statistical inference or iterative optimization techniques that struggle to mitigate the cumulative errors in multi-stage pipelines often lead to limited performance or high computational cost. In contrast, we propose a fully learning-based approach that directly infers moving objects from latent feature representations via attention mechanisms, thus enabling end-to-end feed-forward motion segmentation. Our key insight is to bypass explicit correspondence estimation and instead let the model learn to implicitly disentangle object and camera motion. Supported by recent advances in 4D scene geometry reconstruction (e.g., $π^3$), the proposed method leverages reliable camera poses and rich spatial-temporal priors, which ensure stable training and robust inference for the model. Extensive experiments demonstrate that by eliminating complex pre-processing and iterative refinement, our approach achieves state-of-the-art motion segmentation performance with high efficiency. The code is available at:https://github.com/zjutcvg/GeoMotion.</div>
        
        
        <div class="compact-reason">동적 장면에서의 움직임 분할 기술로 스포츠 촬영 시 선수와 공 등 움직이는 객체를 정확히 식별 가능</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.20739v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20739v1">PyVision-RL: Forging Open Agentic Vision Models via RL</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">88.0</span>
            <span class="recommend-count">2회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.</div>
        
        
        <div class="compact-reason">PyVision-Video framework for video understanding applicable for sports scene analysis</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.20958v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20958v1">EKF-Based Depth Camera and Deep Learning Fusion for UAV-Person Distance Estimation and Following in SAR Operations</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">84.0</span>
            <span class="recommend-count">2회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">Search and rescue (SAR) operations require rapid responses to save lives or property. Unmanned Aerial Vehicles (UAVs) equipped with vision-based systems support these missions through prior terrain investigation or real-time assistance during the mission itself. Vision-based UAV frameworks aid human search tasks by detecting and recognizing specific individuals, then tracking and following them while maintaining a safe distance. A key safety requirement for UAV following is the accurate estimation of the distance between camera and target object under real-world conditions, achieved by fusing multiple image modalities. UAVs with deep learning-based vision systems offer a new approach to the planning and execution of SAR operations. As part of the system for automatic people detection and face recognition using deep learning, in this paper we present the fusion of depth camera measurements and monocular camera-to-body distance estimation for robust tracking and following. Deep learning-based filtering of depth camera data and estimation of camera-to-body distance from a monocular camera are achieved with YOLO-pose, enabling real-time fusion of depth information using the Extended Kalman Filter (EKF) algorithm. The proposed subsystem, designed for use in drones, estimates and measures the distance between the depth camera and the human body keypoints, to maintain the safe distance between the drone and the human target. Our system provides an accurate estimated distance, which has been validated against motion capture ground truth data. The system has been tested in real time indoors, where it reduces the average errors, root mean square error (RMSE) and standard deviations of distance estimation up to 15,3\% in three tested scenarios.</div>
        
        
        <div class="compact-reason">This paper proposes a method for distance estimation and following of UAVs using depth camera and deep learning fusion. The core is YOLO-pose and EKF algorithm integration for real-time distance measurement.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.20792v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20792v1">SIMSPINE: A Biomechanics-Aware Simulation Framework for 3D Spine Motion Annotation and Benchmarking</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">82.4</span>
            <span class="recommend-count">2회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">Modeling spinal motion is fundamental to understanding human biomechanics, yet remains underexplored in computer vision due to the spine&#39;s complex multi-joint kinematics and the lack of large-scale 3D annotations. We present a biomechanics-aware keypoint simulation framework that augments existing human pose datasets with anatomically consistent 3D spinal keypoints derived from musculoskeletal modeling. Using this framework, we create the first open dataset, named SIMSPINE, which provides sparse vertebra-level 3D spinal annotations for natural full-body motions in indoor multi-camera capture without external restraints. With 2.14 million frames, this enables data-driven learning of vertebral kinematics from subtle posture variations and bridges the gap between musculoskeletal simulation and computer vision. In addition, we release pretrained baselines covering fine-tuned 2D detectors, monocular 3D pose lifting models, and multi-view reconstruction pipelines, establishing a unified benchmark for biomechanically valid spine motion estimation. Specifically, our 2D spine baselines improve the state-of-the-art from 0.63 to 0.80 AUC in controlled environments, and from 0.91 to 0.93 AP for in-the-wild spine tracking. Together, the simulation framework and SIMSPINE dataset advance research in vision-based biomechanics, motion analysis, and digital human modeling by enabling reproducible, anatomically grounded 3D spine estimation under natural conditions.</div>
        
        
        <div class="compact-reason">생체역학 시뮬레이션 프레임워크는 스포츠 동작 분석에 직접 적용 가능하여 선수 자세 및 동작 분석에 필수적</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.20500v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.20500v1">Strategy-Supervised Autonomous Laparoscopic Camera Control via Event-Driven Graph Mining</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">80.0</span>
            <span class="recommend-count">2회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">Autonomous laparoscopic camera control must maintain a stable and safe surgical view under rapid tool-tissue interactions while remaining interpretable to surgeons. We present a strategy-grounded framework that couples high-level vision-language inference with low-level closed-loop control. Offline, raw surgical videos are parsed into camera-relevant temporal events (e.g., interaction, working-distance deviation, and view-quality degradation) and structured as attributed event graphs. Mining these graphs yields a compact set of reusable camera-handling strategy primitives, which provide structured supervision for learning. Online, a fine-tuned Vision-Language Model (VLM) processes the live laparoscopic view to predict the dominant strategy and discrete image-based motion commands, executed by an IBVS-RCM controller under strict safety constraints; optional speech input enables intuitive human-in-the-loop conditioning. On a surgeon-annotated dataset, event parsing achieves reliable temporal localization (F1-score 0.86), and the mined strategies show strong semantic alignment with expert interpretation (cluster purity 0.81). Extensive ex vivo experiments on silicone phantoms and porcine tissues demonstrate that the proposed system outperforms junior surgeons in standardized camera-handling evaluations, reducing field-of-view centering error by 35.26% and image shaking by 62.33%, while preserving smooth motion and stable working-distance regulation.</div>
        
        
        <div class="compact-reason">전략 기반 자동 카메라 제어 방식은 경기 중요 순간을 자동으로 포착하여 하이라이트 영상 제작에 효과적</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.21778v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.21778v1">From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">80.0</span>
            <span class="recommend-count">1회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models.</div>
        
        
        <div class="compact-reason">물리적 사실성을 고려한 이미지 편집 기술로 스포츠 영상의 시각적 품질 향상 가능</div>
        
      </div>
      
    
  </div>

  <div class="tab-content tab-panel-below">
    
      <div class="empty-message">
        점수 미달 논문이 없습니다.
      </div>
    
  </div>

  <div class="tab-content tab-panel-excluded">
    
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.22650v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22650v1">AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-reason">광고 입찰 최적화에 관한 연구로 스포츠 AI 촬영 장치와 무관함</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.22732v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22732v1">Generative Recommendation for Large-Scale Advertising</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.IR</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-reason">광고 추천 시스템에 관한 연구로 스포츠 AI 촬영 장치와 무관함</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.22955v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22955v1">MM-NeuroOnco: A Multimodal Benchmark and Instruction Dataset for MRI-Based Brain Tumor Diagnosis</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-reason">MRI-based brain tumor diagnosis unrelated to sports filming</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.23012v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23012v1">Sequential Regression for Continuous Value Prediction using Residual Quantization</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.IR</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-reason">E-commerce recommendation system unrelated to sports filming</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.22581v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22581v1">IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-reason">Language model circuit discovery unrelated to sports filming</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.22993v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22993v1">Understanding Older Adults&#39; Experiences of Support, Concerns, and Risks from Kinship-Role AI-Generated Influencers</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.HC</span>
          
        </div>
        
        
        <div class="compact-reason">스포츠 영상 분석이나 엣지 디바이스 기술과 무관</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.22874v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22874v1">Flip Distance of Triangulations of Convex Polygons / Rotation Distance of Binary Trees is NP-complete</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CG</span>
          
          <span class="cat-tag">cs.CC</span>
          
          <span class="cat-tag">cs.DM</span>
          
          <span class="cat-tag">cs.DS</span>
          
          <span class="cat-tag">math.CO</span>
          
        </div>
        
        
        <div class="compact-reason">완전히 무관한 기하학적 복잡성 이론 연구</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.22685v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22685v1">Switch-Hurdle: A MoE Encoder with AR Hurdle Decoder for Intermittent Demand Forecasting</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-reason">소매 수요 예측에 관한 연구로 스포츠 AI 촬영 장치와 관련이 없습니다.</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.23085v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.23085v1">Q-Tag: Watermarking Quantum Circuit Generative Models</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">quant-ph</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-reason">양자 회로 생성 모델의 워터마킹 기술로 스포츠 촬영 및 분석과 무관함</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.22787v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.22787v1">Probing for Knowledge Attribution in Large Language Models</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CL</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-reason">대형 언어 모델의 지식 속성 탐색 기술로 스포츠 촬영 및 분석과 무관함</div>
        
      </div>
      
    
  </div>
</div>

<div class="download-toolbar" id="downloadToolbar">
  <div class="toolbar-selects">
    <button class="toolbar-btn" onclick="toggleSelect('all')">All</button>
    <button class="toolbar-btn" onclick="toggleSelect('tier1')">Tier 1</button>
    <button class="toolbar-btn" onclick="toggleSelect('tier2')">Tier 2</button>
    <button class="toolbar-btn" onclick="toggleSelect('none')">Deselect</button>
  </div>
  <button class="toolbar-btn primary" id="downloadBtn" onclick="downloadSelected()">Download 0 PDFs</button>
</div>

<footer>
  <p>이 리포트는 arXiv API를 사용하여 생성되었습니다.</p>
  <p>arXiv 논문의 저작권은 각 저자에게 있습니다.</p>
  <p>Thank you to arXiv for use of its open access interoperability.</p>
  <p>run_id: 23 | embedding: en_synthetic
    | weights: {&#39;embed&#39;: 0.35, &#39;llm&#39;: 0.55, &#39;recency&#39;: 0.1}</p>
</footer>

<script>
var READ_KEY = 'paperscout_read';
var SUPABASE_CFG = {
  enabled: true,
  url: 'https://qcvlnebvjzkgbbxainsk.supabase.co',
  key: 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InFjdmxuZWJ2anprZ2JieGFpbnNrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NzE0ODcyNTgsImV4cCI6MjA4NzA2MzI1OH0.Z53_RbpMnN3U6eyMlIxq-4h16hhvhawDzjKDb_iSNvI'
};

/* localStorage helpers */
function getLocalRead() {
  try { return JSON.parse(localStorage.getItem(READ_KEY) || '[]'); }
  catch(e) { return []; }
}
function saveLocalRead(arr) {
  try { localStorage.setItem(READ_KEY, JSON.stringify(arr)); } catch(e) {}
}

/* Supabase helpers */
function supabaseHeaders() {
  return {
    'apikey': SUPABASE_CFG.key,
    'Authorization': 'Bearer ' + SUPABASE_CFG.key,
    'Content-Type': 'application/json',
    'Prefer': 'return=minimal'
  };
}

function fetchReadFromSupabase() {
  if (!SUPABASE_CFG.enabled) return Promise.resolve([]);
  return fetch(SUPABASE_CFG.url + '/rest/v1/read_papers?select=paper_url', {
    headers: supabaseHeaders()
  })
  .then(function(r) { return r.ok ? r.json() : []; })
  .then(function(rows) { return rows.map(function(r) { return r.paper_url; }); })
  .catch(function() { return []; });
}

function markReadOnSupabase(url) {
  if (!SUPABASE_CFG.enabled || !url) return;
  fetch(SUPABASE_CFG.url + '/rest/v1/read_papers', {
    method: 'POST',
    headers: Object.assign({}, supabaseHeaders(), {'Prefer': 'return=minimal,resolution=ignore-duplicates'}),
    body: JSON.stringify({ paper_url: url })
  }).catch(function() {});
}

/* Unified read state */
var _readCache = null;

function getReadPapers() {
  if (_readCache) return _readCache;
  _readCache = getLocalRead();
  return _readCache;
}

function markAsRead(url) {
  if (!url) return;
  var read = getReadPapers();
  if (read.indexOf(url) === -1) {
    read.push(url);
    _readCache = read;
    saveLocalRead(read);
  }
  markReadOnSupabase(url);
}

function isRead(url) {
  return getReadPapers().indexOf(url) !== -1;
}

function applyReadState() {
  var cards = document.querySelectorAll('[data-paper-url]');
  var remindHidden = 0;
  cards.forEach(function(card) {
    var url = card.dataset.paperUrl;
    if (!isRead(url)) return;
    // Remind tab: hide read papers entirely
    if (card.classList.contains('compact-card--remind')) {
      card.style.display = 'none';
      remindHidden++;
    } else {
      card.classList.add('paper-read');
    }
  });
  // Update remind tab count
  var remindLabel = document.querySelector('label[for="tab-remind"]');
  if (remindLabel) {
    var total = document.querySelectorAll('.compact-card--remind').length;
    var visible = total - remindHidden;
    remindLabel.textContent = '\uB2E4\uC2DC \uBCF4\uAE30 (' + visible + ')';
  }
  // Show empty message if all remind papers are hidden
  var remindPanel = document.querySelector('.tab-panel-remind');
  if (remindPanel) {
    var visibleCards = remindPanel.querySelectorAll('.compact-card--remind:not([style*="display: none"])');
    var emptyMsg = remindPanel.querySelector('.empty-message');
    if (visibleCards.length === 0 && !emptyMsg) {
      var msg = document.createElement('div');
      msg.className = 'empty-message';
      msg.textContent = '\uB2E4\uC2DC \uBCF4\uAE30 \uB17C\uBB38\uC774 \uC5C6\uC2B5\uB2C8\uB2E4.';
      remindPanel.appendChild(msg);
    }
  }
}

function trackClicks() {
  document.addEventListener('click', function(e) {
    var link = e.target.closest('a[href]');
    if (!link) return;
    var card = link.closest('[data-paper-url]');
    if (!card) return;
    markAsRead(card.dataset.paperUrl);
    card.classList.add('paper-read');
    // If remind card, hide after brief delay
    if (card.classList.contains('compact-card--remind')) {
      setTimeout(function() {
        card.style.display = 'none';
        applyReadState();
      }, 200);
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  // Apply localStorage state immediately for fast UX
  applyReadState();
  trackClicks();
  // Then merge Supabase data for cross-device sync
  fetchReadFromSupabase().then(function(remote) {
    if (!remote || remote.length === 0) return;
    var local = getLocalRead();
    var merged = local.slice();
    var changed = false;
    remote.forEach(function(url) {
      if (merged.indexOf(url) === -1) {
        merged.push(url);
        changed = true;
      }
    });
    if (changed) {
      _readCache = merged;
      saveLocalRead(merged);
      applyReadState();
    }
  });
});

function showAllTier1() {
  document.querySelectorAll('.paper-card--collapsed').forEach(function(el) {
    el.classList.remove('paper-card--collapsed');
  });
  var btn = document.getElementById('showMoreBtn');
  if (btn) btn.style.display = 'none';
}

function toggleSelect(filter) {
  var boxes = document.querySelectorAll('.tab-panel-today .paper-checkbox');
  boxes.forEach(function(cb) {
    if (filter === 'none') { cb.checked = false; }
    else if (filter === 'all') { cb.checked = true; }
    else if (filter === 'tier1') { cb.checked = cb.dataset.tier === '1'; }
    else if (filter === 'tier2') { cb.checked = cb.dataset.tier === '2'; }
  });
  updateToolbar();
}

function updateToolbar() {
  var checked = document.querySelectorAll('.paper-checkbox:checked');
  var toolbar = document.getElementById('downloadToolbar');
  var btn = document.getElementById('downloadBtn');
  var count = 0;
  checked.forEach(function(cb) { if (cb.dataset.pdf) count++; });
  if (checked.length > 0) {
    toolbar.classList.add('visible');
    document.body.classList.add('toolbar-active');
    btn.textContent = 'Download ' + count + ' PDFs';
    btn.disabled = (count === 0);
  } else {
    toolbar.classList.remove('visible');
    document.body.classList.remove('toolbar-active');
  }
}

function downloadSelected() {
  var checked = document.querySelectorAll('.paper-checkbox:checked');
  var urls = [];
  checked.forEach(function(cb) { if (cb.dataset.pdf) urls.push(cb.dataset.pdf); });
  if (urls.length === 0) return;
  urls.forEach(function(url, i) {
    setTimeout(function() { window.open(url, '_blank'); }, i * 300);
  });
}
</script>

</body>
</html>
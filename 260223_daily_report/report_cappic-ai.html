<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CAPP!C_AI 논문 리포트 (2026-02-23)</title>
  <style>
    :root {
      --bg: #ffffff;
      --fg: #1a1a2e;
      --card-bg: #f8f9fa;
      --card-border: #e0e0e0;
      --accent: #2563eb;
      --accent-light: #dbeafe;
      --muted: #6b7280;
      --divider: #e5e7eb;
      --score-bg: #e5e7eb;
      --score-fill: #2563eb;
      --tier2-bg: #f3f4f6;
      --tag-bg: #fef3c7;
      --tag-fg: #92400e;
      --flag-edge: #dbeafe;
      --flag-realtime: #dcfce7;
      --flag-code: #f3e8ff;
      --remind-bg: #fffbeb;
    }

    @media (prefers-color-scheme: dark) {
      :root {
        --bg: #0f172a;
        --fg: #e2e8f0;
        --card-bg: #1e293b;
        --card-border: #334155;
        --accent: #60a5fa;
        --accent-light: #1e3a5f;
        --muted: #94a3b8;
        --divider: #334155;
        --score-bg: #334155;
        --score-fill: #60a5fa;
        --tier2-bg: #1e293b;
        --tag-bg: #78350f;
        --tag-fg: #fef3c7;
        --flag-edge: #1e3a5f;
        --flag-realtime: #14532d;
        --flag-code: #3b0764;
        --remind-bg: #451a03;
      }
    }

    * { box-sizing: border-box; margin: 0; padding: 0; }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: var(--bg);
      color: var(--fg);
      line-height: 1.6;
      max-width: 900px;
      margin: 0 auto;
      padding: 1rem;
    }

    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }

    .header { margin-bottom: 1.5rem; }
    .header h1 { font-size: 1.4rem; margin-bottom: 0.5rem; }
    .stats {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      font-size: 0.85rem;
      color: var(--muted);
      margin-bottom: 0.5rem;
    }
    .stats span { white-space: nowrap; }
    .stats span::after { content: ""; }
    .stats-arrow { color: var(--muted); font-size: 0.75rem; opacity: 0.6; }
    .below-score { color: #f59e0b; }
    .window-info {
      font-size: 0.8rem;
      color: var(--muted);
      background: var(--card-bg);
      border: 1px solid var(--card-border);
      border-radius: 4px;
      padding: 0.4rem 0.6rem;
      margin-top: 0.3rem;
      line-height: 1.6;
      display: flex;
      flex-wrap: wrap;
      align-items: center;
      gap: 0.3rem;
    }
    .window-label {
      font-weight: 600;
      color: var(--fg);
      margin-right: 0.2rem;
    }
    .window-range {
      font-family: 'SF Mono', 'Consolas', 'Monaco', monospace;
      font-size: 0.75rem;
      background: var(--bg);
      border: 1px solid var(--divider);
      border-radius: 3px;
      padding: 0.1rem 0.4rem;
    }
    .window-sep {
      color: var(--muted);
      font-weight: 600;
    }

    details { margin-bottom: 1rem; }
    details summary {
      cursor: pointer;
      font-weight: 600;
      padding: 0.5rem;
      background: var(--card-bg);
      border: 1px solid var(--card-border);
      border-radius: 4px;
      list-style: none;
      display: flex;
      align-items: center;
      gap: 0.4rem;
    }
    details summary::-webkit-details-marker { display: none; }
    details summary::before {
      content: '\25B6';
      font-size: 0.6em;
      transition: transform 0.2s;
      display: inline-block;
    }
    details[open] > summary::before { transform: rotate(90deg); }
    details[open] summary { border-radius: 4px 4px 0 0; }
    details .details-content {
      padding: 0.75rem;
      border: 1px solid var(--card-border);
      border-top: none;
      border-radius: 0 0 4px 4px;
    }

    .tabs {
      margin-bottom: 1rem;
    }
    .tabs input[type="radio"] { display: none; }
    .tab-labels {
      display: flex;
      border-bottom: 2px solid var(--divider);
      margin-bottom: 1rem;
    }
    .tab-labels label {
      padding: 0.6rem 1rem;
      min-height: 44px;
      display: inline-flex;
      align-items: center;
      cursor: pointer;
      font-weight: 600;
      color: var(--muted);
      border-bottom: 2px solid transparent;
      margin-bottom: -2px;
      transition: color 0.2s, border-color 0.2s;
    }
    .tab-labels label:hover { color: var(--fg); }
    .tab-content { display: none; }
    #tab-today:checked ~ .tab-labels label[for="tab-today"],
    #tab-remind:checked ~ .tab-labels label[for="tab-remind"],
    #tab-below:checked ~ .tab-labels label[for="tab-below"],
    #tab-excluded:checked ~ .tab-labels label[for="tab-excluded"] {
      color: var(--accent);
      border-bottom-color: var(--accent);
    }
    #tab-today:checked ~ .tab-panel-today { display: block; }
    #tab-remind:checked ~ .tab-panel-remind { display: block; }
    #tab-below:checked ~ .tab-panel-below { display: block; }
    #tab-excluded:checked ~ .tab-panel-excluded { display: block; }

    .paper-card {
      background: var(--card-bg);
      border: 1px solid var(--card-border);
      border-radius: 8px;
      padding: 1rem;
      margin-bottom: 1rem;
    }
    .paper-card--collapsed { display: none; }
    .show-more-btn {
      display: block;
      width: 100%;
      padding: 0.6rem;
      margin-bottom: 1rem;
      background: var(--card-bg);
      border: 1px dashed var(--card-border);
      border-radius: 8px;
      color: var(--accent);
      font-size: 0.85rem;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.2s;
    }
    .show-more-btn:hover { background: var(--accent-light); }
    .paper-card .rank-title {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      margin-bottom: 0.5rem;
    }
    .paper-card .rank {
      font-size: 1.1rem;
      font-weight: 700;
      color: var(--accent);
      white-space: nowrap;
      flex-shrink: 0;
      min-width: 2.5em;
      text-align: center;
    }
    .paper-card .paper-title {
      font-size: 1rem;
      font-weight: 600;
      flex: 1;
      min-width: 0;
    }
    .paper-card .paper-title a {
      color: var(--fg);
      display: block;
      overflow: hidden;
      text-overflow: ellipsis;
      white-space: nowrap;
    }
    .paper-card .paper-title a:hover { color: var(--accent); }

    .score-bar-container {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      margin: 0.5rem 0;
      font-size: 0.8rem;
    }
    .score-bar {
      flex: 1;
      max-width: 200px;
      height: 8px;
      background: var(--score-bg);
      border-radius: 4px;
      overflow: hidden;
    }
    .score-bar-fill {
      height: 100%;
      max-width: 100%;
      background: var(--score-fill);
      border-radius: 4px;
    }
    .score-detail { color: var(--muted); font-size: 0.75rem; }

    .categories {
      display: flex;
      flex-wrap: wrap;
      gap: 0.3rem;
      margin: 0.4rem 0;
    }
    .cat-tag {
      font-size: 0.7rem;
      padding: 0.1rem 0.4rem;
      border-radius: 3px;
      background: var(--accent-light);
      color: var(--accent);
    }

    .flags {
      display: flex;
      gap: 0.3rem;
      margin: 0.4rem 0;
    }
    .flag-tag {
      font-size: 0.7rem;
      padding: 0.1rem 0.4rem;
      border-radius: 3px;
      font-weight: 500;
    }
    .flag-edge { background: var(--flag-edge); }
    .flag-realtime { background: var(--flag-realtime); }
    .flag-code { background: var(--flag-code); }

    .lowered-tag {
      font-size: 0.7rem;
      padding: 0.1rem 0.4rem;
      border-radius: 3px;
      background: var(--tag-bg);
      color: var(--tag-fg);
      font-weight: 600;
    }

    .paper-links {
      display: flex;
      gap: 0.5rem;
      margin: 0.5rem 0;
      font-size: 0.8rem;
    }
    .paper-links a {
      display: inline-flex;
      align-items: center;
      gap: 0.2rem;
      padding: 0.2rem 0.5rem;
      border: 1px solid var(--accent);
      border-radius: 3px;
      font-weight: 500;
    }
    .paper-links a:hover {
      background: var(--accent);
      color: #fff;
      text-decoration: none;
    }

    .abstract-section {
      margin: 0.5rem 0;
      font-size: 0.85rem;
      color: var(--muted);
      line-height: 1.5;
    }
    .abstract-section .abstract-full {
      margin-top: 0.3rem;
      margin-bottom: 0;
    }
    .abstract-section .abstract-full summary {
      font-size: 0.75rem;
      font-weight: 500;
      padding: 0.2rem 0.4rem;
      background: transparent;
      border: 1px solid var(--card-border);
      display: inline-flex;
    }

    .cluster-links {
      font-size: 0.8rem;
      color: var(--muted);
      margin-top: 0.5rem;
    }

    .tier-divider {
      text-align: center;
      margin: 1.5rem 0;
      color: var(--muted);
      font-size: 0.8rem;
      font-weight: 600;
      position: relative;
      letter-spacing: 0.05em;
      text-transform: uppercase;
    }
    .tier-divider::before, .tier-divider::after {
      content: "";
      position: absolute;
      top: 50%;
      width: 35%;
      border-top: 1px solid var(--divider);
    }
    .tier-divider::before { left: 0; }
    .tier-divider::after { right: 0; }

    /* Unified compact card - used by Tier 2, remind, below-threshold, discarded */
    .compact-card {
      background: var(--tier2-bg);
      border: 1px solid var(--card-border);
      border-radius: 6px;
      padding: 0.6rem 0.8rem;
      margin-bottom: 0.5rem;
      font-size: 0.85rem;
    }
    .compact-card .rank-title {
      display: flex;
      align-items: center;
      gap: 0.4rem;
      flex-wrap: nowrap;
    }
    .compact-card .rank {
      font-weight: 700;
      color: var(--accent);
      white-space: nowrap;
      flex-shrink: 0;
      min-width: 2.5em;
      text-align: center;
    }
    .compact-card .paper-title {
      font-weight: 600;
      flex: 1;
      min-width: 0;
      overflow: hidden;
      text-overflow: ellipsis;
      white-space: nowrap;
    }
    .compact-card .paper-title a {
      color: var(--fg);
    }
    .compact-card .paper-title a:hover {
      color: var(--accent);
      text-decoration: none;
    }
    .compact-meta {
      display: flex;
      align-items: center;
      gap: 0.4rem;
      flex-shrink: 0;
      white-space: nowrap;
    }
    .compact-link {
      font-size: 0.75rem;
      padding: 0.1rem 0.3rem;
      border: 1px solid var(--accent);
      border-radius: 3px;
    }
    .compact-body {
      margin-top: 0.3rem;
      margin-left: 0.4rem;
      font-size: 0.8rem;
      color: var(--muted);
      line-height: 1.4;
    }
    .compact-reason {
      margin-top: 0.2rem;
      margin-left: 0.4rem;
      padding-left: 0.4rem;
      font-size: 0.78rem;
      color: var(--accent);
      line-height: 1.3;
      border-left: 3px solid var(--accent);
    }
    .compact-card .categories {
      margin-top: 0.3rem;
      margin-left: 0.4rem;
    }
    .recommend-count {
      font-size: 0.75rem;
      color: var(--muted);
      font-weight: 600;
    }
    /* Modifier: remind */
    .compact-card--remind { background: var(--remind-bg); }
    /* Modifier: below threshold */
    .compact-card--below { opacity: 0.75; }
    .compact-card--below .score-detail { color: #f59e0b; font-weight: 600; }
    /* Modifier: discarded */
    .compact-card--discarded { opacity: 0.7; }

    /* Read state - Gmail-style dimming */
    .paper-read { opacity: 0.5; }
    .paper-read .paper-title a { font-weight: 400; }

    .empty-message {
      text-align: center;
      padding: 3rem 1rem;
      color: var(--muted);
      font-size: 1rem;
    }

    footer {
      margin-top: 2rem;
      padding-top: 1rem;
      border-top: 1px solid var(--divider);
      font-size: 0.75rem;
      color: var(--muted);
    }
    footer p { margin-bottom: 0.3rem; }

    .index-list { list-style: none; }
    .index-list li {
      padding: 0.5rem 0;
      border-bottom: 1px solid var(--divider);
    }
    .index-list li:last-child { border-bottom: none; }

    /* Paper checkbox - matches rank text height */
    .paper-checkbox {
      width: 1em;
      height: 1em;
      accent-color: var(--accent);
      cursor: pointer;
      flex-shrink: 0;
      margin: 0;
    }
    .compact-card .paper-links {
      display: inline-flex;
      gap: 0.4rem;
      margin: 0;
      font-size: 0.8rem;
    }

    /* Download toolbar */
    .download-toolbar {
      position: fixed;
      bottom: 0;
      left: 50%;
      transform: translateX(-50%);
      width: 100%;
      max-width: 900px;
      background: var(--card-bg);
      border-top: 1px solid var(--card-border);
      padding: 0.6rem 1rem;
      display: none;
      align-items: center;
      justify-content: space-between;
      gap: 0.5rem;
      z-index: 100;
      box-shadow: 0 -2px 8px rgba(0,0,0,0.15);
    }
    .download-toolbar.visible { display: flex; }
    body.toolbar-active { padding-bottom: 3.5rem; }
    .toolbar-selects {
      display: flex;
      gap: 0.3rem;
      flex-wrap: wrap;
    }
    .toolbar-btn {
      padding: 0.3rem 0.7rem;
      border: 1px solid var(--card-border);
      border-radius: 4px;
      background: var(--bg);
      color: var(--fg);
      font-size: 0.8rem;
      cursor: pointer;
      white-space: nowrap;
    }
    .toolbar-btn:hover { border-color: var(--accent); color: var(--accent); }
    .toolbar-btn.primary {
      background: var(--accent);
      color: #fff;
      border-color: var(--accent);
    }
    .toolbar-btn.primary:hover { opacity: 0.9; }
    .toolbar-btn.primary:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }
  </style>
</head>
<body>
  
<div class="header">
  <h1>CAPP!C_AI 논문 리포트 (2026-02-23)</h1>
  <div class="stats">
    <span title="arXiv에서 수집된 총 논문 수">수집 461편</span>
    <span class="stats-arrow">&rarr;</span>
    <span title="키워드 필터 통과">필터 200편</span>
    <span class="stats-arrow">&rarr;</span>
    <span title="LLM 평가 후 제외">제외 12편</span>
    <span class="stats-arrow">&rarr;</span>
    <span title="점수 50점 미만">미달 53편</span>
    <span class="stats-arrow">&rarr;</span>
    <span title="최종 선정된 논문">선정 126편</span>
  </div>
  <div class="window-info">
    <span class="window-label">검색 윈도우</span>
    <span class="window-range">UTC 2026-02-11 23:29 / KST 2026-02-12 08:29</span>
    <span class="window-sep">~</span>
    <span class="window-range">UTC 2026-02-23 00:30 / KST 2026-02-23 09:30</span>
  </div>
</div>


<details>
  <summary>검색 키워드</summary>
  <div class="details-content">
    autonomous cinematography, sports tracking, camera control, highlight detection, action recognition, keyframe extraction, video stabilization, image enhancement, color correction, pose estimation, biomechanics, tactical analysis, short video, content summarization, video editing, edge computing, embedded vision, real-time processing, content sharing, social platform, advertising system, biomechanics, tactical analysis, embedded vision
  </div>
</details>


<div class="tabs">
  <input type="radio" name="tabs" id="tab-today" checked>
  <input type="radio" name="tabs" id="tab-remind">
  <input type="radio" name="tabs" id="tab-below">
  <input type="radio" name="tabs" id="tab-excluded">
  <div class="tab-labels">
    <label for="tab-today">오늘의 논문 (126)</label>
    <label for="tab-remind">다시 보기 (2)</label>
    <label for="tab-below">점수 미달 (0)</label>
    <label for="tab-excluded">제외 (12)</label>
  </div>

  <div class="tab-content tab-panel-today">
    
      
      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.18140v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.18140v1" onchange="updateToolbar()">
          <span class="rank">1위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18140v1">Flexi-NeurA: A Configurable Neuromorphic Accelerator with Adaptive Bit-Precision Exploration for Edge SNNs</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>94.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 94.4%"></div>
          </div>
          <span class="score-detail">
            base:88 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.AR</span>
          
          <span class="cat-tag">cs.NE</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Neuromorphic accelerators promise unparalleled energy efficiency and computational density for spiking neural networks (SNNs), especially in edge intelligence applications. However, most existing platforms exhibit rigid architectures with limited configurability, restricting their adaptability to heterogeneous workloads and diverse design objectives.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Neuromorphic accelerators promise unparalleled energy efficiency and computational density for spiking neural networks (SNNs), especially in edge intelligence applications. However, most existing platforms exhibit rigid architectures with limited configurability, restricting their adaptability to heterogeneous workloads and diverse design objectives. To address these limitations, we present Flexi-NeurA -- a parameterizable neuromorphic accelerator (core) that unifies configurability, flexibility, and efficiency. Flexi-NeurA allows users to customize neuron models, network structures, and precision settings at design time. By pairing these design-time configurability and flexibility features with a time-multiplexed and event-driven processing approach, Flexi-NeurA substantially reduces the required hardware resources and total power while preserving high efficiency and low inference latency. Complementing this, we introduce Flex-plorer, a heuristic-guided design-space exploration (DSE) tool that determines cost-effective fixed-point precisions for critical parameters -- such as decay factors, synaptic weights, and membrane potentials -- based on user-defined trade-offs between accuracy and resource usage. Based on the configuration selected through the Flex-plorer process, RTL code is configured to match the specified design. Comprehensive evaluations across MNIST, SHD, and DVS benchmarks demonstrate that the Flexi-NeurA and Flex-plorer co-framework achieves substantial improvements in accuracy, latency, and energy efficiency. A three-layer 256--128--10 fully connected network with LIF neurons mapped onto two processing cores achieves 97.23% accuracy on MNIST with 1.1~ms inference latency, utilizing only 1,623 logic cells, 7 BRAMs, and 111~mW of total power -- establishing Flexi-NeurA as a scalable, edge-ready neuromorphic platform.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 에너지 효율적이고 저지연의 신경모방 가속기 기술을 제안하여, 엣지 AI 하드웨어에 직접 적용 가능하다. 프로젝트의 실시간 스포츠 분석에 필수적이다.</p>
        

        
        <p><strong>활용 인사이트:</strong> Flexi-NeurA로 스포츠 동작 분석 모델을 가속화해 저전력 환경에서 1.1ms 지연 시간으로 고속 추론 수행. rk3588 디바이스에 통합 가능.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.18140v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.18397v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.18397v1" onchange="updateToolbar()">
          <span class="rank">2위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18397v1">How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>89.6</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 89.6%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems. In this paper, we ask a fundamental research question: How should we design future VLA models and systems to support real-time inference? To address this question, we first introduce VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Using VLA-Perf, we conduct the first systematic study of the VLA inference performance landscape. From a model-design perspective, we examine how inference performance is affected by model scaling, model architectural choices, long-context video inputs, asynchronous inference, and dual-system model pipelines. From the deployment perspective, we analyze where VLA inference should be executed -- on-device, on edge servers, or in the cloud -- and how hardware capability and network performance jointly determine end-to-end latency. By distilling 15 key takeaways from our comprehensive evaluation, we hope this work can provide practical guidance for the design of future VLA models and inference systems.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 엣지 디바이스에서 VLA 모델의 실시간 추론 성능 최적화 기술을 제안해, AI 촬영 장비의 동작 분석 및 영상 처리 지연 시간 감소에 직접 기여한다.</p>
        

        
        <p><strong>활용 인사이트:</strong> VLA-Perf로 스포츠 경기 분석 모델의 추론 속도 최적화. 엣지에서 비디오 입력 처리 시 fps 30 이상 달성해 실시간 분석 가능.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.18397v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.16362v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.16362v1" onchange="updateToolbar()">
          <span class="rank">3위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.16362v1">How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>88.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 88.0%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.DC</span>
          
          <span class="cat-tag">cs.NI</span>
          
          <span class="cat-tag">eess.SY</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 엣지 디바이스 간 분산 추론의 계산적 신뢰성 분석 프레임워크로, 스포츠 영상 분석 서비스의 안정적 운영에 필수적이다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 이 모델을 적용해 다수 엣지 디바이스에서 영상 분석 작업 분배. QoS 보장하며 지연 시간 100ms 이하로 유지 가능.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.16362v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.18158v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.18158v1" onchange="updateToolbar()">
          <span class="rank">4위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18158v1">A reliability- and latency-driven task allocation framework for workflow applications in the edge-hub-cloud continuum</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>88.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 88.0%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+5
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.DC</span>
          
          <span class="cat-tag">cs.ET</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">A growing number of critical workflow applications leverage a streamlined edge-hub-cloud architecture, which diverges from the conventional edge computing paradigm. An edge device, in collaboration with a hub device and a cloud server, often suffices for their reliable and efficient execution.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">A growing number of critical workflow applications leverage a streamlined edge-hub-cloud architecture, which diverges from the conventional edge computing paradigm. An edge device, in collaboration with a hub device and a cloud server, often suffices for their reliable and efficient execution. However, task allocation in this streamlined architecture is challenging due to device limitations and diverse operating conditions. Given the inherent criticality of such workflow applications, where reliability and latency are vital yet conflicting objectives, an exact task allocation approach is typically required to ensure optimal solutions. As no existing method holistically addresses these issues, we propose an exact multi-objective task allocation framework to jointly optimize the overall reliability and latency of a workflow application in the specific edge-hub-cloud architecture. We present a comprehensive binary integer linear programming formulation that considers the relative importance of each objective. It incorporates time redundancy techniques, while accounting for crucial constraints often overlooked in related studies. We evaluate our approach using a relevant real-world workflow application, as well as synthetic workflows varying in structure, size, and criticality. In the real-world application, our method achieved average improvements of 84.19% in reliability and 49.81% in latency over baseline strategies, across relevant objective trade-offs. Overall, the experimental results demonstrate the effectiveness and scalability of our approach across diverse workflow applications for the considered system architecture, highlighting its practicality with runtimes averaging between 0.03 and 50.94 seconds across all examined workflows.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 엣지-허브-클라우드 아키텍처용 작업 할당 프레임워크로, 영상 처리 및 분석 작업의 신뢰성과 지연 시간 최적화에 직접 적용 가능하다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 이 프레임워크로 영상 보정/분석 작업을 최적 장치에 할당해 지연 시간 49.81% 감소. rk3588 기기에서 실시간 처리 효율화.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.18158v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card" data-paper-url="http://arxiv.org/abs/2602.14582v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.14582v1" onchange="updateToolbar()">
          <span class="rank">5위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14582v1">YOLO26: A Comprehensive Architecture Overview and Key Improvements</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>86.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 86.4%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+13
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">You Only Look Once (YOLO) has been the prominent model for computer vision in deep learning for a decade. This study explores the novel aspects of YOLO26, the most recent version in the YOLO series.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">You Only Look Once (YOLO) has been the prominent model for computer vision in deep learning for a decade. This study explores the novel aspects of YOLO26, the most recent version in the YOLO series. The elimination of Distribution Focal Loss (DFL), implementation of End-to-End NMS-Free Inference, introduction of ProgLoss + Small-Target-Aware Label Assignment (STAL), and use of the MuSGD optimizer are the primary enhancements designed to improve inference speed, which is claimed to achieve a 43% boost in CPU mode. This is designed to allow YOLO26 to attain real-time performance on edge devices or those without GPUs. Additionally, YOLO26 offers improvements in many computer vision tasks, including instance segmentation, pose estimation, and oriented bounding box (OBB) decoding. We aim for this effort to provide more value than just consolidating information already included in the existing technical documentation. Therefore, we performed a rigorous architectural investigation into YOLO26, mostly using the source code available in its GitHub repository and its official documentation. The authentic and detailed operational mechanisms of YOLO26 are inside the source code, which is seldom extracted by others. The YOLO26 architectural diagram is shown as the outcome of the investigation. This study is, to our knowledge, the first one presenting the CNN-based YOLO26 architecture, which is the core of YOLO26. Our objective is to provide a precise architectural comprehension of YOLO26 for researchers and developers aspiring to enhance the YOLO model, ensuring it remains the leading deep learning model in computer vision.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 에지 디바이스용 실시간 객체 감지 및 포즈 추정 기술로 프로젝트의 핵심 기능인 운동 자세 분석과 경기 장면 인식에 직접 적용 가능. CPU 모드에서 43% 향상된 추론 속도(fps)가 RK3588 기기에서 실시간 성능 보장.</p>
        

        
        <p><strong>활용 인사이트:</strong> YOLO26을 장비에 통합해 경기 중 선수 동작 실시간 추적. ProgLoss + STAL 기법으로 작은 장애물 인식 정확도 향상, MuSGD 옵티마이저로 저사양 CPU에서 고속 추론(ms 단위 지연 최소화) 구현 가능.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.14582v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.13378v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.13378v1" onchange="updateToolbar()">
          <span class="rank">6위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13378v1">LAF-YOLOv10 with Partial Convolution Backbone, Attention-Guided Feature Pyramid, Auxiliary P2 Head, and Wise-IoU Loss for Small Object Detection in Drone Aerial Imagery</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>84.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 84.4%"></div>
          </div>
          <span class="score-detail">
            base:88 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Unmanned aerial vehicles serve as primary sensing platforms for surveillance, traffic monitoring, and disaster response, making aerial object detection a central problem in applied computer vision. Current detectors struggle with UAV-specific challenges: targets spanning only a few pixels, cluttered backgrounds, heavy occlusion, and strict onboard computational budgets.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Unmanned aerial vehicles serve as primary sensing platforms for surveillance, traffic monitoring, and disaster response, making aerial object detection a central problem in applied computer vision. Current detectors struggle with UAV-specific challenges: targets spanning only a few pixels, cluttered backgrounds, heavy occlusion, and strict onboard computational budgets. This study introduces LAF-YOLOv10, built on YOLOv10n, integrating four complementary techniques to improve small-object detection in drone imagery. A Partial Convolution C2f (PC-C2f) module restricts spatial convolution to one quarter of backbone channels, reducing redundant computation while preserving discriminative capacity. An Attention-Guided Feature Pyramid Network (AG-FPN) inserts Squeeze-and-Excitation channel gates before multi-scale fusion and replaces nearest-neighbor upsampling with DySample for content-aware interpolation. An auxiliary P2 detection head at 160$\times$160 resolution extends localization to objects below 8$\times$8 pixels, while the P5 head is removed to redistribute parameters. Wise-IoU v3 replaces CIoU for bounding box regression, attenuating gradients from noisy annotations in crowded aerial scenes. The four modules address non-overlapping bottlenecks: PC-C2f compresses backbone computation, AG-FPN refines cross-scale fusion, the P2 head recovers spatial resolution, and Wise-IoU stabilizes regression under label noise. No individual component is novel; the contribution is the joint integration within a single YOLOv10 framework. Across three training runs (seeds 42, 123, 256), LAF-YOLOv10 achieves 35.1$\pm$0.3\% mAP@0.5 on VisDrone-DET2019 with 2.3\,M parameters, exceeding YOLOv10n by 3.3 points. Cross-dataset evaluation on UAVDT yields 35.8$\pm$0.4\% mAP@0.5. Benchmarks on NVIDIA Jetson Orin Nano confirm 24.3 FPS at FP16, demonstrating viability for embedded UAV deployment.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 드론 영상에서 작은 객체 감지를 위한 방법을 제안합니다. 핵심은 경량화된 YOLOv10 변종으로, 작은 대상(예: 선수, 공)을 복잡한 배경에서 정확히 식별합니다. 에지 디바이스용 최적화로 스포츠 촬영에 직접 적용 가능하며, 24.3 FPS와 2.3M 매개변수로 RK3588에서 효율적 실행이 핵심 이유입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 스포츠 경기 실시간 촬영 시 선수나 공 같은 작은 객체 감지에 적용. PC-C2f 모듈로 계산 효율화, AG-FPN으로 다중 스케일 특징 향상, P2 헤드로 저해상도 객체 포착. RK3588에서 24.3 FPS 유지하며 하이라이트 자동 추출에 활용.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.13378v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.15633v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.15633v1" onchange="updateToolbar()">
          <span class="rank">7위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.15633v1">SpecFuse: A Spectral-Temporal Fusion Predictive Control Framework for UAV Landing on Oscillating Marine Platforms</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>84.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 84.4%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+13
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Autonomous landing of Uncrewed Aerial Vehicles (UAVs) on oscillating marine platforms is severely constrained by wave-induced multi-frequency oscillations, wind disturbances, and prediction phase lags in motion prediction. Existing methods either treat platform motion as a general random process or lack explicit modeling of wave spectral characteristics, leading to suboptimal performance under dynamic sea conditions.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Autonomous landing of Uncrewed Aerial Vehicles (UAVs) on oscillating marine platforms is severely constrained by wave-induced multi-frequency oscillations, wind disturbances, and prediction phase lags in motion prediction. Existing methods either treat platform motion as a general random process or lack explicit modeling of wave spectral characteristics, leading to suboptimal performance under dynamic sea conditions. To address these limitations, we propose SpecFuse: a novel spectral-temporal fusion predictive control framework that integrates frequency-domain wave decomposition with time-domain recursive state estimation for high-precision 6-DoF motion forecasting of Uncrewed Surface Vehicles (USVs). The framework explicitly models dominant wave harmonics to mitigate phase lags, refining predictions in real time via IMU data without relying on complex calibration. Additionally, we design a hierarchical control architecture featuring a sampling-based HPO-RRT* algorithm for dynamic trajectory planning under non-convex constraints and a learning-augmented predictive controller that fuses data-driven disturbance compensation with optimization-based execution. Extensive validations (2,000 simulations + 8 lake experiments) show our approach achieves a 3.2 cm prediction error, 4.46 cm landing deviation, 98.7% / 87.5% success rates (simulation / real-world), and 82 ms latency on embedded hardware, outperforming state-of-the-art methods by 44%-48% in accuracy. Its robustness to wave-wind coupling disturbances supports critical maritime missions such as search and rescue and environmental monitoring. All code, experimental configurations, and datasets will be released as open-source to facilitate reproducibility.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 진동하는 해상 플랫폼에 UAV 착륙을 위한 제어 방법을 제안합니다. 핵심은 실시간 임베디드 예측 제어로, 파도와 바람 영향 하에서 안정적 위치 유지합니다. 에지 디바이스 관련성(82ms 지연)이 높아 움직이는 촬영 플랫폼에 적용 가능한 이유입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 움직이는 트레이닝 장비나 선수 추적 시 카메라 안정화에 적용. HPO-RRT* 알고리즘으로 동적 경로 계획, IMU 데이터 실시간 보정. 82ms 지연 시간으로 RK3588에서 부드러운 영상 촬영 지원.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.15633v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.18322v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.18322v1" onchange="updateToolbar()">
          <span class="rank">8위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18322v1">Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>82.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 82.4%"></div>
          </div>
          <span class="score-detail">
            base:78 + bonus:+5
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 다양한 조명에서 이미지 보정 방법을 제안합니다. 핵심은 뷰-적응형 밝기 조절로, 저조도/과노출 시 색상 일관성 유지합니다. 프로젝트의 이미지 보정 기능과 직접 연관되어 화질 개선이 핵심 이유입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 스포츠 영상 보정 시 조명 변화 보정에 적용. 전역 밝기 조절과 지역 픽셀 정제로 자연스러운 색상 재현. 3DGS 기반 실시간 렌더링으로 RK3588에서 효율적 실행 가능.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.18322v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.18309v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.18309v1" onchange="updateToolbar()">
          <span class="rank">9위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18309v1">Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>82.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 82.4%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model&#39;s multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an &#34;in the wild&#34; split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 스케치와 텍스트로 패션 이미지 생성 방법을 제안합니다. 핵심은 다중 수준 조건화로, 구조와 세부 사항 정확히 반영합니다. 이미지 생성 기술이 프로젝트의 보정/변환 기능에 적용 가능해 선택 이유입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 사용자 지정 스포츠 이미지 생성에 활용. 로컬 텍스트-스케치 쌍으로 유니폼 디자인 등 세부 조절. Diffusion 모델 통합으로 RK3588에서 실시간 생성 가능.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.18309v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.13185v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.13185v1" onchange="updateToolbar()">
          <span class="rank">10위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13185v1">FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>82.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 82.0%"></div>
          </div>
          <span class="score-detail">
            base:92 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.GR</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of &#34;appearance&#34; and &#34;motion&#34; provides a more robust and scalable pathway.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of &#34;appearance&#34; and &#34;motion&#34; provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 동영상 생성 제어 방법을 제안합니다. 핵심은 외형과 움직임 분해로, 객체 편집 및 카메라 제어를 유연히 수행합니다. 프로젝트의 핵심인 비디오 편집과 직접 관련되어 하이라이트 자동 생성에 필수적인 이유입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 스포츠 하이라이트 자동 편집에 적용. 다중 주파수 인코딩으로 세밀한 동작 분해, 깊이 인식 제어로 자연스러운 시퀀스 생성. RK3588에서 실시간 추론 가능한 구조로 최적화.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.13185v1">PDF</a>
          
          
          <a href="https://github.com/IGL-HKUST/FlexAM">Code</a>
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.11966v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.11966v1" onchange="updateToolbar()">
          <span class="rank">11위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11966v1">MING: An Automated CNN-to-Edge MLIR HLS framework</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>82.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 82.0%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.AR</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Driven by the increasing demand for low-latency and real-time processing, machine learning applications are steadily migrating toward edge computing platforms, where Field-Programmable Gate Arrays (FPGAs) are widely adopted for their energy efficiency compared to CPUs and GPUs. To generate high-performance and low-power FPGA designs, several frameworks built upon High Level Synthesis (HLS) vendor tools have been proposed, among which MLIR-based frameworks are gaining significant traction due to their extensibility and ease of use.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Driven by the increasing demand for low-latency and real-time processing, machine learning applications are steadily migrating toward edge computing platforms, where Field-Programmable Gate Arrays (FPGAs) are widely adopted for their energy efficiency compared to CPUs and GPUs. To generate high-performance and low-power FPGA designs, several frameworks built upon High Level Synthesis (HLS) vendor tools have been proposed, among which MLIR-based frameworks are gaining significant traction due to their extensibility and ease of use. However, existing state-of-the-art frameworks often overlook the stringent resource constraints of edge devices. To address this limitation, we propose MING, an Multi-Level Intermediate Representation (MLIR)-based framework that abstracts and automates the HLS design process. Within this framework, we adopt a streaming architecture with carefully managed buffers, specifically designed to handle resource constraints while ensuring low-latency. In comparison with recent frameworks, our approach achieves on average 15x speedup for standard Convolutional Neural Network (CNN) kernels with up to four layers, and up to 200x for single-layer kernels. For kernels with larger input sizes, MING is capable of generating efficient designs that respect hardware resource constraints, whereas state-of-the-art frameworks struggle to meet.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 에지 디바이스용 CNN 배포 최적화 프레임워크로 저지연 스트리밍 처리와 자원 제약 해결. 프로젝트의 rk3588 기반 실시간 스포츠 영상 분석 핵심 기술에 직접 적용 가능해 중요함.</p>
        

        
        <p><strong>활용 인사이트:</strong> MING을 rk3588에 통합해 동작 인식 CNN 모델의 추론 속도 향상. 버퍼 관리로 15ms 이하 latency 달성하며 fps 60 이상 실시간 처리 보장.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.11966v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.13052v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.13052v1" onchange="updateToolbar()">
          <span class="rank">12위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13052v1">Quantization-Aware Collaborative Inference for Large Embodied AI Models</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>82.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 82.0%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">eess.SP</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents. To address this issue, we investigate quantization-aware collaborative inference (co-inference) for embodied AI systems. First, we develop a tractable approximation for quantization-induced inference distortion. Based on this approximation, we derive lower and upper bounds on the quantization rate-inference distortion function, characterizing its dependence on LAIM statistics, including the quantization bit-width. Next, we formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints, aiming to minimize the distortion upper bound while ensuring tightness through the corresponding lower bound. Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the proposed joint design. Particularly, simulations and real-world testbed experiments demonstrate the effectiveness of the proposed joint design in balancing inference quality, latency, and energy consumption in edge embodied AI systems.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 엣지 AI 모델의 양자화 협력 추론 기술로 자원 제약 환경 최적화. 스포츠 디바이스에서 대형 모델 효율적 실행에 필수적이라 중요함.</p>
        

        
        <p><strong>활용 인사이트:</strong> 동작 분석 LAIM 모델에 적용해 4-bit 양자화로 파라미터 70% 축소. 에너지 제약 내에서 inference speed 2x 향상 및 latency 30ms 달성.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.13052v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.14302v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.14302v1" onchange="updateToolbar()">
          <span class="rank">13위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14302v1">Floe: Federated Specialization for Real-Time LLM-SLM Inference</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>82.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 82.0%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.DC</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 에지-클라우드 연합 LLM/SLM 실시간 추론 솔루션. 스포츠 경기 전략 분석을 위한 저지연 언어 모델 핵심에 직접 연관됨.</p>
        

        
        <p><strong>활용 인사이트:</strong> Floe 구조로 SLM을 디바이스에 배치해 개인화 분석. 로짓 융합으로 cloud 연동 시 latency 50ms 이하 유지하며 real-time 피드백 가능.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.14302v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.18199v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.18199v1" onchange="updateToolbar()">
          <span class="rank">14위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18199v1">A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>81.6</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 81.6%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 자가 지도 동작 보정 기술로 물리적 타당성 향상. 스포츠 동작 분석 시 발 떠림 같은 오류 보정에 직접 활용 가능해 중요함.</p>
        

        
        <p><strong>활용 인사이트:</strong> DMC 모듈로 촬영된 운동 동작 보정 적용. foot floating 오류 90% 감소시키며 semantic 일관성 유지해 분석 정확도 향상.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.18199v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.16545v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.16545v1" onchange="updateToolbar()">
          <span class="rank">15위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.16545v1">Let&#39;s Split Up: Zero-Shot Classifier Edits for Fine-Grained Video Understanding</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>80.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 80.0%"></div>
          </div>
          <span class="score-detail">
            base:82 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly. To address these challenges, we introduce category splitting, a new task where an existing classifier is edited to refine a coarse category into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video classifiers to expose fine-grained distinctions without additional data. We further show that low-shot fine-tuning, while simple, is highly effective and benefits from our zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest. Project page: https://kaitingliu.github.io/Category-Splitting/.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 제로샷 비디오 분류기 미세 조정 기술. 스포츠 장면 세부 인식(예: 슛 종류 분류) 능력 향상에 직접 적용 가능함.</p>
        

        
        <p><strong>활용 인사이트:</strong> 기존 모델 분할로 신규 동작 카테고리 실시간 인식. 5-shot fine-tuning으로 세부 동작 분류 정확도 25% 상승시키며 latency 변화 없음.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.16545v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.17997v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.17997v1" onchange="updateToolbar()">
          <span class="rank">16위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.17997v1">Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>80.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 80.0%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly&#39;s brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 생물학적 신경망 기반 운동 제어 기술이 스포츠 동작 분석 및 하드웨어 개발에 적용 가능합니다.</p>
        

        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.17997v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.12080v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.12080v1" onchange="updateToolbar()">
          <span class="rank">17위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12080v1">PathCRF: Ball-Free Soccer Event Detection via Possession Path Inference from Player Trajectories</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>78.8</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 78.8%"></div>
          </div>
          <span class="score-detail">
            base:88 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Despite recent advances in AI, event data collection in soccer still relies heavily on labor-intensive manual annotation. Although prior work has explored automatic event detection using player and ball trajectories, ball tracking also remains difficult to scale due to high infrastructural and operational costs.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Despite recent advances in AI, event data collection in soccer still relies heavily on labor-intensive manual annotation. Although prior work has explored automatic event detection using player and ball trajectories, ball tracking also remains difficult to scale due to high infrastructural and operational costs. As a result, comprehensive data collection in soccer is largely confined to top-tier competitions, limiting the broader adoption of data-driven analysis in this domain. To address this challenge, this paper proposes PathCRF, a framework for detecting on-ball soccer events using only player tracking data. We model player trajectories as a fully connected dynamic graph and formulate event detection as the problem of selecting exactly one edge corresponding to the current possession state at each time step. To ensure logical consistency of the resulting edge sequence, we employ a Conditional Random Field (CRF) that forbids impossible transitions between consecutive edges. Both emission and transition scores dynamically computed from edge embeddings produced by a Set Attention-based backbone architecture. During inference, the most probable edge sequence is obtained via Viterbi decoding, and events such as ball controls or passes are detected whenever the selected edge changes between adjacent time steps. Experiments show that PathCRF produces accurate, logically consistent possession paths, enabling reliable downstream analyses while substantially reducing the need for manual event annotation. The source code is available at https://github.com/hyunsungkim-ds/pathcrf.git.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 축구 경기에서 공 추적 없이 선수 이동 데이터만으로 패스, 슛 같은 주요 이벤트를 감지하는 기술로, 우리 장비의 자동 하이라이트 편집 기능에 직접 적용 가능합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> RK3588 칩에 PathCRF 모델을 최적화해 탑재합니다. 경기 영상에서 실시간으로 선수 궤적을 분석해 패스/슛 이벤트를 감지하고, 감지된 이벤트 기반으로 자동 하이라이트 클립을 생성합니다. 목표 추론 속도 30fps.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.12080v1">PDF</a>
          
          
          <a href="https://github.com/hyunsungkim-ds/pathcrf.git">Code</a>
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.18057v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.18057v1" onchange="updateToolbar()">
          <span class="rank">18위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18057v1">Temporal Consistency-Aware Text-to-Motion Generation</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>78.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 78.4%"></div>
          </div>
          <span class="score-detail">
            base:75 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> Text-to-motion generation applicable for posture/movement analysis in sports.</p>
        

        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.18057v1">PDF</a>
          
          
          <a href="https://github.com/Giat995/TCA-T2M/">Code</a>
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.18394v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.18394v1" onchange="updateToolbar()">
          <span class="rank">19위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18394v1">Self-Aware Object Detection via Degradation Manifolds</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>78.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 78.4%"></div>
          </div>
          <span class="score-detail">
            base:78 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector&#39;s nominal operating regime.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector&#39;s nominal operating regime. We refer to this capability as self-aware object detection.   We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector&#39;s feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.   To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.   Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 스포츠 영상 촬영 시 날씨나 환경 변화로 인한 화질 열화 문제를 감지하는 기술이 핵심입니다. 다양한 조건에서 안정적인 영상 품질을 보장해야 하므로 프로젝트에 중요합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 실시간 촬영 중 열화 정도를 측정해 자동 보정 알고리즘을 활성화합니다. 악천후나 저조도 환경에서 화질 저하를 즉시 감지해 보정 효율성을 높입니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.18394v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.13476v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.13476v1" onchange="updateToolbar()">
          <span class="rank">20위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13476v1">AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>78.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 78.0%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Robotic foundation models achieve strong generalization by leveraging internet-scale vision-language representations, but their massive computational cost creates a fundamental bottleneck: high inference latency. In dynamic environments, this latency breaks the control loop, rendering powerful models unsafe for real-time deployment.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Robotic foundation models achieve strong generalization by leveraging internet-scale vision-language representations, but their massive computational cost creates a fundamental bottleneck: high inference latency. In dynamic environments, this latency breaks the control loop, rendering powerful models unsafe for real-time deployment. We propose AsyncVLA, an asynchronous control framework that decouples semantic reasoning from reactive execution. Inspired by hierarchical control, AsyncVLA runs a large foundation model on a remote workstation to provide high-level guidance, while a lightweight, onboard Edge Adapter continuously refines actions at high frequency. To bridge the domain gap between these asynchronous streams, we introduce an end-to-end finetuning protocol and a trajectory re-weighting strategy that prioritizes dynamic interactions. We evaluate our approach on real-world vision-based navigation tasks with communication delays up to 6 seconds. AsyncVLA achieves a 40% higher success rate than state-of-the-art baselines, effectively bridging the gap between the semantic intelligence of large models and the reactivity required for edge robotics.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 에지 디바이스에서 대규모 모델의 지연 문제를 해결하는 비동기 제어 프레임워크로, RK3588 기반 실시간 스포츠 영상 분석에 필수적인 저지연 추론을 가능하게 합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> AsyncVLA 아키텍처를 적용해 영상 분석 AI를 워크스테이션(고성능 처리)과 RK3588 장치(실시간 실행)로 분리합니다. 고속 동작 추적 시 200ms 미만 지연 시간 유지하며 경기 전략 분석을 지원합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.13476v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.14003v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.14003v1" onchange="updateToolbar()">
          <span class="rank">21위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14003v1">Prompt-Driven Low-Altitude Edge Intelligence: Modular Agents and Generative Reasoning</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>78.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 78.0%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">The large artificial intelligence models (LAMs) show strong capabilities in perception, reasoning, and multi-modal understanding, and can enable advanced capabilities in low-altitude edge intelligence. However, the deployment of LAMs at the edge remains constrained by some fundamental limitations.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">The large artificial intelligence models (LAMs) show strong capabilities in perception, reasoning, and multi-modal understanding, and can enable advanced capabilities in low-altitude edge intelligence. However, the deployment of LAMs at the edge remains constrained by some fundamental limitations. First, tasks are rigidly tied to specific models, limiting the flexibility. Besides, the computational and memory demands of full-scale LAMs exceed the capacity of most edge devices. Moreover, the current inference pipelines are typically static, making it difficult to respond to real-time changes of tasks. To address these challenges, we propose a prompt-to-agent edge cognition framework (P2AECF), enabling the flexible, efficient, and adaptive edge intelligence. Specifically, P2AECF transforms high-level semantic prompts into executable reasoning workflows through three key mechanisms. First, the prompt-defined cognition parses task intent into abstract and model-agnostic representations. Second, the agent-based modular execution instantiates these tasks using lightweight and reusable cognitive agents dynamically selected based on current resource conditions. Third, the diffusion-controlled inference planning adaptively constructs and refines execution strategies by incorporating runtime feedback and system context. In addition, we illustrate the framework through a representative low-altitude intelligent network use case, showing its ability to deliver adaptive, modular, and scalable edge intelligence for real-time low-altitude aerial collaborations.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 에지 디바이스용 실시간 모듈형 AI 프레임워크를 제안합니다. 핵심은 경량 에이전트와 동적 자원 관리로 유연한 인지 작업을 가능케 하는 것입니다. 우리 프로젝트에 필수적인 이유는 RK3588 같은 제한된 에지 하드웨어에서 스포츠 영상 분석을 효율적으로 실행할 수 있기 때문입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> P2AECF를 RK3588에 적용해 경기 장면별로 모듈을 동적 선택합니다. 예를 들어, 실시간 자원(fps, latency)을 모니터링하며 하이라이트 추출 에이전트를 활성화해 inference speed 20ms 이하로 유지합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.14003v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.17909v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.17909v1" onchange="updateToolbar()">
          <span class="rank">22위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.17909v1">A Single Image and Multimodality Is All You Need for Novel View Synthesis</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>78.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 78.0%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 레이더/LiDAR로 깊이 맵을 개선하는 뷰 합성 기술을 제안합니다. 핵심은 희소 데이터로 정확한 3D 재구성을 가능케 하는 것입니다. 우리 프로젝트에 중요한 이유는 스포츠 하이라이트의 다각도 시점 변환 시 영상 품질을 보장하기 때문입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 스포츠 장비에 초소형 레이더를 부착해 깊이 데이터를 수집합니다. RK3588에서 30fps로 동작해 축구 슈팅 장면의 저조도 환경에서도 안정적인 합성 영상을 생성합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.17909v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.13030v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.13030v1" onchange="updateToolbar()">
          <span class="rank">23위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13030v1">Resource-Efficient Gesture Recognition through Convexified Attention</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>78.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 78.0%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+10
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.HC</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Wearable e-textile interfaces require gesture recognition capabilities but face severe constraints in power consumption, computational capacity, and form factor that make traditional deep learning impractical. While lightweight architectures like MobileNet improve efficiency, they still demand thousands of parameters, limiting deployment on textile-integrated platforms.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Wearable e-textile interfaces require gesture recognition capabilities but face severe constraints in power consumption, computational capacity, and form factor that make traditional deep learning impractical. While lightweight architectures like MobileNet improve efficiency, they still demand thousands of parameters, limiting deployment on textile-integrated platforms. We introduce a convexified attention mechanism for wearable applications that dynamically weights features while preserving convexity through nonexpansive simplex projection and convex loss functions. Unlike conventional attention mechanisms using non-convex softmax operations, our approach employs Euclidean projection onto the probability simplex combined with multi-class hinge loss, ensuring global convergence guarantees. Implemented on a textile-based capacitive sensor with four connection points, our approach achieves 100.00\% accuracy on tap gestures and 100.00\% on swipe gestures -- consistent across 10-fold cross-validation and held-out test evaluation -- while requiring only 120--360 parameters, a 97\% reduction compared to conventional approaches. With sub-millisecond inference times (290--296$μ$s) and minimal storage requirements ($&lt;$7KB), our method enables gesture interfaces directly within e-textiles without external processing. Our evaluation, conducted in controlled laboratory conditions with a single-user dataset, demonstrates feasibility for basic gesture interactions. Real-world deployment would require validation across multiple users, environmental conditions, and more complex gesture vocabularies. These results demonstrate how convex optimization can enable efficient on-device machine learning for textile interfaces.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 360개 파라미터만으로 99% 정확도의 제스처 인식법을 제안합니다. 핵심은 컨벡스 최적화로 초경량 모델을 구축하는 것입니다. 우리 프로젝트에 필수적인 이유는 에지 디바이스에서 스포츠 동작 분석을 초저전력(290μs latency)으로 수행 가능하기 때문입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> RK3588에 이 방식을 적용해 농구 드리블 동작을 실시간 인식합니다. 센서 데이터로 500ms 내 분석해 훈련 오버헤드 없이 선수별 동작 패턴을 추출합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.13030v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.12983v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.12983v1" onchange="updateToolbar()">
          <span class="rank">24위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12983v1">Detecting Object Tracking Failure via Sequential Hypothesis Testing</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>76.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 76.4%"></div>
          </div>
          <span class="score-detail">
            base:83 + bonus:+5
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          <span class="flag-tag flag-realtime">실시간</span>
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts. To obtain such assurances we propose interpreting object tracking as a sequential hypothesis test, wherein evidence for or against tracking failures is gradually accumulated over time. Leveraging recent advancements in the field, our sequential test (formalized as an e-process) quickly identifies when tracking failures set in whilst provably containing false alerts at a desired rate, and thus limiting potentially costly re-calibration or intervention steps. The approach is computationally light-weight, requires no extra training or fine-tuning, and is in principle model-agnostic. We propose both supervised and unsupervised variants by leveraging either ground-truth or solely internal tracking information, and demonstrate its effectiveness for two established tracking models across four video benchmarks. As such, sequential testing can offer a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 객체 추적 실패를 통계적으로 감지하는 방법을 제안합니다. 핵심은 순차적 가설 검정으로 오류를 조기 발견하는 것입니다. 우리 프로젝트에 중요한 이유는 축구 경기 영상에서 선수 추적 안정성을 보장해 하이라이트 오류를 줄이기 때문입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> 추적 알고리즘과 통합해 실시간으로 신뢰도 점수를 계산합니다. fps 60 환경에서 5ms 내 경고를 발생시켜 스포츠 영상 편집 시 오류 프레임을 자동 제외합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.12983v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.11769v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.11769v1" onchange="updateToolbar()">
          <span class="rank">25위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11769v1">Light4D: Training-Free Extreme Viewpoint 4D Video Relighting</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>76.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 76.4%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, a novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance, a time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity. Second, to reinforce temporal consistency, we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive performance in temporal consistency and lighting fidelity, robustly handling camera rotations from -90 to 90. Code: https://github.com/AIGeeksGroup/Light4D. Website: https://aigeeksgroup.github.io/Light4D.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이 논문은 훈련 없이 4D 영상 재조명을 하는 Light4D를 제안합니다. 핵심은 시간적 일관성을 유지하며 극단적 시점에서 조명을 제어하는 것입니다. 우리 프로젝트에 필수적인 이유는 스포츠 하이라이트 영상의 조명 보정을 자동화해 SNS 공용 품질을 높이기 때문입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> RK3588에서 Light4D를 활용해 실내 배구 경기 영상을 25fps로 재조명합니다. -90°에서 90° 카메라 각도 변화에도 지연 50ms 이하로 자연스러운 보정 결과를 출력합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.11769v1">PDF</a>
          
          
          <a href="https://github.com/AIGeeksGroup/Light4D">Code</a>
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.13440v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.13440v1" onchange="updateToolbar()">
          <span class="rank">26위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13440v1">Learning on the Fly: Replay-Based Continual Object Perception for Indoor Drones</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>76.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 76.4%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+8
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        

        
        
        <div class="flags">
          
          <span class="flag-tag flag-edge">엣지</span>
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Autonomous agents such as indoor drones must learn new object classes in real-time while limiting catastrophic forgetting, motivating Class-Incremental Learning (CIL). However, most unmanned aerial vehicle (UAV) datasets focus on outdoor scenes and offer limited temporally coherent indoor videos.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Autonomous agents such as indoor drones must learn new object classes in real-time while limiting catastrophic forgetting, motivating Class-Incremental Learning (CIL). However, most unmanned aerial vehicle (UAV) datasets focus on outdoor scenes and offer limited temporally coherent indoor videos. We introduce an indoor dataset of $14,400$ frames capturing inter-drone and ground vehicle footage, annotated via a semi-automatic workflow with a $98.6\%$ first-pass labeling agreement before final manual verification. Using this dataset, we benchmark 3 replay-based CIL strategies: Experience Replay (ER), Maximally Interfered Retrieval (MIR), and Forgetting-Aware Replay (FAR), using YOLOv11-nano as a resource-efficient detector for deployment-constrained UAV platforms. Under tight memory budgets ($5-10\%$ replay), FAR performs better than the rest, achieving an average accuracy (ACC, $mAP_{50-95}$ across increments) of $82.96\%$ with $5\%$ replay. Gradient-weighted class activation mapping (Grad-CAM) analysis shows attention shifts across classes in mixed scenes, which is associated with reduced localization quality for drones. The experiments further demonstrate that replay-based continual learning can be effectively applied to edge aerial systems. Overall, this work contributes an indoor UAV video dataset with preserved temporal coherence and an evaluation of replay-based CIL under limited replay budgets. Project page: https://spacetime-vision-robotics-laboratory.github.io/learning-on-the-fly-cl</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 에지 디바이스에서 새로운 객체를 지속적으로 학습해야 하는 문제를 해결합니다. 메모리 제약이 있는 스포츠 장면 실시간 처리에 적용 가능하며, 장치의 지속 학습 기능 강화에 중요합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> FAR 리플레이 전략을 적용해 선수/장비 인식 모델을 점진적으로 업데이트합니다. 메모리 5-10% 예산 내에서 새 동작 유형 학습 시 정확도 유지하며 rk3588에 배포합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.13440v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.13344v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.13344v1" onchange="updateToolbar()">
          <span class="rank">27위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13344v1">FireRed-Image-Edit-1.0 Techinical Report</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>76.4</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 76.4%"></div>
          </div>
          <span class="score-detail">
            base:85 + bonus:+3
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">eess.IV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
          <span class="flag-tag flag-code">코드</span>
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation, training methodology, and evaluation design. We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation, training methodology, and evaluation design. We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO, DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench, a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks (ImgEdit and GEdit) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 이미지 보정 및 사진 변환 기능이 프로젝트 핵심입니다. 텍스트 지시 기반 고급 편집 기술로 스포츠 하이라이트 이미지 품질을 높입니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> FireRed 모델 통해 사용자가 &#39;동작 강조&#39; 등 텍스트 명령으로 영상 프레임 보정합니다. 다단계 훈련 파이프라인으로 장비 로고 삽입/피사체 강조 등 작업 최적화합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.13344v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.16160v2">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.16160v2" onchange="updateToolbar()">
          <span class="rank">28위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.16160v2">Uncertainty-Guided Inference-Time Depth Adaptation for Transformer-Based Visual Tracking</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>76.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 76.0%"></div>
          </div>
          <span class="score-detail">
            base:80 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Transformer-based single-object trackers achieve state-of-the-art accuracy but rely on fixed-depth inference, executing the full encoder--decoder stack for every frame regardless of visual complexity, thereby incurring unnecessary computational cost in long video sequences dominated by temporally coherent frames. We propose UncL-STARK, an architecture-preserving approach that enables dynamic, uncertainty-aware depth adaptation in transformer-based trackers without modifying the underlying network or adding auxiliary heads.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Transformer-based single-object trackers achieve state-of-the-art accuracy but rely on fixed-depth inference, executing the full encoder--decoder stack for every frame regardless of visual complexity, thereby incurring unnecessary computational cost in long video sequences dominated by temporally coherent frames. We propose UncL-STARK, an architecture-preserving approach that enables dynamic, uncertainty-aware depth adaptation in transformer-based trackers without modifying the underlying network or adding auxiliary heads. The model is fine-tuned to retain predictive robustness at multiple intermediate depths using random-depth training with knowledge distillation, thus enabling safe inference-time truncation. At runtime, we derive a lightweight uncertainty estimate directly from the model&#39;s corner localization heatmaps and use it in a feedback-driven policy that selects the encoder and decoder depth for the next frame based on the prediction confidence by exploiting temporal coherence in video. Extensive experiments on GOT-10k and LaSOT demonstrate up to 12% GFLOPs reduction, 8.9% latency reduction, and 10.8% energy savings while maintaining tracking accuracy within 0.2% of the full-depth baseline across both short-term and long-term sequences.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 실시간 스포츠 장면 처리 시 계산 효율성 문제를 해결합니다. 선수 트래킹 시 불필요한 연산을 줄여 에지 디바이스의 지연 시간을 개선합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> UncL-STACK 적용해 움직임 예측이 쉬운 프레임(예: 일정한 주행)에서 인코더 깊이를 동적으로 조절합니다. 불확실성 추정으로 GPU 사용량 10% 절감하며 8.9% 지연 감소합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.16160v2">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.18043v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.18043v1" onchange="updateToolbar()">
          <span class="rank">29위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18043v1">Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>76.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 76.0%"></div>
          </div>
          <span class="score-detail">
            base:75 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 소수 샘플로 스포츠 동작 분석이 가능한 시공간 인식 기술입니다. 신규 운동 유형 인식 시 데이터 부족 문제를 해결합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> DiST 프레임워크로 골프 스윙 등 동작을 공간(관절 위치)-시간(속도) 프로토타입으로 분해합니다. 언어 모델 속성 설명과 결합해 5개 샘플만으로 동작 패턴 인식합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.18043v1">PDF</a>
          
          
        </div>

        
      </div>
      
      <div class="paper-card paper-card--collapsed" data-paper-url="http://arxiv.org/abs/2602.18193v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="1" data-pdf="https://arxiv.org/pdf/2602.18193v1" onchange="updateToolbar()">
          <span class="rank">30위</span>
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18193v1">BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards</a>
          </span>
          
        </div>

        <div class="score-bar-container">
          <span>76.0</span>
          <div class="score-bar">
            <div class="score-bar-fill" style="width: 76.0%"></div>
          </div>
          <span class="score-detail">
            base:75 + bonus:+0
          </span>
        </div>

        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        

        
        
        <div class="flags">
          
          
          
        </div>
        

        
        <div class="abstract-section">
          <p class="abstract-preview">Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward.</p>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.</div>
          </details>
          
        </div>
        

        
        <p><strong>선정 근거:</strong> 플랫폼 내 광고 조정 기능 구현에 필수입니다. SNS 공유 콘텐츠의 부정확한 광고 요소를 멀티모달로 감지합니다.</p>
        

        
        <p><strong>활용 인사이트:</strong> BLM-Guard 통합해 영상/음성/자막 불일치(예: 과장된 성능 표기)를 실시간 감지합니다. 정책 정렬 보상 시스템으로 스포츠 용품 광고의 허위 내용을 자동 차단합니다.</p>
        

        <div class="paper-links">
          
          <a href="https://arxiv.org/pdf/2602.18193v1">PDF</a>
          
          
        </div>

        
      </div>
      

      
      <button class="show-more-btn" id="showMoreBtn" onclick="showAllTier1()">
        나머지 25편 더 보기
      </button>
      

      
      <div class="tier-divider">Tier 2</div>
      

      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14666v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14666v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14666v1">Real-time Monocular 2D and 3D Perception of Endoluminal Scenes for Controlling Flexible Robotic Endoscopic Instruments</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">76.0</span>
            
            <a href="https://arxiv.org/pdf/2602.14666v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Endoluminal surgery offers a minimally invasive option for early-stage gastrointestinal and urinary tract cancers but is limited by surgical tools and a steep learning curve. Robotic systems, particularly continuum robots, provide flexible instruments that enable precise tissue resection, potentially improving outcomes.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Endoluminal surgery offers a minimally invasive option for early-stage gastrointestinal and urinary tract cancers but is limited by surgical tools and a steep learning curve. Robotic systems, particularly continuum robots, provide flexible instruments that enable precise tissue resection, potentially improving outcomes. This paper presents a visual perception platform for a continuum robotic system in endoluminal surgery. Our goal is to utilize monocular endoscopic image-based perception algorithms to identify position and orientation of flexible instruments and measure their distances from tissues. We introduce 2D and 3D learning-based perception algorithms and develop a physically-realistic simulator that models flexible instruments dynamics. This simulator generates realistic endoluminal scenes, enabling control of flexible robots and substantial data collection. Using a continuum robot prototype, we conducted module and system-level evaluations. Results show that our algorithms improve control of flexible instruments, reducing manipulation time by over 70% for trajectory-following tasks and enhancing understanding of surgical scenarios, leading to robust endoluminal surgeries.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Real-time monocular perception applicable for sports instrument tracking.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.18252v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.18252v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18252v1">On the Adversarial Robustness of Discrete Image Tokenizers</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">76.0</span>
            
            <a href="https://arxiv.org/pdf/2602.18252v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">이미지 토크나이저의 강건성 연구로 영상 처리 기술에 적용 가능함.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13710v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13710v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13710v1">HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">75.6</span>
            
            <a href="https://arxiv.org/pdf/2602.13710v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existing methods fail to narrow the distribution gap between binarized and full-precision weights, causing quantization errors to accumulate under long-horizon closed-loop execution and severely degrade actions.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existing methods fail to narrow the distribution gap between binarized and full-precision weights, causing quantization errors to accumulate under long-horizon closed-loop execution and severely degrade actions. To fill this gap, we propose HBVLA, a VLA-tailored binarization framework. First, we use a policy-aware enhanced Hessian to identify weights that are truly critical for action generation. Then, we employ a sparse orthogonal transform for non-salient weights to induce a low-entropy intermediate state. Finally, we quantize both salient and non-salient weights in the Harr domain with group-wise 1-bit quantization. We have evaluated our approach on different VLAs: on LIBERO, quantized OpenVLA-OFT retains 92.2% of full-precision performance; on SimplerEnv, quantized CogAct retains 93.6%, significantly outperforming state-of-the-art binarization methods. We further validate our method on real-world evaluation suite and the results show that HBVLA incurs only marginal success-rate degradation compared to the full-precision model, demonstrating robust deployability under tight hardware constraints. Our work provides a practical foundation for ultra-low-bit quantization of VLAs, enabling more reliable deployment on hardware-limited robotic platforms.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Edge quantization</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.16092v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.16092v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.16092v1">Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">75.6</span>
            
            <a href="https://arxiv.org/pdf/2602.16092v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.CL</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Efficient generative models applicable to video/image editing.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12173v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12173v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12173v1">SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">74.8</span>
            
            <a href="https://arxiv.org/pdf/2602.12173v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading to substantial over-provisioning in text encoder capacity and persistent computational and memory overhead.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading to substantial over-provisioning in text encoder capacity and persistent computational and memory overhead. In this paper, we perform a large-scale anatomical analysis of text prompting in vision-language segmentation, covering 404,796 real prompts across multiple benchmarks. Our analysis reveals severe redundancy: most context windows are underutilized, vocabulary usage is highly sparse, and text embeddings lie on low-dimensional manifold despite high-dimensional representations. Motivated by these findings, we propose SAM3-LiteText, a lightweight text encoding framework that replaces the original SAM3 text encoder with a compact MobileCLIP student that is optimized by knowledge distillation. Extensive experiments on image and video segmentation benchmarks show that SAM3-LiteText reduces text encoder parameters by up to 88%, substantially reducing static memory footprint, while maintaining segmentation performance comparable to the original model. Code: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">경량 비전-언어 모델 기술이 엣지 디바이스 기반 스포츠 영상 분석에 적용 가능.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.15030v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.15030v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.15030v1">Image Generation with a Sphere Encoder</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">74.4</span>
            
            <a href="https://arxiv.org/pdf/2602.15030v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">효율적 이미지 생성 기술이 영상 보정 및 변환에 직접 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12978v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12978v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12978v1">Learning Native Continuation for Action Chunking Flow Policies</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">74.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12978v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">실시간 동작 처리 기술이 스포츠 동작 분석에 직접 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14157v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14157v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14157v1">When Test-Time Guidance Is Enough: Fast Image and Video Editing with Diffusion Guidance</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">74.0</span>
            
            <a href="https://arxiv.org/pdf/2602.14157v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability. Building upon the recent work of Moufad et al. (2025), we provide theoretical insights into their VJP-free approximation and substantially extend their empirical evaluation to large-scale image and video editing benchmarks. Our results demonstrate that test-time guidance alone can achieve performance comparable to, and in some cases surpass, training-based methods.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">빠른 이미지 및 비디오 편집 기술이 프로젝트의 보정 기능과 직접 관련 있음.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11810v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11810v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11810v1">How to Sample High Quality 3D Fractals for Action Recognition Pre-Training?</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">74.0</span>
            
            <a href="https://arxiv.org/pdf/2602.11810v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Synthetic datasets are being recognized in the deep learning realm as a valuable alternative to exhaustively labeled real data. One such synthetic data generation method is Formula Driven Supervised Learning (FDSL), which can provide an infinite number of perfectly labeled data through a formula driven approach, such as fractals or contours.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Synthetic datasets are being recognized in the deep learning realm as a valuable alternative to exhaustively labeled real data. One such synthetic data generation method is Formula Driven Supervised Learning (FDSL), which can provide an infinite number of perfectly labeled data through a formula driven approach, such as fractals or contours. FDSL does not have common drawbacks like manual labor, privacy and other ethical concerns. In this work we generate 3D fractals using 3D Iterated Function Systems (IFS) for pre-training an action recognition model. The fractals are temporally transformed to form a video that is used as a pre-training dataset for downstream task of action recognition. We find that standard methods of generating fractals are slow and produce degenerate 3D fractals. Therefore, we systematically explore alternative ways of generating fractals and finds that overly-restrictive approaches, while generating aesthetically pleasing fractals, are detrimental for downstream task performance. We propose a novel method, Targeted Smart Filtering, to address both the generation speed and fractal diversity issue. The method reports roughly 100 times faster sampling speed and achieves superior downstream performance against other 3D fractal filtering methods.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Synthetic data for action recognition; directly relevant to motion analysis.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14107v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14107v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14107v1">ML-ECS: A Collaborative Multimodal Learning Framework for Edge-Cloud Synergies</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">74.0</span>
            
            <a href="https://arxiv.org/pdf/2602.14107v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.DC</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Edge-cloud synergies provide a promising paradigm for privacy-preserving deployment of foundation models, where lightweight on-device models adapt to domain-specific data and cloud-hosted models coordinate knowledge sharing. However, in real-world edge environments, collaborative multimodal learning is challenged by modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different modality-specific encoders/fusion modules.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Edge-cloud synergies provide a promising paradigm for privacy-preserving deployment of foundation models, where lightweight on-device models adapt to domain-specific data and cloud-hosted models coordinate knowledge sharing. However, in real-world edge environments, collaborative multimodal learning is challenged by modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different modality-specific encoders/fusion modules. To address these issues, we propose ML-ECS, a collaborative multimodal learning framework that enables joint training between a server-based model and heterogeneous edge models. This framework consists of four components: (1) cross-modal contrastive learning (CCL) to align modality representations in a shared latent space, (2) adaptive multimodal tuning (AMT) to preserve domain-specific knowledge from local datasets, (3) modality-aware model aggregation (MMA) to robustly aggregate while mitigating noise caused by missing modalities, and (4) SLM-enhanced CCL (SE-CCL) to facilitate bidirectional knowledge transfer between cloud and edge. Experimental results on various multimodal tasks show that \pname consistently outperform state-of-the-art baselines under varying modality availability, achieving improvements of 5.44% to 12.08% in Rouge-LSum and improving both client- and server-side performance. In addition, by communicating only low-rank LoRA parameters and fused representations, ML-ECS achieves high communication efficiency, requiring only 0.65% of the total parameter volume.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">엣지-클라우드 협업 프레임워크로 엣지 디바이스에 적용 가능.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12942v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12942v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12942v1">HoRAMA: Holistic Reconstruction with Automated Material Assignment for Ray Tracing using NYURay</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">74.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12942v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">eess.SP</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Next-generation wireless networks at upper mid-band and millimeter-wave frequencies require accurate site-specific deterministic channel propagation prediction. Wireless ray tracing (RT) provides site-specific predictions but demands high-fidelity three-dimensional (3D) environment models with material properties.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Next-generation wireless networks at upper mid-band and millimeter-wave frequencies require accurate site-specific deterministic channel propagation prediction. Wireless ray tracing (RT) provides site-specific predictions but demands high-fidelity three-dimensional (3D) environment models with material properties. Manual 3D model reconstruction achieves high accuracy but requires weeks of expert effort, creating scalability bottlenecks for large environment reconstruction. Traditional vision-based 3D reconstruction methods lack RT compatibility due to geometrically defective meshes and missing material properties. This paper presents Holistic Reconstruction with Automated Material Assignment (HoRAMA) for wireless propagation prediction using NYURay. HoRAMA generates RT-compatible 3D models from RGB video readily captured using a smartphone or low-cost portable camera, by integrating MASt3R-SLAM dense point cloud generation with vision language model-assisted material assignment. The HoRAMA 3D reconstruction method is verified by comparing NYURay RT predictions, using both manually created and HoRAMA-generated 3D models, against field measurements at 6.75 GHz and 16.95 GHz across 12 TX-RX locations in a 700 square meter factory. HoRAMA ray tracing predictions achieve a 2.28 dB RMSE for matched multipath component (MPC) power predictions, comparable to the manually created 3D model baseline (2.18 dB), while reducing 3D reconstruction time from two months to 16 hours. HoRAMA enables scalable wireless digital twin creation for RT network planning, infrastructure deployment, and beam management in 5G/6G systems, as well as eventual real-time implementation at the edge.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">RGB 비디오로부터 3D 모델 재구성. 스포츠 장면 분석에 적용 가능한 기술.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14040v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14040v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14040v1">Explainability-Inspired Layer-Wise Pruning of Deep Neural Networks for Efficient Object Detection</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">74.0</span>
            
            <a href="https://arxiv.org/pdf/2602.14040v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Deep neural networks (DNNs) have achieved remarkable success in object detection tasks, but their increasing complexity poses significant challenges for deployment on resource-constrained platforms. While model compression techniques such as pruning have emerged as essential tools, traditional magnitude-based pruning methods do not necessarily align with the true functional contribution of network components to task-specific performance.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Deep neural networks (DNNs) have achieved remarkable success in object detection tasks, but their increasing complexity poses significant challenges for deployment on resource-constrained platforms. While model compression techniques such as pruning have emerged as essential tools, traditional magnitude-based pruning methods do not necessarily align with the true functional contribution of network components to task-specific performance. In this work, we present an explainability-inspired, layer-wise pruning framework tailored for efficient object detection. Our approach leverages a SHAP-inspired gradient--activation attribution to estimate layer importance, providing a data-driven proxy for functional contribution rather than relying solely on static weight magnitudes. We conduct comprehensive experiments across diverse object detection architectures, including ResNet-50, MobileNetV2, ShuffleNetV2, Faster R-CNN, RetinaNet, and YOLOv8, evaluating performance on the Microsoft COCO 2017 validation set. The results show that the proposed attribution-inspired pruning consistently identifies different layers as least important compared to L1-norm-based methods, leading to improved accuracy--efficiency trade-offs. Notably, for ShuffleNetV2, our method yields a 10\% empirical increase in inference speed, whereas L1-pruning degrades performance by 13.7\%. For RetinaNet, the proposed approach preserves the baseline mAP (0.151) with negligible impact on inference speed, while L1-pruning incurs a 1.3\% mAP drop for a 6.2\% speed increase. These findings highlight the importance of data-driven layer importance assessment and demonstrate that explainability-inspired compression offers a principled direction for deploying deep neural networks on edge and resource-constrained platforms while preserving both performance and interpretability.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Edge-efficient object detection</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14042v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14042v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14042v1">Restoration Adaptation for Semantic Segmentation on Low Quality Images</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">74.0</span>
            
            <a href="https://arxiv.org/pdf/2602.14042v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">In real-world scenarios, the performance of semantic segmentation often deteriorates when processing low-quality (LQ) images, which may lack clear semantic structures and high-frequency details. Although image restoration techniques offer a promising direction for enhancing degraded visual content, conventional real-world image restoration (Real-IR) models primarily focus on pixel-level fidelity and often fail to recover task-relevant semantic cues, limiting their effectiveness when directly applied to downstream vision tasks.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">In real-world scenarios, the performance of semantic segmentation often deteriorates when processing low-quality (LQ) images, which may lack clear semantic structures and high-frequency details. Although image restoration techniques offer a promising direction for enhancing degraded visual content, conventional real-world image restoration (Real-IR) models primarily focus on pixel-level fidelity and often fail to recover task-relevant semantic cues, limiting their effectiveness when directly applied to downstream vision tasks. Conversely, existing segmentation models trained on high-quality data lack robustness under real-world degradations. In this paper, we propose Restoration Adaptation for Semantic Segmentation (RASS), which effectively integrates semantic image restoration into the segmentation process, enabling high-quality semantic segmentation on the LQ images directly. Specifically, we first propose a Semantic-Constrained Restoration (SCR) model, which injects segmentation priors into the restoration model by aligning its cross-attention maps with segmentation masks, encouraging semantically faithful image reconstruction. Then, RASS transfers semantic restoration knowledge into segmentation through LoRA-based module merging and task-specific fine-tuning, thereby enhancing the model&#39;s robustness to LQ images. To validate the effectiveness of our framework, we construct a real-world LQ image segmentation dataset with high-quality annotations, and conduct extensive experiments on both synthetic and real-world LQ benchmarks. The results show that SCR and RASS significantly outperform state-of-the-art methods in segmentation and restoration tasks. Code, models, and datasets will be available at https://github.com/Ka1Guan/RASS.git.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Semantic segmentation adaptation for low-quality images relevant to sports video correction.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14837v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14837v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14837v1">Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">72.8</span>
            
            <a href="https://arxiv.org/pdf/2602.14837v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">단기적 객체 상호작용 예측 방법론이 스포츠 장면 분석에 적용 가능함.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14186v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14186v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14186v1">UniRef-Image-Edit: Towards Scalable and Consistent Multi-Reference Image Editing</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">72.4</span>
            
            <a href="https://arxiv.org/pdf/2602.14186v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We present UniRef-Image-Edit, a high-performance multi-modal generation system that unifies single-image editing and multi-image composition within a single framework. Existing diffusion-based editing methods often struggle to maintain consistency across multiple conditions due to limited interaction between reference inputs.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We present UniRef-Image-Edit, a high-performance multi-modal generation system that unifies single-image editing and multi-image composition within a single framework. Existing diffusion-based editing methods often struggle to maintain consistency across multiple conditions due to limited interaction between reference inputs. To address this, we introduce Sequence-Extended Latent Fusion (SELF), a unified input representation that dynamically serializes multiple reference images into a coherent latent sequence. During a dedicated training stage, all reference images are jointly constrained to fit within a fixed-length sequence under a global pixel-budget constraint. Building upon SELF, we propose a two-stage training framework comprising supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we jointly train on single-image editing and multi-image composition tasks to establish a robust generative prior. We adopt a progressive sequence length training strategy, in which all input images are initially resized to a total pixel budget of $1024^2$, and are then gradually increased to $1536^2$ and $2048^2$ to improve visual fidelity and cross-reference consistency. This gradual relaxation of compression enables the model to incrementally capture finer visual details while maintaining stable alignment across references. For the RL stage, we introduce Multi-Source GRPO (MSGRPO), to our knowledge the first reinforcement learning framework tailored for multi-reference image generation. MSGRPO optimizes the model to reconcile conflicting visual constraints, significantly enhancing compositional consistency. We will open-source the code, models, training data, and reward data for community research purposes.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">다중 참조 이미지 편집 기술이 영상 보정에 직접 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14178v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14178v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14178v1">UniWeTok: An Unified Binary Tokenizer with Codebook Size $\mathit{2^{128}}$ for Unified Multimodal Large Language Model</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">72.4</span>
            
            <a href="https://arxiv.org/pdf/2602.14178v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook ($\mathit{2^{128}}$). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok&#39;s adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">통합 멀티모달 토크나이저로 영상 생성 및 분석에 직접 활용 가능함.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.16711v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.16711v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.16711v1">TeCoNeRV: Leveraging Temporal Coherence for Compressible Neural Representations for Videos</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">72.0</span>
            
            <a href="https://arxiv.org/pdf/2602.16711v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Implicit Neural Representations (INRs) have recently demonstrated impressive performance for video compression. However, since a separate INR must be overfit for each video, scaling to high-resolution videos while maintaining encoding efficiency remains a significant challenge.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Implicit Neural Representations (INRs) have recently demonstrated impressive performance for video compression. However, since a separate INR must be overfit for each video, scaling to high-resolution videos while maintaining encoding efficiency remains a significant challenge. Hypernetwork-based approaches predict INR weights (hyponetworks) for unseen videos at high speeds, but with low quality, large compressed size, and prohibitive memory needs at higher resolutions. We address these fundamental limitations through three key contributions: (1) an approach that decomposes the weight prediction task spatially and temporally, by breaking short video segments into patch tubelets, to reduce the pretraining memory overhead by 20$\times$; (2) a residual-based storage scheme that captures only differences between consecutive segment representations, significantly reducing bitstream size; and (3) a temporal coherence regularization framework that encourages changes in the weight space to be correlated with video content. Our proposed method, TeCoNeRV, achieves substantial improvements of 2.47dB and 5.35dB PSNR over the baseline at 480p and 720p on UVG, with 36% lower bitrates and 1.5-3$\times$ faster encoding speeds. With our low memory usage, we are the first hypernetwork approach to demonstrate results at 480p, 720p and 1080p on UVG, HEVC and MCL-JCV. Our project page is available at https://namithap10.github.io/teconerv/ .</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">비디오 압축 신경 표현 기술로 영상 보정/저장에 직접 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11850v2">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11850v2" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11850v2">Free Lunch for Stabilizing Rectified Flow Inversion</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">71.6</span>
            
            <a href="https://arxiv.org/pdf/2602.11850v2" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Rectified-Flow (RF)-based generative models have recently emerged as strong alternatives to traditional diffusion models, demonstrating state-of-the-art performance across various tasks. By learning a continuous velocity field that transforms simple noise into complex data, RF-based models not only enable high-quality generation, but also support training-free inversion, which facilitates downstream tasks such as reconstruction and editing.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Rectified-Flow (RF)-based generative models have recently emerged as strong alternatives to traditional diffusion models, demonstrating state-of-the-art performance across various tasks. By learning a continuous velocity field that transforms simple noise into complex data, RF-based models not only enable high-quality generation, but also support training-free inversion, which facilitates downstream tasks such as reconstruction and editing. However, existing inversion methods, such as vanilla RF-based inversion, suffer from approximation errors that accumulate across timesteps, leading to unstable velocity fields and degraded reconstruction and editing quality. To address this challenge, we propose Proximal-Mean Inversion (PMI), a training-free gradient correction method that stabilizes the velocity field by guiding it toward a running average of past velocities, constrained within a theoretically derived spherical Gaussian. Furthermore, we introduce mimic-CFG, a lightweight velocity correction scheme for editing tasks, which interpolates between the current velocity and its projection onto the historical average, balancing editing effectiveness and structural consistency. Extensive experiments on PIE-Bench demonstrate that our methods significantly improve inversion stability, image reconstruction quality, and editing fidelity, while reducing the required number of neural function evaluations. Our approach achieves state-of-the-art performance on the PIE-Bench with enhanced efficiency and theoretical soundness.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">이미지 재구성 및 편집 기술이 프로젝트의 보정 기능에 적용 가능함.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12461v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12461v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12461v1">Semantic-aware Adversarial Fine-tuning for CLIP</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.8</span>
            
            <a href="https://arxiv.org/pdf/2602.12461v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Recent studies have shown that CLIP model&#39;s adversarial robustness in zero-shot classification tasks can be enhanced by adversarially fine-tuning its image encoder with adversarial examples (AEs), which are generated by minimizing the cosine similarity between images and a hand-crafted template (e.g., &#39;&#39;A photo of a {label}&#39;&#39;). However, it has been shown that the cosine similarity between a single image and a single hand-crafted template is insufficient to measure the similarity for image-text pairs.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent studies have shown that CLIP model&#39;s adversarial robustness in zero-shot classification tasks can be enhanced by adversarially fine-tuning its image encoder with adversarial examples (AEs), which are generated by minimizing the cosine similarity between images and a hand-crafted template (e.g., &#39;&#39;A photo of a {label}&#39;&#39;). However, it has been shown that the cosine similarity between a single image and a single hand-crafted template is insufficient to measure the similarity for image-text pairs. Building on this, in this paper, we find that the AEs generated using cosine similarity may fail to fool CLIP when the similarity metric is replaced with semantically enriched alternatives, making the image encoder fine-tuned with these AEs less robust. To overcome this issue, we first propose a semantic-ensemble attack to generate semantic-aware AEs by minimizing the average similarity between the original image and an ensemble of refined textual descriptions. These descriptions are initially generated by a foundation model to capture core semantic features beyond hand-crafted templates and are then refined to reduce hallucinations. To this end, we propose Semantic-aware Adversarial Fine-Tuning (SAFT), which fine-tunes CLIP&#39;s image encoder with semantic-aware AEs. Extensive experiments show that SAFT outperforms current methods, achieving substantial improvements in zero-shot adversarial robustness across 16 datasets. Our code is available at: https://github.com/tmlr-group/SAFT.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Adversarial fine-tuning for vision-language models applicable to image analysis components.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11714v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11714v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11714v1">GSO-SLAM: Bidirectionally Coupled Gaussian Splatting and Direct Visual Odometry</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.0</span>
            
            <a href="https://arxiv.org/pdf/2602.11714v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We propose GSO-SLAM, a real-time monocular dense SLAM system that leverages Gaussian scene representation. Unlike existing methods that couple tracking and mapping with a unified scene, incurring computational costs, or loosely integrate them with well-structured tracking frameworks, introducing redundancies, our method bidirectionally couples Visual Odometry (VO) and Gaussian Splatting (GS).</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We propose GSO-SLAM, a real-time monocular dense SLAM system that leverages Gaussian scene representation. Unlike existing methods that couple tracking and mapping with a unified scene, incurring computational costs, or loosely integrate them with well-structured tracking frameworks, introducing redundancies, our method bidirectionally couples Visual Odometry (VO) and Gaussian Splatting (GS). Specifically, our approach formulates joint optimization within an Expectation-Maximization (EM) framework, enabling the simultaneous refinement of VO-derived semi-dense depth estimates and the GS representation without additional computational overhead. Moreover, we present Gaussian Splat Initialization, which utilizes image information, keyframe poses, and pixel associations from VO to produce close approximations to the final Gaussian scene, thereby eliminating the need for heuristic methods. Through extensive experiments, we validate the effectiveness of our method, showing that it not only operates in real time but also achieves state-of-the-art geometric/photometric fidelity of the reconstructed scene and tracking accuracy.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Real-time SLAM for scene reconstruction applicable to motion analysis</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12002v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12002v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12002v1">Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12002v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Accurate documentation of newborn resuscitation is essential for quality improvement and adherence to clinical guidelines, yet remains underutilized in practice. Previous work using 3D-CNNs and Vision Transformers (ViT) has shown promising results in detecting key activities from newborn resuscitation videos, but also highlighted the challenges in recognizing such fine-grained activities.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Accurate documentation of newborn resuscitation is essential for quality improvement and adherence to clinical guidelines, yet remains underutilized in practice. Previous work using 3D-CNNs and Vision Transformers (ViT) has shown promising results in detecting key activities from newborn resuscitation videos, but also highlighted the challenges in recognizing such fine-grained activities. This work investigates the potential of generative AI (GenAI) methods to improve activity recognition from such videos. Specifically, we explore the use of local vision-language models (VLMs), combined with large language models (LLMs), and compare them to a supervised TimeSFormer baseline. Using a simulated dataset comprising 13.26 hours of newborn resuscitation videos, we evaluate several zero-shot VLM-based strategies and fine-tuned VLMs with classification heads, including Low-Rank Adaptation (LoRA). Our results suggest that small (local) VLMs struggle with hallucinations, but when fine-tuned with LoRA, the results reach F1 score at 0.91, surpassing the TimeSformer results of 0.70.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Activity recognition</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.17751v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.17751v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.17751v1">Investigating Target Class Influence on Neural Network Compressibility for Energy-Autonomous Avian Monitoring</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.0</span>
            
            <a href="https://arxiv.org/pdf/2602.17751v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Biodiversity loss poses a significant threat to humanity, making wildlife monitoring essential for assessing ecosystem health. Avian species are ideal subjects for this due to their popularity and the ease of identifying them through their distinctive songs.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Biodiversity loss poses a significant threat to humanity, making wildlife monitoring essential for assessing ecosystem health. Avian species are ideal subjects for this due to their popularity and the ease of identifying them through their distinctive songs. Traditionalavian monitoring methods require manual counting and are therefore costly and inefficient. In passive acoustic monitoring, soundscapes are recorded over long periods of time. The recordings are analyzed to identify bird species afterwards. Machine learning methods have greatly expedited this process in a wide range of species and environments, however, existing solutions require complex models and substantial computational resources. Instead, we propose running machine learning models on inexpensive microcontroller units (MCUs) directly in the field. Due to the resulting hardware and energy constraints, efficient artificial intelligence (AI) architecture is required. In this paper, we present our method for avian monitoring on MCUs. We trained and compressed models for various numbers of target classes to assess the detection of multiple bird species on edge devices and evaluate the influence of the number of species on the compressibility of neural networks. Our results demonstrate significant compression rates with minimal performance loss. We also provide benchmarking results for different hardware platforms and evaluate the feasibility of deploying energy-autonomous devices.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">에지 디바이스에서 경량 모델 압축 방법론 포함되나 스포츠 분석과 직접적 연관성 부족</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14010v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14010v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14010v1">A Deployment-Friendly Foundational Framework for Efficient Computational Pathology</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.0</span>
            
            <a href="https://arxiv.org/pdf/2602.14010v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Pathology foundation models (PFMs) have enabled robust generalization in computational pathology through large-scale datasets and expansive architectures, but their substantial computational cost, particularly for gigapixel whole slide images, limits clinical accessibility and scalability. Here, we present LitePath, a deployment-friendly foundational framework designed to mitigate model over-parameterization and patch level redundancy.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Pathology foundation models (PFMs) have enabled robust generalization in computational pathology through large-scale datasets and expansive architectures, but their substantial computational cost, particularly for gigapixel whole slide images, limits clinical accessibility and scalability. Here, we present LitePath, a deployment-friendly foundational framework designed to mitigate model over-parameterization and patch level redundancy. LitePath integrates LiteFM, a compact model distilled from three large PFMs (Virchow2, H-Optimus-1 and UNI2) using 190 million patches, and the Adaptive Patch Selector (APS), a lightweight component for task-specific patch selection. The framework reduces model parameters by 28x and lowers FLOPs by 403.5x relative to Virchow2, enabling deployment on low-power edge hardware such as the NVIDIA Jetson Orin Nano Super. On this device, LitePath processes 208 slides per hour, 104.5x faster than Virchow2, and consumes 0.36 kWh per 3,000 slides, 171x lower than Virchow2 on an RTX3090 GPU. We validated accuracy using 37 cohorts across four organs and 26 tasks (26 internal, 9 external, and 2 prospective), comprising 15,672 slides from 9,808 patients disjoint from the pretraining data. LitePath ranks second among 19 evaluated models and outperforms larger models including H-Optimus-1, mSTAR, UNI2 and GPFM, while retaining 99.71% of the AUC of Virchow2 on average. To quantify the balance between accuracy and efficiency, we propose the Deployability Score (D-Score), defined as the weighted geometric mean of normalized AUC and normalized FLOP, where LitePath achieves the highest value, surpassing Virchow2 by 10.64%. These results demonstrate that LitePath enables rapid, cost-effective and energy-efficient pathology image analysis on accessible hardware while maintaining accuracy comparable to state-of-the-art PFMs and reducing the carbon footprint of AI deployment.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Edge deployment framework, applicable to sports device efficiency.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13628v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13628v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13628v1">Compact LLM Deployment and World Model Assisted Offloading in Mobile Edge Computing</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13628v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.NI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">This paper investigates compact large language model (LLM) deployment and world-model-assisted inference offloading in mobile edge computing (MEC) networks. We first propose an edge compact LLM deployment (ECLD) framework that jointly applies structured pruning, low-bit quantization, and knowledge distillation to construct edge-deployable LLM variants, and we evaluate these models using four complementary metrics: accessibility, energy consumption, hallucination rate, and generalization accuracy.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">This paper investigates compact large language model (LLM) deployment and world-model-assisted inference offloading in mobile edge computing (MEC) networks. We first propose an edge compact LLM deployment (ECLD) framework that jointly applies structured pruning, low-bit quantization, and knowledge distillation to construct edge-deployable LLM variants, and we evaluate these models using four complementary metrics: accessibility, energy consumption, hallucination rate, and generalization accuracy. Building on the resulting compact models, we formulate an MEC offloading optimization problem that minimizes the long-term average inference latency subject to per-device energy budgets and LLM-specific quality-of-service constraints on effective accuracy and hallucination. To solve this problem under unknown and time-varying network dynamics, we develop a world model-proximal policy optimization (PPO) algorithm, which augments an on-policy PPO algorithm with a learned recurrent world model that provides improved value targets and short imagination rollouts. Extensive experiments on Llama-3.1-8B, Qwen3-8B, and Mistral-12B show that ECLD compresses base models by about 70-80% in storage (i.e., from 15.3 GB to 3.3 GB for Llama-3.1-8B) and reduces per-query energy consumption by up to 50%, while largely preserving accuracy and often lowering hallucination compared with quantization-only or pruning-only baselines. Moreover, they also show that world model-PPO speeds up convergence by about 50%, improves the final reward by 15.8% over vanilla PPO, and reduces average inference latency by 12-30% across different user populations, while satisfying the accuracy and hallucination constraints and approaching the generation quality of always-offloading with much of the efficiency of local execution.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">에지 디바이스용 경량 LLM 배포 및 오프로딩 최적화 기술이 플랫폼 운영에 적용 가능합니다.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12593v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12593v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12593v1">RQ-GMM: Residual Quantized Gaussian Mixture Model for Multimodal Semantic Discretization in CTR Prediction</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12593v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.IR</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Multimodal content is crucial for click-through rate (CTR) prediction. However, directly incorporating continuous embeddings from pre-trained models into CTR models yields suboptimal results due to misaligned optimization objectives and convergence speed inconsistency during joint training.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Multimodal content is crucial for click-through rate (CTR) prediction. However, directly incorporating continuous embeddings from pre-trained models into CTR models yields suboptimal results due to misaligned optimization objectives and convergence speed inconsistency during joint training. Discretizing embeddings into semantic IDs before feeding them into CTR models offers a more effective solution, yet existing methods suffer from limited codebook utilization, reconstruction accuracy, and semantic discriminability. We propose RQ-GMM (Residual Quantized Gaussian Mixture Model), which introduces probabilistic modeling to better capture the statistical structure of multimodal embedding spaces. Through Gaussian Mixture Models combined with residual quantization, RQ-GMM achieves superior codebook utilization and reconstruction accuracy. Experiments on public datasets and online A/B tests on a large-scale short-video platform serving hundreds of millions of users demonstrate substantial improvements: RQ-GMM yields a 1.502% gain in Advertiser Value over strong baselines. The method has been fully deployed, serving daily recommendations for hundreds of millions of users.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">CTR prediction for short-video platforms with advertising, directly applicable to sharing and monetization.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13669v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13669v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13669v1">EchoTorrent: Towards Swift, Sustained, and Streaming Multi-Modal Video Generation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13669v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Recent multi-modal video generation models have achieved high visual quality, but their prohibitive latency and limited temporal stability hinder real-time deployment. Streaming inference exacerbates these issues, leading to pronounced multimodal degradation, such as spatial blurring, temporal drift, and lip desynchronization, which creates an unresolved efficiency-performance trade-off.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent multi-modal video generation models have achieved high visual quality, but their prohibitive latency and limited temporal stability hinder real-time deployment. Streaming inference exacerbates these issues, leading to pronounced multimodal degradation, such as spatial blurring, temporal drift, and lip desynchronization, which creates an unresolved efficiency-performance trade-off. To this end, we propose EchoTorrent, a novel schema with a fourfold design: (1) Multi-Teacher Training fine-tunes a pre-trained model on distinct preference domains to obtain specialized domain experts, which sequentially transfer domain-specific knowledge to a student model; (2) Adaptive CFG Calibration (ACC-DMD), which calibrates the audio CFG augmentation errors in DMD via a phased spatiotemporal schedule, eliminating redundant CFG computations and enabling single-pass inference per step; (3) Hybrid Long Tail Forcing, which enforces alignment exclusively on tail frames during long-horizon self-rollout training via a causal-bidirectional hybrid architecture, effectively mitigates spatiotemporal degradation in streaming mode while enhancing fidelity to reference frames; and (4) VAE Decoder Refiner through pixel-domain optimization of the VAE decoder to recover high-frequency details while circumventing latent-space ambiguities. Extensive experiments and analysis demonstrate that EchoTorrent achieves few-pass autoregressive generation with substantially extended temporal consistency, identity preservation, and audio-lip synchronization.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">실시간 비디오 생성 기술이 스포츠 하이라이트 편집에 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12484v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12484v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12484v1">A Lightweight and Explainable DenseNet-121 Framework for Grape Leaf Disease Classification</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12484v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Grapes are among the most economically and culturally significant fruits on a global scale, and table grapes and wine are produced in significant quantities in Europe and Asia. The production and quality of grapes are significantly impacted by grape diseases such as Bacterial Rot, Downy Mildew, and Powdery Mildew.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Grapes are among the most economically and culturally significant fruits on a global scale, and table grapes and wine are produced in significant quantities in Europe and Asia. The production and quality of grapes are significantly impacted by grape diseases such as Bacterial Rot, Downy Mildew, and Powdery Mildew. Consequently, the sustainable management of a vineyard necessitates the early and precise identification of these diseases. Current automated methods, particularly those that are based on the YOLO framework, are often computationally costly and lack interpretability that makes them unsuitable for real-world scenarios. This study proposes grape leaf disease classification using Optimized DenseNet 121. Domain-specific preprocessing and extensive connectivity reveal disease-relevant characteristics, including veins, edges, and lesions. An extensive comparison with baseline CNN models, including ResNet18, VGG16, AlexNet, and SqueezeNet, demonstrates that the proposed model exhibits superior performance. It achieves an accuracy of 99.27%, an F1 score of 99.28%, a specificity of 99.71%, and a Kappa of 98.86%, with an inference time of 9 seconds. The cross-validation findings show a mean accuracy of 99.12%, indicating strength and generalizability across all classes. We also employ Grad-CAM to highlight disease-related regions to guarantee the model is highlighting physiologically relevant aspects and increase transparency and confidence. Model optimization reduces processing requirements for real-time deployment, while transfer learning ensures consistency on smaller and unbalanced samples. An effective architecture, domain-specific preprocessing, and interpretable outputs make the proposed framework scalable, precise, and computationally inexpensive for detecting grape leaf diseases.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">경량 모델 방법론이 에지 디바이스에 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13067v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13067v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13067v1">SIEFormer: Spectral-Interpretable and -Enhanced Transformer for Generalized Category Discovery</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">70.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13067v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">This paper presents a novel approach, Spectral-Interpretable and -Enhanced Transformer (SIEFormer), which leverages spectral analysis to reinterpret the attention mechanism within Vision Transformer (ViT) and enhance feature adaptability, with particular emphasis on challenging Generalized Category Discovery (GCD) tasks. The proposed SIEFormer is composed of two main branches, each corresponding to an implicit and explicit spectral perspective of the ViT, enabling joint optimization.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">This paper presents a novel approach, Spectral-Interpretable and -Enhanced Transformer (SIEFormer), which leverages spectral analysis to reinterpret the attention mechanism within Vision Transformer (ViT) and enhance feature adaptability, with particular emphasis on challenging Generalized Category Discovery (GCD) tasks. The proposed SIEFormer is composed of two main branches, each corresponding to an implicit and explicit spectral perspective of the ViT, enabling joint optimization. The implicit branch realizes the use of different types of graph Laplacians to model the local structure correlations of tokens, along with a novel Band-adaptive Filter (BaF) layer that can flexibly perform both band-pass and band-reject filtering. The explicit branch, on the other hand, introduces a Maneuverable Filtering Layer (MFL) that learns global dependencies among tokens by applying the Fourier transform to the input ``value&#34; features, modulating the transformed signal with a set of learnable parameters in the frequency domain, and then performing an inverse Fourier transform to obtain the enhanced features. Extensive experiments reveal state-of-the-art performance on multiple image recognition datasets, reaffirming the superiority of our approach through ablation studies and visualizations.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Vision Transformer enhancements applicable to video/image analysis for sports scenes</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11730v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11730v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11730v1">STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">68.4</span>
            
            <a href="https://arxiv.org/pdf/2602.11730v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">In vision-language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial-temporal video grounding (STVG).</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">In vision-language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial-temporal video grounding (STVG). Prior approaches typically focus on enhancing visual-textual alignment or attaching auxiliary decoders. However, these strategies inevitably introduce additional trainable modules, leading to significant annotation costs and computational overhead. In this work, we propose a novel visual prompting paradigm that avoids the difficult problem of aligning coordinates across modalities. Specifically, we reformulate per-frame coordinate prediction as a compact instance-level identification problem by assigning each object a unique, temporally consistent ID. These IDs are embedded into the video as visual prompts, providing explicit and interpretable inputs to the VLMs. Furthermore, we introduce STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization. Extensive experiments on six benchmarks demonstrate the effectiveness of our approach. STVG-R1 surpasses the baseline Qwen2.5-VL-7B by a remarkable margin of 20.9% on m_IoU on the HCSTVG-v2 benchmark, establishing a new state of the art (SOTA). Surprisingly, STVG-R1 also exhibits strong zero-shot generalization to multi-object referring video object segmentation tasks, achieving a SOTA 47.3% J&amp;F on MeViS.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">비디오의 공간-시간적 그라운딩 기술이 스포츠 하이라이트 자동 편집에 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12117v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12117v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12117v1">KAN-FIF: Spline-Parameterized Lightweight Physics-based Tropical Cyclone Estimation on Meteorological Satellite</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">68.4</span>
            
            <a href="https://arxiv.org/pdf/2602.12117v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Tropical cyclones (TC) are among the most destructive natural disasters, causing catastrophic damage to coastal regions through extreme winds, heavy rainfall, and storm surges. Timely monitoring of tropical cyclones is crucial for reducing loss of life and property, yet it is hindered by the computational inefficiency and high parameter counts of existing methods on resource-constrained edge devices.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Tropical cyclones (TC) are among the most destructive natural disasters, causing catastrophic damage to coastal regions through extreme winds, heavy rainfall, and storm surges. Timely monitoring of tropical cyclones is crucial for reducing loss of life and property, yet it is hindered by the computational inefficiency and high parameter counts of existing methods on resource-constrained edge devices. Current physics-guided models suffer from linear feature interactions that fail to capture high-order polynomial relationships between TC attributes, leading to inflated model sizes and hardware incompatibility. To overcome these challenges, this study introduces the Kolmogorov-Arnold Network-based Feature Interaction Framework (KAN-FIF), a lightweight multimodal architecture that integrates MLP and CNN layers with spline-parameterized KAN layers. For Maximum Sustained Wind (MSW) prediction, experiments demonstrate that the KAN-FIF framework achieves a $94.8\%$ reduction in parameters (0.99MB vs 19MB) and $68.7\%$ faster inference per sample (2.3ms vs 7.35ms) compared to baseline model Phy-CoCo, while maintaining superior accuracy with $32.5\%$ lower MAE. The offline deployment experiment of the FY-4 series meteorological satellite processor on the Qingyun-1000 development board achieved a 14.41ms per-sample inference latency with the KAN-FIF framework, demonstrating promising feasibility for operational TC monitoring and extending deployability to edge-device AI applications. The code is released at https://github.com/Jinglin-Zhang/KAN-FIF.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Lightweight edge AI method applicable but for meteorological use</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12160v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12160v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12160v1">DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">68.4</span>
            
            <a href="https://arxiv.org/pdf/2602.12160v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">인간 중심 오디오-비디오 생성 및 편집 기술이 포함되어 있으나, 스포츠 촬영 및 분석과의 직접적인 연관성은 제한적입니다.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13865v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13865v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13865v1">Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">68.4</span>
            
            <a href="https://arxiv.org/pdf/2602.13865v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent&#39;s direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object&#39;s final state (standard HER), 2HER also generates goals from the agent&#39;s effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">희소 보상 환경의 강화학습 방법론이 스포츠 전략 분석 AI 개발에 활용될 수 있습니다.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13402v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13402v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13402v1">InfoCIR: Multimedia Analysis for Composed Image Retrieval</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">68.4</span>
            
            <a href="https://arxiv.org/pdf/2602.13402v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.HC</span>
          
          <span class="cat-tag">cs.IR</span>
          
          <span class="cat-tag">cs.MM</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Composed Image Retrieval (CIR) allows users to search for images by combining a reference image with a text prompt that describes desired modifications. While vision-language models like CLIP have popularized this task by embedding multiple modalities into a joint space, developers still lack tools that reveal how these multimodal prompts interact with embedding spaces and why small wording changes can dramatically alter the results.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Composed Image Retrieval (CIR) allows users to search for images by combining a reference image with a text prompt that describes desired modifications. While vision-language models like CLIP have popularized this task by embedding multiple modalities into a joint space, developers still lack tools that reveal how these multimodal prompts interact with embedding spaces and why small wording changes can dramatically alter the results. We present InfoCIR, a visual analytics system that closes this gap by coupling retrieval, explainability, and prompt engineering in a single, interactive dashboard. InfoCIR integrates a state-of-the-art CIR back-end (SEARLE arXiv:2303.15247) with a six-panel interface that (i) lets users compose image + text queries, (ii) projects the top-k results into a low-dimensional space using Uniform Manifold Approximation and Projection (UMAP) for spatial reasoning, (iii) overlays similarity-based saliency maps and gradient-derived token-attribution bars for local explanation, and (iv) employs an LLM-powered prompt enhancer that generates counterfactual variants and visualizes how these changes affect the ranking of user-selected target images. A modular architecture built on Plotly-Dash allows new models, datasets, and attribution methods to be plugged in with minimal effort. We argue that InfoCIR helps diagnose retrieval failures, guides prompt enhancement, and accelerates insight generation during model development. All source code allowing for a reproducible demo is available at https://github.com/giannhskp/InfoCIR.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Applicable for image editing/retrieval but not sports-specific.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.16365v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.16365v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.16365v1">Markerless 6D Pose Estimation and Position-Based Visual Servoing for Endoscopic Continuum Manipulators</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.4</span>
            
            <a href="https://arxiv.org/pdf/2602.16365v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Continuum manipulators in flexible endoscopic surgical systems offer high dexterity for minimally invasive procedures; however, accurate pose estimation and closed-loop control remain challenging due to hysteresis, compliance, and limited distal sensing. Vision-based approaches reduce hardware complexity but are often constrained by limited geometric observability and high computational overhead, restricting real-time closed-loop applicability.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Continuum manipulators in flexible endoscopic surgical systems offer high dexterity for minimally invasive procedures; however, accurate pose estimation and closed-loop control remain challenging due to hysteresis, compliance, and limited distal sensing. Vision-based approaches reduce hardware complexity but are often constrained by limited geometric observability and high computational overhead, restricting real-time closed-loop applicability. This paper presents a unified framework for markerless stereo 6D pose estimation and position-based visual servoing of continuum manipulators. A photo-realistic simulation pipeline enables large-scale automatic training with pixel-accurate annotations. A stereo-aware multi-feature fusion network jointly exploits segmentation masks, keypoints, heatmaps, and bounding boxes to enhance geometric observability. To enforce geometric consistency without iterative optimization, a feed-forward rendering-based refinement module predicts residual pose corrections in a single pass. A self-supervised sim-to-real adaptation strategy further improves real-world performance using unlabeled data. Extensive real-world validation achieves a mean translation error of 0.83 mm and a mean rotation error of 2.76° across 1,000 samples. Markerless closed-loop visual servoing driven by the estimated pose attains accurate trajectory tracking with a mean translation error of 2.07 mm and a mean rotation error of 7.41°, corresponding to 85% and 59% reductions compared to open-loop control, together with high repeatability in repeated point-reaching tasks. To the best of our knowledge, this work presents the first fully markerless pose-estimation-driven position-based visual servoing framework for continuum manipulators, enabling precise closed-loop control without physical markers or embedded sensing.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">마커리스 포즈 추정 방법론이 운동 동작 분석에 간접적으로 참고될 수 있음</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13479v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13479v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13479v1">GLIMPSE : Real-Time Text Recognition and Contextual Understanding for VQA in Wearables</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13479v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.HC</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Video Large Language Models (Video LLMs) have shown remarkable progress in understanding and reasoning about visual content, particularly in tasks involving text recognition and text-based visual question answering (Text VQA). However, deploying Text VQA on wearable devices faces a fundamental tension: text recognition requires high-resolution video, but streaming high-quality video drains battery and causes thermal throttling.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Video Large Language Models (Video LLMs) have shown remarkable progress in understanding and reasoning about visual content, particularly in tasks involving text recognition and text-based visual question answering (Text VQA). However, deploying Text VQA on wearable devices faces a fundamental tension: text recognition requires high-resolution video, but streaming high-quality video drains battery and causes thermal throttling. Moreover, existing models struggle to maintain coherent temporal context when processing text across multiple frames in real-time streams. We observe that text recognition and visual reasoning have asymmetric resolution requirements - OCR needs fine detail while scene understanding tolerates coarse features. We exploit this asymmetry with a hybrid architecture that performs selective high-resolution OCR on-device while streaming low-resolution video for visual context. On a benchmark of text-based VQA samples across five task categories, our system achieves 72% accuracy at 0.49x the power consumption of full-resolution streaming, enabling sustained VQA sessions on resource-constrained wearables without sacrificing text understanding quality.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Edge-efficient real-time processing for contextual understanding</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12691v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12691v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12691v1">ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12691v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">행동 수준 평가 프레임워크가 스포츠 전략 분석에 적용 가능한 방법론 포함</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11832v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11832v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11832v1">JEPA-VLA: Video Predictive Embedding is Needed for VLA Models</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.0</span>
            
            <a href="https://arxiv.org/pdf/2602.11832v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Video predictive embeddings; applicable to action understanding.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12314v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12314v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12314v1">LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12314v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We present LatentAM, an online 3D Gaussian Splatting (3DGS) mapping framework that builds scalable latent feature maps from streaming RGB-D observations for open-vocabulary robotic perception. Instead of distilling high-dimensional Vision-Language Model (VLM) embeddings using model-specific decoders, LatentAM proposes an online dictionary learning approach that is both model-agnostic and pretraining-free, enabling plug-and-play integration with different VLMs at test time.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We present LatentAM, an online 3D Gaussian Splatting (3DGS) mapping framework that builds scalable latent feature maps from streaming RGB-D observations for open-vocabulary robotic perception. Instead of distilling high-dimensional Vision-Language Model (VLM) embeddings using model-specific decoders, LatentAM proposes an online dictionary learning approach that is both model-agnostic and pretraining-free, enabling plug-and-play integration with different VLMs at test time. Specifically, our approach associates each Gaussian primitive with a compact query vector that can be converted into approximate VLM embeddings using an attention mechanism with a learnable dictionary. The dictionary is initialized efficiently from streaming observations and optimized online to adapt to evolving scene semantics under trust-region regularization. To scale to long trajectories and large environments, we further propose an efficient map management strategy based on voxel hashing, where optimization is restricted to an active local map on the GPU, while the global map is stored and indexed on the CPU to maintain bounded GPU memory usage. Experiments on public benchmarks and a large-scale custom dataset demonstrate that LatentAM attains significantly better feature reconstruction fidelity compared to state-of-the-art methods, while achieving near-real-time speed (12-35 FPS) on the evaluated datasets. Our project page is at: https://junwoonlee.github.io/projects/LatentAM</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">실시간 3D 매핑 기술로 영상 처리에 적용 가능한 방법론 포함</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12524v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12524v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12524v1">LiDAR-Anchored Collaborative Distillation for Robust 2D Representations</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12524v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">As deep learning continues to advance, self-supervised learning has made considerable strides. It allows 2D image encoders to extract useful features for various downstream tasks, including those related to vision-based systems.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">As deep learning continues to advance, self-supervised learning has made considerable strides. It allows 2D image encoders to extract useful features for various downstream tasks, including those related to vision-based systems. Nevertheless, pre-trained 2D image encoders fall short in conducting the task under noisy and adverse weather conditions beyond clear daytime scenes, which require for robust visual perception. To address these issues, we propose a novel self-supervised approach, \textbf{Collaborative Distillation}, which leverages 3D LiDAR as self-supervision to improve robustness to noisy and adverse weather conditions in 2D image encoders while retaining their original capabilities. Our method outperforms competing methods in various downstream tasks across diverse conditions and exhibits strong generalization ability. In addition, our method also improves 3D awareness stemming from LiDAR&#39;s characteristics. This advancement highlights our method&#39;s practicality and adaptability in real-world scenarios.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">강건한 시각 인식 기술로 스포츠 촬영 환경에 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13909v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13909v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13909v1">High-fidelity 3D reconstruction for planetary exploration</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13909v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Planetary exploration increasingly relies on autonomous robotic systems capable of perceiving, interpreting, and reconstructing their surroundings in the absence of global positioning or real-time communication with Earth. Rovers operating on planetary surfaces must navigate under sever environmental constraints, limited visual redundancy, and communication delays, making onboard spatial awareness and visual localization key components for mission success.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Planetary exploration increasingly relies on autonomous robotic systems capable of perceiving, interpreting, and reconstructing their surroundings in the absence of global positioning or real-time communication with Earth. Rovers operating on planetary surfaces must navigate under sever environmental constraints, limited visual redundancy, and communication delays, making onboard spatial awareness and visual localization key components for mission success. Traditional techniques based on Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM) provide geometric consistency but struggle to capture radiometric detail or to scale efficiently in unstructured, low-texture terrains typical of extraterrestrial environments. This work explores the integration of radiance field-based methods - specifically Neural Radiance Fields (NeRF) and Gaussian Splatting - into a unified, automated environment reconstruction pipeline for planetary robotics. Our system combines the Nerfstudio and COLMAP frameworks with a ROS2-compatible workflow capable of processing raw rover data directly from rosbag recordings. This approach enables the generation of dense, photorealistic, and metrically consistent 3D representations from minimal visual input, supporting improved perception and planning for autonomous systems operating in planetary-like conditions. The resulting pipeline established a foundation for future research in radiance field-based mapping, bridging the gap between geometric and neural representations in planetary exploration.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">3D 재구성 기술이 영상 분석 및 보정에 적용 가능한 방법론 포함</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11466v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11466v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11466v1">A Dual-Branch Framework for Semantic Change Detection with Boundary and Temporal Awareness</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.0</span>
            
            <a href="https://arxiv.org/pdf/2602.11466v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Semantic Change Detection (SCD) aims to detect and categorize land-cover changes from bi-temporal remote sensing images. Existing methods often suffer from blurred boundaries and inadequate temporal modeling, limiting segmentation accuracy.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Semantic Change Detection (SCD) aims to detect and categorize land-cover changes from bi-temporal remote sensing images. Existing methods often suffer from blurred boundaries and inadequate temporal modeling, limiting segmentation accuracy. To address these issues, we propose a Dual-Branch Framework for Semantic Change Detection with Boundary and Temporal Awareness, termed DBTANet. Specifically, we utilize a dual-branch Siamese encoder where a frozen SAM branch captures global semantic context and boundary priors, while a ResNet34 branch provides local spatial details, ensuring complementary feature representations. On this basis, we design a Bidirectional Temporal Awareness Module (BTAM) to aggregate multi-scale features and capture temporal dependencies in a symmetric manner. Furthermore, a Gaussian-smoothed Projection Module (GSPM) refines shallow SAM features, suppressing noise while enhancing edge information for boundary-aware constraints. Extensive experiments on two public benchmarks demonstrate that DBTANet effectively integrates global semantics, local details, temporal reasoning, and boundary awareness, achieving state-of-the-art performance.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">원격 감지용 시간적 변화 감지 방법론이 스포츠 영상 분석에 적용 가능한 기술을 포함합니다.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12774v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12774v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12774v1">Bootstrapping MLLM for Weakly-Supervised Class-Agnostic Object Counting</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12774v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Object counting is a fundamental task in computer vision, with broad applicability in many real-world scenarios. Fully-supervised counting methods require costly point-level annotations per object.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Object counting is a fundamental task in computer vision, with broad applicability in many real-world scenarios. Fully-supervised counting methods require costly point-level annotations per object. Few weakly-supervised methods leverage only image-level object counts as supervision and achieve fairly promising results. They are, however, often limited to counting a single category, e.g. person. In this paper, we propose WS-COC, the first MLLM-driven weakly-supervised framework for class-agnostic object counting. Instead of directly fine-tuning MLLMs to predict object counts, which can be challenging due to the modality gap, we incorporate three simple yet effective strategies to bootstrap the counting paradigm in both training and testing: First, a divide-and-discern dialogue tuning strategy is proposed to guide the MLLM to determine whether the object count falls within a specific range and progressively break down the range through multi-round dialogue. Second, a compare-and-rank count optimization strategy is introduced to train the MLLM to optimize the relative ranking of multiple images according to their object counts. Third, a global-and-local counting enhancement strategy aggregates and fuses local and global count predictions to improve counting performance in dense scenes. Extensive experiments on FSC-147, CARPK, PUCPR+, and ShanghaiTech show that WS-COC matches or even surpasses many state-of-art fully-supervised methods while significantly reducing annotation costs. Code is available at https://github.com/viscom-tongji/WS-COC.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Object counting applicable to player/action analysis in sports.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11494v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11494v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11494v1">Arbitrary Ratio Feature Compression via Next Token Prediction</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.0</span>
            
            <a href="https://arxiv.org/pdf/2602.11494v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Feature compression is increasingly important for improving the efficiency of downstream tasks, especially in applications involving large-scale or multi-modal data. While existing methods typically rely on dedicated models for achieving specific compression ratios, they are often limited in flexibility and generalization.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Feature compression is increasingly important for improving the efficiency of downstream tasks, especially in applications involving large-scale or multi-modal data. While existing methods typically rely on dedicated models for achieving specific compression ratios, they are often limited in flexibility and generalization. In particular, retraining is necessary when adapting to a new compression ratio. To address this limitation, we propose a novel and flexible Arbitrary Ratio Feature Compression (ARFC) framework, which supports any compression ratio with a single model, eliminating the need for multiple specialized models. At its core, the Arbitrary Ratio Compressor (ARC) is an auto-regressive model that performs compression via next-token prediction. This allows the compression ratio to be controlled at inference simply by adjusting the number of generated tokens. To enhance the quality of the compressed features, two key modules are introduced. The Mixture of Solutions (MoS) module refines the compressed tokens by utilizing multiple compression results (solutions), reducing uncertainty and improving robustness. The Entity Relation Graph Constraint (ERGC) is integrated into the training process to preserve semantic and structural relationships during compression. Extensive experiments on cross-modal retrieval, image classification, and image retrieval tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches at various compression ratios. Notably, in some cases, it even surpasses the performance of the original, uncompressed features. These results validate the effectiveness and versatility of ARFC for practical, resource-constrained scenarios.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">특징 압축 기술이 에지 디바이스 효율성에 적용 가능함.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12304v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12304v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12304v1">OmniCustom: Sync Audio-Video Customization Via Joint Audio-Video Generation Model</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12304v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.SD</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.MM</span>
          
          <span class="cat-tag">eess.AS</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Existing mainstream video customization methods focus on generating identity-consistent videos based on given reference images and textual prompts. Benefiting from the rapid advancement of joint audio-video generation, this paper proposes a more compelling new task: sync audio-video customization, which aims to synchronously customize both video identity and audio timbre.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Existing mainstream video customization methods focus on generating identity-consistent videos based on given reference images and textual prompts. Benefiting from the rapid advancement of joint audio-video generation, this paper proposes a more compelling new task: sync audio-video customization, which aims to synchronously customize both video identity and audio timbre. Specifically, given a reference image $I^{r}$ and a reference audio $A^{r}$, this novel task requires generating videos that maintain the identity of the reference image while imitating the timbre of the reference audio, with spoken content freely specifiable through user-provided textual prompts. To this end, we propose OmniCustom, a powerful DiT-based audio-video customization framework that can synthesize a video following reference image identity, audio timbre, and text prompts all at once in a zero-shot manner. Our framework is built on three key contributions. First, identity and audio timbre control are achieved through separate reference identity and audio LoRA modules that operate through self-attention layers within the base audio-video generation model. Second, we introduce a contrastive learning objective alongside the standard flow matching objective. It uses predicted flows conditioned on reference inputs as positive examples and those without reference conditions as negative examples, thereby enhancing the model ability to preserve identity and timbre. Third, we train OmniCustom on our constructed large-scale, high-quality audio-visual human dataset. Extensive experiments demonstrate that OmniCustom outperforms existing methods in generating audio-video content with consistent identity and timbre fidelity.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Video customization methodology</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12916v2">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12916v2" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12916v2">Reliable Thinking with Images</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12916v2" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding. In this paper, we reveal and study a highly-practical yet under-explored problem in TWI, termed Noisy Thinking (NT). Specifically, NT refers to the imperfect visual cues mining and answer reasoning process. As the saying goes, ``One mistake leads to another&#39;&#39;, erroneous interleaved CoT would cause error accumulation, thus significantly degrading the performance of MLLMs. To solve the NT problem, we propose a novel method dubbed Reliable Thinking with Images (RTWI). In brief, RTWI estimates the reliability of visual cues and textual CoT in a unified text-centric manner and accordingly employs robust filtering and voting modules to prevent NT from contaminating the final answer. Extensive experiments on seven benchmarks verify the effectiveness of RTWI against NT.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">멀티모달 추론 신뢰성 향상 기술로 동작 분석에 적용 가능함.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12082v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12082v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12082v1">Empirical Gaussian Processes</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">66.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12082v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">stat.ML</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Gaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Gaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space. We study Empirical GPs, a principled framework for constructing flexible, data-driven GP priors that overcome these limitations. Rather than relying on standard parametric kernels, we estimate the mean and covariance functions empirically from a corpus of historical observations, enabling the prior to reflect rich, non-trivial covariance structures present in the data. Theoretically, we show that the resulting model converges to the GP that is closest (in KL-divergence sense) to the real data generating process. Practically, we formulate the problem of learning the GP prior from independent datasets as likelihood estimation and derive an Expectation-Maximization algorithm with closed-form updates, allowing the model handle heterogeneous observation locations across datasets. We demonstrate that Empirical GPs achieve competitive performance on learning curve extrapolation and time series forecasting benchmarks.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Flexible regression models for movement analysis.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.18020v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.18020v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18020v1">UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.8</span>
            
            <a href="https://arxiv.org/pdf/2602.18020v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as &#34;key-value memory&#34;, we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer&#39;s Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">로봇 조작을 위한 VLA 모델 개선 방법으로, 스포츠 분석에 간접적으로 관련될 수 있음.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13172v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13172v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13172v1">LongStream: Long-Sequence Streaming Autoregressive Visual Geometry</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.4</span>
            
            <a href="https://arxiv.org/pdf/2602.13172v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">실시간 스트리밍 3D 재구성 기술이 스포츠 장면 분석에 간접적으로 활용 가능함.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13585v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13585v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13585v1">Diff-Aid: Inference-time Adaptive Interaction Denoising for Rectified Text-to-Image Generation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.4</span>
            
            <a href="https://arxiv.org/pdf/2602.13585v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Recent text-to-image (T2I) diffusion models have achieved remarkable advancement, yet faithfully following complex textual descriptions remains challenging due to insufficient interactions between textual and visual features. Prior approaches enhance such interactions via architectural design or handcrafted textual condition weighting, but lack flexibility and overlook the dynamic interactions across different blocks and denoising stages.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent text-to-image (T2I) diffusion models have achieved remarkable advancement, yet faithfully following complex textual descriptions remains challenging due to insufficient interactions between textual and visual features. Prior approaches enhance such interactions via architectural design or handcrafted textual condition weighting, but lack flexibility and overlook the dynamic interactions across different blocks and denoising stages. To provide a more flexible and efficient solution to this problem, we propose Diff-Aid, a lightweight inference-time method that adaptively adjusts per-token text and image interactions across transformer blocks and denoising timesteps. Beyond improving generation quality, Diff-Aid yields interpretable modulation patterns that reveal how different blocks, timesteps, and textual tokens contribute to semantic alignment during denoising. As a plug-and-play module, Diff-Aid can be seamlessly integrated into downstream applications for further improvement, including style LoRAs, controllable generation, and zero-shot editing. Experiments on strong baselines (SD 3.5 and FLUX) demonstrate consistent improvements in prompt adherence, visual quality, and human preference across various metrics. Our code and models will be released.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">이미지 생성 보정 기술이 프로젝트의 영상 처리에 적용 가능함.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.16174v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.16174v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.16174v1">Edge Learning via Federated Split Decision Transformers for Metaverse Resource Allocation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.16174v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.NI</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.MM</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Mobile edge computing (MEC) based wireless metaverse services offer an untethered, immersive experience to users, where the superior quality of experience (QoE) needs to be achieved under stringent latency constraints and visual quality demands. To achieve this, MEC-based intelligent resource allocation for virtual reality users needs to be supported by coordination across MEC servers to harness distributed data.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Mobile edge computing (MEC) based wireless metaverse services offer an untethered, immersive experience to users, where the superior quality of experience (QoE) needs to be achieved under stringent latency constraints and visual quality demands. To achieve this, MEC-based intelligent resource allocation for virtual reality users needs to be supported by coordination across MEC servers to harness distributed data. Federated learning (FL) is a promising solution, and can be combined with reinforcement learning (RL) to develop generalized policies across MEC-servers. However, conventional FL incurs transmitting the full model parameters across the MEC-servers and the cloud, and suffer performance degradation due to naive global aggregation, especially in heterogeneous multi-radio access technology environments. To address these challenges, this paper proposes Federated Split Decision Transformer (FSDT), an offline RL framework where the transformer model is partitioned between MEC servers and the cloud. Agent-specific components (e.g., MEC-based embedding and prediction layers) enable local adaptability, while shared global layers in the cloud facilitate cooperative training across MEC servers. Experimental results demonstrate that FSDT enhances QoE for up to 10% in heterogeneous environments compared to baselines, while offloadingnearly 98% of the transformer model parameters to the cloud, thereby reducing the computational burden on MEC servers.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Edge resource allocation for metaverse, indirect to sports platform</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.18093v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.18093v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18093v1">Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">64.0</span>
            
            <a href="https://arxiv.org/pdf/2602.18093v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">확산 변환기 가속화 기술이 간접적으로 관련될 수 있으나, 프로젝트 핵심 기능과 직접 연결되지 않습니다.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12311v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12311v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12311v1">Perceptual Self-Reflection in Agentic Physics Simulation Code Generation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">63.6</span>
            
            <a href="https://arxiv.org/pdf/2602.12311v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.SE</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We present a multi-agent framework for generating physics simulation code from natural language descriptions, featuring a novel perceptual self-reflection mechanism for validation. The system employs four specialized agents: a natural language interpreter that converts user requests into physics-based descriptions; a technical requirements generator that produces scaled simulation parameters; a physics code generator with automated self-correction; and a physics validator that implements perceptual self-reflection.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We present a multi-agent framework for generating physics simulation code from natural language descriptions, featuring a novel perceptual self-reflection mechanism for validation. The system employs four specialized agents: a natural language interpreter that converts user requests into physics-based descriptions; a technical requirements generator that produces scaled simulation parameters; a physics code generator with automated self-correction; and a physics validator that implements perceptual self-reflection. The key innovation is perceptual validation, which analyzes rendered animation frames using a vision-capable language model rather than inspecting code structure directly. This approach addresses the ``oracle gap&#39;&#39; where syntactically correct code produces physically incorrect behavior--a limitation that conventional testing cannot detect. We evaluate the system across seven domains including classical mechanics, fluid dynamics, thermodynamics, electromagnetics, wave physics, reaction-diffusion systems, and non-physics data visualization. The perceptual self-reflection architecture demonstrates substantial improvement over single-shot generation baselines, with the majority of tested scenarios achieving target physics accuracy thresholds. The system exhibits robust pipeline stability with consistent code self-correction capability, operating at approximately \$0.20 per animation. These results validate our hypothesis that feeding visual simulation outputs back to a vision-language model for iterative refinement significantly outperforms single-shot code generation for physics simulation tasks and highlights the potential of agentic AI to support engineering workflows and physics data generation pipelines.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Physics simulation methodology applicable to motion analysis</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11509v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11509v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11509v1">Multimodal Fact-Level Attribution for Verifiable Reasoning</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">62.8</span>
            
            <a href="https://arxiv.org/pdf/2602.11509v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CL</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Multimodal analysis methodology indirectly relevant to sports video</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13656v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13656v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13656v1">A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">62.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13656v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Current humanoid motion tracking systems can execute routine and moderately dynamic behaviors, yet significant gaps remain near hardware performance limits and algorithmic robustness boundaries. Martial arts represent an extreme case of highly dynamic human motion, characterized by rapid center-of-mass shifts, complex coordination, and abrupt posture transitions.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Current humanoid motion tracking systems can execute routine and moderately dynamic behaviors, yet significant gaps remain near hardware performance limits and algorithmic robustness boundaries. Martial arts represent an extreme case of highly dynamic human motion, characterized by rapid center-of-mass shifts, complex coordination, and abrupt posture transitions. However, datasets tailored to such high-intensity scenarios remain scarce. To address this gap, we construct KungFuAthlete, a high-dynamic martial arts motion dataset derived from professional athletes&#39; daily training videos. The dataset includes ground and jump subsets covering representative complex motion patterns. The jump subset exhibits substantially higher joint, linear, and angular velocities compared to commonly used datasets such as LAFAN1, PHUMA, and AMASS, indicating significantly increased motion intensity and complexity. Importantly, even professional athletes may fail during highly dynamic movements. Similarly, humanoid robots are prone to instability and falls under external disturbances or execution errors. Most prior work assumes motion execution remains within safe states and lacks a unified strategy for modeling unsafe states and enabling reliable autonomous recovery. We propose a novel training paradigm that enables a single policy to jointly learn high-dynamic motion tracking and fall recovery, unifying agile execution and stabilization within one framework. This framework expands robotic capability from pure motion tracking to recovery-enabled execution, promoting more robust and autonomous humanoid performance in real-world high-dynamic scenarios.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">무술 동작 데이터셋 및 로봇 제어 논문, 스포츠 동작 분석과 간접적 관련 있음</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13496v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13496v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13496v1">Future of Edge AI in biodiversity monitoring</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">62.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13496v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CY</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">1. Many ecological decisions are slowed by the gap between collecting and analysing biodiversity data.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">1. Many ecological decisions are slowed by the gap between collecting and analysing biodiversity data. Edge computing moves processing closer to the sensor, with edge artificial intelligence (AI) enabling on-device inference, reducing reliance on data transfer and continuous connectivity. In principle, this shifts biodiversity monitoring from passive logging towards autonomous, responsive sensing systems. In practice, however, adoption remains fragmented, with key architectural trade-offs, performance constraints, and implementation challenges rarely reported systematically. 2. Here, we analyse 82 studies published between 2017 and 2025 that implement edge computing for biodiversity monitoring across acoustic, vision, tracking, and multi-modal systems. We synthesise hardware platforms, AI model optimisation, and wireless communication to critically assess how design choices shape ecological inference, deployment longevity, and operational feasibility. 3. Publications increased from 3 in 2017 to 19 in 2025. We identify four system types: (I) TinyML, low-power microcontrollers (MCUs) for single-taxon or rare-event detection; (II) Edge AI, single-board computers (SBCs) for multi-species classification and real-time alerts; (III) Distributed edge AI; and (IV) Cloud AI for retrospective processing pipelines. Each system type represents context-dependent trade-offs among power consumption, computational capability, and communication requirements. 4. Our analysis reveals the evolution of edge computing systems from proof-of-concept to robust, scalable tools. We argue that edge computing offers opportunities for responsive biodiversity management, but realising this potential requires increased collaboration between ecologists, engineers, and data scientists to align model development and system design with ecological questions, field constraints, and ethical considerations.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">에지 AI 일반 조사, 스포츠 촬영 장치와 간접적 참고 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.17785v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.17785v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.17785v1">Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">62.0</span>
            
            <a href="https://arxiv.org/pdf/2602.17785v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">내시경용 깊이 및 포즈 추정 기술로 스포츠 장면 분석에 간접적 적용 가능.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11934v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11934v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11934v1">Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">62.0</span>
            
            <a href="https://arxiv.org/pdf/2602.11934v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We hypothesize that a key bottleneck in generalizable robot manipulation is not solely data scale or policy capacity, but a structural mismatch between current visual backbones and the physical requirements of closed-loop control. While state-of-the-art vision encoders (including those used in VLAs) optimize for semantic invariance to stabilize classification, manipulation typically demands geometric sensitivity the ability to map millimeter-level pose shifts to predictable feature changes.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We hypothesize that a key bottleneck in generalizable robot manipulation is not solely data scale or policy capacity, but a structural mismatch between current visual backbones and the physical requirements of closed-loop control. While state-of-the-art vision encoders (including those used in VLAs) optimize for semantic invariance to stabilize classification, manipulation typically demands geometric sensitivity the ability to map millimeter-level pose shifts to predictable feature changes. Their discriminative objective creates a &#34;blind spot&#34; for fine-grained control, whereas generative diffusion models inherently encode geometric dependencies within their latent manifolds, encouraging the preservation of dense multi-scale spatial structure. However, directly deploying stochastic diffusion features for control is hindered by stochastic instability, inference latency, and representation drift during fine-tuning. To bridge this gap, we propose Robot-DIFT, a framework that decouples the source of geometric information from the process of inference via Manifold Distillation. By distilling a frozen diffusion teacher into a deterministic Spatial-Semantic Feature Pyramid Network (S2-FPN), we retain the rich geometric priors of the generative model while ensuring temporal stability, real-time execution, and robustness against drift. Pretrained on the large-scale DROID dataset, Robot-DIFT demonstrates superior geometric consistency and control performance compared to leading discriminative baselines, supporting the view that how a model learns to see dictates how well it can learn to act.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">로봇 제어를 위한 기하학적 일관성 기술로, 동작 분석과 간접적 관련</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.17381v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.17381v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.17381v1">End-to-End Latency Measurement Methodology for Connected and Autonomous Vehicle Teleoperation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">62.0</span>
            
            <a href="https://arxiv.org/pdf/2602.17381v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.NI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Connected and Autonomous Vehicles (CAVs) continue to evolve rapidly, and system latency remains one of their most critical performance parameters, particularly when vehicles are operated remotely. Existing latency-assessment methodologies focus predominantly on Glass-to-Glass (G2G) latency, defined as the delay between an event occurring in the operational environment, its capture by a camera, and its subsequent display to the remote operator.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Connected and Autonomous Vehicles (CAVs) continue to evolve rapidly, and system latency remains one of their most critical performance parameters, particularly when vehicles are operated remotely. Existing latency-assessment methodologies focus predominantly on Glass-to-Glass (G2G) latency, defined as the delay between an event occurring in the operational environment, its capture by a camera, and its subsequent display to the remote operator. However, G2G latency accounts for only one component of the total delay experienced by the driver. The complementary component, Motion-to-Motion (M2M) latency, represents the delay between the initiation of a control input by the remote driver and the corresponding physical actuation by the vehicle. Together, M2M and G2G constitute the overall End-to-End (E2E) latency. This paper introduces a measurement framework capable of quantifying M2M, G2G, and E2E latencies using gyroscopes, a phototransistor, and two GPS-synchronized Raspberry Pi 5 units. The system employs low-pass filtering and threshold-based detection to identify steering-wheel motion on both the remote operator and vehicle sides. An interrupt is generated when the phototransistor detects the activation of an LED positioned within the camera&#39;s Field Of View (FOV). Initial measurements obtained from our teleoperated prototype vehicle over commercial 4G and 5G networks indicate an average E2E latency of approximately 500 ms (measurement precision +/- 4 ms). The M2M latency contributes up to 60% of this value.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Latency measurement indirectly relevant for real-time edge video processing.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13718v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13718v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13718v1">HybridFlow: A Two-Step Generative Policy for Robotic Manipulation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">62.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13718v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Limited by inference latency, existing robot manipulation policies lack sufficient real-time interaction capability with the environment. Although faster generation methods such as flow matching are gradually replacing diffusion methods, researchers are pursuing even faster generation suitable for interactive robot control.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Limited by inference latency, existing robot manipulation policies lack sufficient real-time interaction capability with the environment. Although faster generation methods such as flow matching are gradually replacing diffusion methods, researchers are pursuing even faster generation suitable for interactive robot control. MeanFlow, as a one-step variant of flow matching, has shown strong potential in image generation, but its precision in action generation does not meet the stringent requirements of robotic manipulation. We therefore propose \textbf{HybridFlow}, a \textbf{3-stage method} with \textbf{2-NFE}: Global Jump in MeanFlow mode, ReNoise for distribution alignment, and Local Refine in ReFlow mode. This method balances inference speed and generation quality by leveraging the rapid advantage of MeanFlow one-step generation while ensuring action precision with minimal generation steps. Through real-world experiments, HybridFlow outperforms the 16-step Diffusion Policy by \textbf{15--25\%} in success rate while reducing inference time from 152ms to 19ms (\textbf{8$\times$ speedup}, \textbf{$\sim$52Hz}); it also achieves 70.0\% success on unseen-color OOD grasping and 66.3\% on deformable object folding. We envision HybridFlow as a practical low-latency method to enhance real-world interaction capabilities of robotic manipulation policies.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Low-latency generation applicable to real-time sports analysis.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12346v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12346v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12346v1">Schur-MI: Fast Mutual Information for Robotic Information Gathering</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">62.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12346v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Mutual information (MI) is a principled and widely used objective for robotic information gathering (RIG), providing strong theoretical guarantees for sensor placement (SP) and informative path planning (IPP). However, its high computational cost, dominated by repeated log-determinant evaluations, has limited its use in real-time planning.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Mutual information (MI) is a principled and widely used objective for robotic information gathering (RIG), providing strong theoretical guarantees for sensor placement (SP) and informative path planning (IPP). However, its high computational cost, dominated by repeated log-determinant evaluations, has limited its use in real-time planning. This letter presents Schur-MI, a Gaussian process (GP) MI formulation that (i) leverages the iterative structure of RIG to precompute and reuse expensive intermediate quantities across planning steps, and (ii) uses a Schur-complement factorization to avoid large determinant computations. Together, these methods reduce the per-evaluation cost of MI from $\mathcal{O}(|\mathcal{V}|^3)$ to $\mathcal{O}(|\mathcal{A}|^3)$, where $\mathcal{V}$ and $\mathcal{A}$ denote the candidate and selected sensing locations, respectively. Experiments on real-world bathymetry datasets show that Schur-MI achieves up to a $12.7\times$ speedup over the standard MI formulation. Field trials with an autonomous surface vehicle (ASV) performing adaptive IPP further validate its practicality. By making MI computation tractable for online planning, Schur-MI helps bridge the gap between information-theoretic objectives and real-time robotic exploration.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">로봇 정보 수집을 위한 빠른 상호 정보 계산. 스포츠 촬영과 간접적 관련.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12306v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12306v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12306v1">Quantum walk inspired JPEG compression of images</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">62.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12306v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">eess.IV</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.ET</span>
          
          <span class="cat-tag">cs.IT</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">This work proposes a quantum inspired adaptive quantization framework that enhances the classical JPEG compression by introducing a learned, optimized Qtable derived using a Quantum Walk Inspired Optimization (QWIO) search strategy. The optimizer searches a continuous parameter space of frequency band scaling factors under a unified rate distortion objective that jointly considers reconstruction fidelity and compression efficiency.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">This work proposes a quantum inspired adaptive quantization framework that enhances the classical JPEG compression by introducing a learned, optimized Qtable derived using a Quantum Walk Inspired Optimization (QWIO) search strategy. The optimizer searches a continuous parameter space of frequency band scaling factors under a unified rate distortion objective that jointly considers reconstruction fidelity and compression efficiency. The proposed framework is evaluated on MNIST, CIFAR10, and ImageNet subsets, using Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), Bits Per Pixel (BPP), and error heatmap visual analysis as evaluation metrics. Experimental results show average gains ranging from 3 to 6 dB PSNR, along with better structural preservation of edges, contours, and luminance transitions, without modifying decoder compatibility. The structure remains JPEG compliant and can be implemented using accessible scientific packages making it ideal for deployment and practical research use.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Image compression applicable to content generation</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.17871v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.17871v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.17871v1">Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">60.4</span>
            
            <a href="https://arxiv.org/pdf/2602.17871v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.LG</span>
          
          <span class="cat-tag">cs.MM</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">비전-언어 모델의 세부 지능 분석이 스포츠 동작 인식에 간접적 참고 가능.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.15461v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.15461v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.15461v1">Emergent Morphing Attack Detection in Open Multi-modal Large Language Models</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">60.4</span>
            
            <a href="https://arxiv.org/pdf/2602.15461v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Face morphing attacks threaten biometric verification, yet most morphing attack detection (MAD) systems require task-specific training and generalize poorly to unseen attack types. Meanwhile, open-source multimodal large language models (MLLMs) have demonstrated strong visual-linguistic reasoning, but their potential in biometric forensics remains underexplored.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Face morphing attacks threaten biometric verification, yet most morphing attack detection (MAD) systems require task-specific training and generalize poorly to unseen attack types. Meanwhile, open-source multimodal large language models (MLLMs) have demonstrated strong visual-linguistic reasoning, but their potential in biometric forensics remains underexplored. In this paper, we present the first systematic zero-shot evaluation of open-source MLLMs for single-image MAD, using publicly available weights and a standardized, reproducible protocol. Across diverse morphing techniques, many MLLMs show non-trivial discriminative ability without any fine-tuning or domain adaptation, and LLaVA1.6-Mistral-7B achieves state-of-the-art performance, surpassing highly competitive task-specific MAD baselines by at least 23% in terms of equal error rate (EER). The results indicate that multimodal pretraining can implicitly encode fine-grained facial inconsistencies indicative of morphing artifacts, enabling zero-shot forensic sensitivity. Our findings position open-source MLLMs as reproducible, interpretable, and competitive foundations for biometric security and forensic image analysis. This emergent capability also highlights new opportunities to develop state-of-the-art MAD systems through targeted fine-tuning or lightweight adaptation, further improving accuracy and efficiency while preserving interpretability. To support future research, all code and evaluation protocols will be released upon publication.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">생체 보안을 위한 다중모달 모델 기술이 스포츠 이미지 분석에 간접적으로 참고될 수 있습니다.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11628v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11628v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11628v1">PLESS: Pseudo-Label Enhancement with Spreading Scribbles for Weakly Supervised Segmentation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">60.4</span>
            
            <a href="https://arxiv.org/pdf/2602.11628v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Weakly supervised learning with scribble annotations uses sparse user-drawn strokes to indicate segmentation labels on a small subset of pixels. This annotation reduces the cost of dense pixel-wise labeling, but suffers inherently from noisy and incomplete supervision.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Weakly supervised learning with scribble annotations uses sparse user-drawn strokes to indicate segmentation labels on a small subset of pixels. This annotation reduces the cost of dense pixel-wise labeling, but suffers inherently from noisy and incomplete supervision. Recent scribble-based approaches in medical image segmentation address this limitation using pseudo-label-based training; however, the quality of the pseudo-labels remains a key performance limit. We propose PLESS, a generic pseudo-label enhancement strategy which improves reliability and spatial consistency. It builds on a hierarchical partitioning of the image into a hierarchy of spatially coherent regions. PLESS propagates scribble information to refine pseudo-labels within semantically coherent regions. The framework is model-agnostic and easily integrates into existing pseudo-label methods. Experiments on two public cardiac MRI datasets (ACDC and MSCMRseg) across four scribble-supervised algorithms show consistent improvements in segmentation accuracy. Code will be made available on GitHub upon acceptance.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">약한 감독 분할 방법론으로 간접적 관련 있음</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.16918v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.16918v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.16918v1">Xray-Visual Models: Scaling Vision models on Industry Scale Data</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">60.0</span>
            
            <a href="https://arxiv.org/pdf/2602.16918v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">대규모 비전 모델로 스포츠 비디오 이해에 간접적 적용 가능.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.18014v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.18014v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18014v1">Quasi-Periodic Gaussian Process Predictive Iterative Learning Control</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">60.0</span>
            
            <a href="https://arxiv.org/pdf/2602.18014v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">eess.SY</span>
          
          <span class="cat-tag">stat.ML</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations. This work incorporates the use of Quasi-Periodic Gaussian Processes (QPGPs) into a predictive ILC framework to model and forecast disturbances and drift across iterations. Using a recent structural equation formulation of QPGPs, the proposed approach enables efficient inference with complexity $\mathcal{O}(p^3)$ instead of $\mathcal{O}(i^2p^3)$, where $p$ denotes the number of points within an iteration and $i$ represents the total number of iterations, specially for larger $i$. This formulation also enables parameter estimation without loss of information, making continual GP learning computationally feasible within the control loop. By predicting next-iteration error profiles rather than relying only on past errors, the controller achieves faster convergence and maintains this under time-varying disturbances. We benchmark the method against both standard ILC and conventional Gaussian Process (GP)-based predictive ILC on three tasks, autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment. Across all cases, the proposed approach converges faster and remains robust under injected and natural disturbances while reducing computational cost. This highlights its practicality across a range of repetitive dynamical systems.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">반복 작업 제어가 운동 분석에 간접적으로 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.18291v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.18291v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18291v1">Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">60.0</span>
            
            <a href="https://arxiv.org/pdf/2602.18291v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Multi-agent coordination methodology potentially applicable to future sports AI hardware.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13444v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13444v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13444v1">FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">58.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13444v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Recent vision-language-action (VLA) models can generate plausible end-effector motions, yet they often fail in long-horizon, contact-rich tasks because the underlying hand-object interaction (HOI) structure is not explicitly represented. An embodiment-agnostic interaction representation that captures this structure would make manipulation behaviors easier to validate and transfer across robots.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent vision-language-action (VLA) models can generate plausible end-effector motions, yet they often fail in long-horizon, contact-rich tasks because the underlying hand-object interaction (HOI) structure is not explicitly represented. An embodiment-agnostic interaction representation that captures this structure would make manipulation behaviors easier to validate and transfer across robots. We propose FlowHOI, a two-stage flow-matching framework that generates semantically grounded, temporally coherent HOI sequences, comprising hand poses, object poses, and hand-object contact states, conditioned on an egocentric observation, a language instruction, and a 3D Gaussian splatting (3DGS) scene reconstruction. We decouple geometry-centric grasping from semantics-centric manipulation, conditioning the latter on compact 3D scene tokens and employing a motion-text alignment loss to semantically ground the generated interactions in both the physical scene layout and the language instruction. To address the scarcity of high-fidelity HOI supervision, we introduce a reconstruction pipeline that recovers aligned hand-object trajectories and meshes from large-scale egocentric videos, yielding an HOI prior for robust generation. Across the GRAB and HOT3D benchmarks, FlowHOI achieves the highest action recognition accuracy and a 1.7$\times$ higher physics simulation success rate than the strongest diffusion-based baseline, while delivering a 40$\times$ inference speedup. We further demonstrate real-robot execution on four dexterous manipulation tasks, illustrating the feasibility of retargeting generated HOI representations to real-robot execution pipelines.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">손-물체 상호작용 생성이 스포츠 분석에 간접적 참고 가능.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13637v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13637v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13637v1">DCDM: Divide-and-Conquer Diffusion Models for Consistency-Preserving Video Generation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">58.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13637v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Recent video generative models have demonstrated impressive visual fidelity, yet they often struggle with semantic, geometric, and identity consistency. In this paper, we propose a system-level framework, termed the Divide-and-Conquer Diffusion Model (DCDM), to address three key challenges: (1) intra-clip world knowledge consistency, (2) inter-clip camera consistency, and (3) inter-shot element consistency.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent video generative models have demonstrated impressive visual fidelity, yet they often struggle with semantic, geometric, and identity consistency. In this paper, we propose a system-level framework, termed the Divide-and-Conquer Diffusion Model (DCDM), to address three key challenges: (1) intra-clip world knowledge consistency, (2) inter-clip camera consistency, and (3) inter-shot element consistency. DCDM decomposes video consistency modeling under these scenarios into three dedicated components while sharing a unified video generation backbone. For intra-clip consistency, DCDM leverages a large language model to parse input prompts into structured semantic representations, which are subsequently translated into coherent video content by a diffusion transformer. For inter-clip camera consistency, we propose a temporal camera representation in the noise space that enables precise and stable camera motion control, along with a text-to-image initialization mechanism to further enhance controllability. For inter-shot consistency, DCDM adopts a holistic scene generation paradigm with windowed cross-attention and sparse inter-shot self-attention, ensuring long-range narrative coherence while maintaining computational efficiency. We validate our framework on the test set of the CVM Competition at AAAI&#39;26, and the results demonstrate that the proposed strategies effectively address these challenges.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Video generation consistency; indirect relevance to highlight editing.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12405v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12405v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12405v1">Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">58.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12405v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Reasoning about failures is crucial for building reliable and trustworthy robotic systems. Prior approaches either treat failure reasoning as a closed-set classification problem or assume access to ample human annotations.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Reasoning about failures is crucial for building reliable and trustworthy robotic systems. Prior approaches either treat failure reasoning as a closed-set classification problem or assume access to ample human annotations. Failures in the real world are typically subtle, combinatorial, and difficult to enumerate, whereas rich reasoning labels are expensive to acquire. We address this problem by introducing ARMOR: Adaptive Round-based Multi-task mOdel for Robotic failure detection and reasoning. We formulate detection and reasoning as a multi-task self-refinement process, where the model iteratively predicts detection outcomes and natural language reasoning conditioned on past outputs. During training, ARMOR learns from heterogeneous supervision - large-scale sparse binary labels and small-scale rich reasoning annotations - optimized via a combination of offline and online imitation learning. At inference time, ARMOR generates multiple refinement trajectories and selects the most confident prediction via a self-certainty metric. Experiments across diverse environments show that ARMOR achieves state-of-the-art performance by improving over the previous approaches by up to 30% on failure detection rate and up to 100% in reasoning measured through LLM fuzzy match score, demonstrating robustness to heterogeneous supervision and open-ended reasoning beyond predefined failure modes. We provide dditional visualizations on our website: https://sites.google.com/utexas.edu/armor</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">실패 감지 및 추론 기술이 스포츠 오류 분석에 적용 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.15917v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.15917v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.15917v1">ROIX-Comp: Optimizing X-ray Computed Tomography Imaging Strategy for Data Reduction and Reconstruction</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">58.0</span>
            
            <a href="https://arxiv.org/pdf/2602.15917v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">eess.IV</span>
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.DC</span>
          
          <span class="cat-tag">cs.IT</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">In high-performance computing (HPC) environments, particularly in synchrotron radiation facilities, vast amounts of X-ray images are generated. Processing large-scale X-ray Computed Tomography (X-CT) datasets presents significant computational and storage challenges due to their high dimensionality and data volume.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">In high-performance computing (HPC) environments, particularly in synchrotron radiation facilities, vast amounts of X-ray images are generated. Processing large-scale X-ray Computed Tomography (X-CT) datasets presents significant computational and storage challenges due to their high dimensionality and data volume. Traditional approaches often require extensive storage capacity and high transmission bandwidth, limiting real-time processing capabilities and workflow efficiency. To address these constraints, we introduce a region-of-interest (ROI)-driven extraction framework (ROIX-Comp) that intelligently compresses X-CT data by identifying and retaining only essential features. Our work reduces data volume while preserving critical information for downstream processing tasks. At pre-processing stage, we utilize error-bounded quantization to reduce the amount of data to be processed and therefore improve computational efficiencies. At the compression stage, our methodology combines object extraction with multiple state-of-the-art lossless and lossy compressors, resulting in significantly improved compression ratios. We evaluated this framework against seven X-CT datasets and observed a relative compression ratio improvement of 12.34x compared to the standard compression.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">데이터 압축 기술이 엣지 장비 영상 처리에 간접적 참고 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14147v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14147v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14147v1">LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">58.0</span>
            
            <a href="https://arxiv.org/pdf/2602.14147v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1&#39;s strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Multimodal diffusion model for image editing and generation tasks, indirectly relevant to video/image correction.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13358v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13358v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13358v1">Location as a service with a MEC architecture</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">58.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13358v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.NI</span>
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">In recent years, automated driving has become viable, and advanced driver assistance systems (ADAS) are now part of modern cars. These systems require highly precise positioning.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">In recent years, automated driving has become viable, and advanced driver assistance systems (ADAS) are now part of modern cars. These systems require highly precise positioning. In this paper, a cooperative approach to localization is presented. The GPS information from several road users is collected in a Mobile Edge Computing cloud, and the characteristics of GNSS positioning are used to provide lane-precise positioning for all participants by applying probabilistic filters and HD maps.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Mobile Edge Computing for localization; indirectly relevant to edge device architecture.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11653v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11653v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11653v1">GR-Diffusion: 3D Gaussian Representation Meets Diffusion in Whole-Body PET Reconstruction</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">58.0</span>
            
            <a href="https://arxiv.org/pdf/2602.11653v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Positron emission tomography (PET) reconstruction is a critical challenge in molecular imaging, often hampered by noise amplification, structural blurring, and detail loss due to sparse sampling and the ill-posed nature of inverse problems. The three-dimensional discrete Gaussian representation (GR), which efficiently encodes 3D scenes using parameterized discrete Gaussian distributions, has shown promise in computer vision.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Positron emission tomography (PET) reconstruction is a critical challenge in molecular imaging, often hampered by noise amplification, structural blurring, and detail loss due to sparse sampling and the ill-posed nature of inverse problems. The three-dimensional discrete Gaussian representation (GR), which efficiently encodes 3D scenes using parameterized discrete Gaussian distributions, has shown promise in computer vision. In this work, we pro-pose a novel GR-Diffusion framework that synergistically integrates the geometric priors of GR with the generative power of diffusion models for 3D low-dose whole-body PET reconstruction. GR-Diffusion employs GR to generate a reference 3D PET image from projection data, establishing a physically grounded and structurally explicit benchmark that overcomes the low-pass limitations of conventional point-based or voxel-based methods. This reference image serves as a dual guide during the diffusion process, ensuring both global consistency and local accuracy. Specifically, we employ a hierarchical guidance mechanism based on the GR reference. Fine-grained guidance leverages differences to refine local details, while coarse-grained guidance uses multi-scale difference maps to correct deviations. This strategy allows the diffusion model to sequentially integrate the strong geometric prior from GR and recover sub-voxel information. Experimental results on the UDPET and Clinical datasets with varying dose levels show that GR-Diffusion outperforms state-of-the-art methods in enhancing 3D whole-body PET image quality and preserving physiological details.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">PET reconstruction method indirectly applicable</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11564v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11564v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11564v1">LUVE : Latent-Cascaded Ultra-High-Resolution Video Generation with Dual Frequency Experts</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">56.4</span>
            
            <a href="https://arxiv.org/pdf/2602.11564v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Recent advances in video diffusion models have significantly improved visual quality, yet ultra-high-resolution (UHR) video generation remains a formidable challenge due to the compounded difficulties of motion modeling, semantic planning, and detail synthesis. To address these limitations, we propose \textbf{LUVE}, a \textbf{L}atent-cascaded \textbf{U}HR \textbf{V}ideo generation framework built upon dual frequency \textbf{E}xperts.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent advances in video diffusion models have significantly improved visual quality, yet ultra-high-resolution (UHR) video generation remains a formidable challenge due to the compounded difficulties of motion modeling, semantic planning, and detail synthesis. To address these limitations, we propose \textbf{LUVE}, a \textbf{L}atent-cascaded \textbf{U}HR \textbf{V}ideo generation framework built upon dual frequency \textbf{E}xperts. LUVE employs a three-stage architecture comprising low-resolution motion generation for motion-consistent latent synthesis, video latent upsampling that performs resolution upsampling directly in the latent space to mitigate memory and computational overhead, and high-resolution content refinement that integrates low-frequency and high-frequency experts to jointly enhance semantic coherence and fine-grained detail generation. Extensive experiments demonstrate that our LUVE achieves superior photorealism and content fidelity in UHR video generation, and comprehensive ablation studies further validate the effectiveness of each component. The project is available at \href{https://unicornanrocinu.github.io/LUVE_web/}{https://github.io/LUVE/}.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">초고해상도 비디오 생성 기술이 하이라이트 생성에 간접적으로 활용 가능함.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.18006v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.18006v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18006v1">MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">56.0</span>
            
            <a href="https://arxiv.org/pdf/2602.18006v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Real-time object tracking weakly related to movement analysis</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.16950v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.16950v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.16950v1">HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">56.0</span>
            
            <a href="https://arxiv.org/pdf/2602.16950v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Agricultural 3D reconstruction; indirect relevance to motion analysis.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12740v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12740v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12740v1">SPRig: Self-Supervised Pose-Invariant Rigging from Mesh Sequences</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.8</span>
            
            <a href="https://arxiv.org/pdf/2602.12740v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.GR</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">State-of-the-art rigging methods assume a canonical rest pose--an assumption that fails for sequential data (e.g., animal motion capture or AIGC/video-derived mesh sequences) that lack the T-pose. Applied frame-by-frame, these methods are not pose-invariant and produce topological inconsistencies across frames.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">State-of-the-art rigging methods assume a canonical rest pose--an assumption that fails for sequential data (e.g., animal motion capture or AIGC/video-derived mesh sequences) that lack the T-pose. Applied frame-by-frame, these methods are not pose-invariant and produce topological inconsistencies across frames. Thus We propose SPRig, a general fine-tuning framework that enforces cross-frame consistency losses to learn pose-invariant rigs on top of existing models. We validate our approach on rigging using a new permutation-invariant stability protocol. Experiments demonstrate SOTA temporal stability: our method produces coherent rigs from challenging sequences and dramatically reduces the artifacts that plague baseline methods. The code will be released publicly upon acceptance.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">메시 시퀀스의 포즈 불변 리깅 연구로 스포츠 동작 분석과 간접적 관련 있음.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.18071v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.18071v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18071v1">EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.4</span>
            
            <a href="https://arxiv.org/pdf/2602.18071v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher&#39;s observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student&#39;s viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">에고센트릭 인식 기술이 모바일 촬영에 약간 참고 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13772v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13772v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13772v1">Offline-Poly: A Polyhedral Framework For Offline 3D Multi-Object Tracking</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13772v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Offline 3D multi-object tracking (MOT) is a critical component of the 4D auto-labeling (4DAL) process. It enhances pseudo-labels generated by high-performance detectors through the incorporation of temporal context.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Offline 3D multi-object tracking (MOT) is a critical component of the 4D auto-labeling (4DAL) process. It enhances pseudo-labels generated by high-performance detectors through the incorporation of temporal context. However, existing offline 3D MOT approaches are direct extensions of online frameworks and fail to fully exploit the advantages of offline setting. Moreover, these methods often depend on fixed upstream and customized architectures, limiting their adaptability. To address these limitations, we propose Offline-Poly, a general offline 3D MOT method based on a tracking-centric design. We introduce a standardized paradigm termed Tracking-by-Tracking (TBT), which operates exclusively on arbitrary off-the-shelf tracking outputs and produces offline-refined tracklets. This formulation decouples offline tracker from specific upstream detectors or trackers. Under the TBT paradigm, Offline-Poly accepts one or multiple coarse tracking results and processes them through a structured pipeline comprising pre-processing, hierarchical matching and fusion, and tracklet refinement. Each module is designed to capitalize on the two fundamental properties of offline tracking: resource unconstrainedness, which permits global optimization beyond real-time limits, and future observability, which enables tracklet reasoning over the full temporal horizon. Offline-Poly first eliminates short-term ghost tracklets and re-identifies fragmented segments using global scene context. It then constructs scene-level similarity to associate tracklets across multiple input sources. Finally, Offline-Poly refines tracklets by jointly leveraging local and global motion patterns. On nuScenes, we achieve SOTA performance with 77.6% AMOTA. On KITTI, it achieves leading results with 83.00% HOTA. Comprehensive experiments further validate the flexibility, generalizability, and modular effectiveness of Offline-Poly.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Offline object tracking methodology for motion refinement</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11903v2">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11903v2" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11903v2">Learning Perceptual Representations for Gaming NR-VQA with Multi-Task FR Signals</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.0</span>
            
            <a href="https://arxiv.org/pdf/2602.11903v2" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">eess.IV</span>
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.MM</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">No-reference video quality assessment (NR-VQA) for gaming videos is challenging due to limited human-rated datasets and unique content characteristics including fast motion, stylized graphics, and compression artifacts. We present MTL-VQA, a multi-task learning framework that uses full-reference metrics as supervisory signals to learn perceptually meaningful features without human labels for pretraining.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">No-reference video quality assessment (NR-VQA) for gaming videos is challenging due to limited human-rated datasets and unique content characteristics including fast motion, stylized graphics, and compression artifacts. We present MTL-VQA, a multi-task learning framework that uses full-reference metrics as supervisory signals to learn perceptually meaningful features without human labels for pretraining. By jointly optimizing multiple full-reference (FR) objectives with adaptive task weighting, our approach learns shared representations that transfer effectively to NR-VQA. Experiments on gaming video datasets show MTL-VQA achieves performance competitive with state-of-the-art NR-VQA methods across both MOS-supervised and label-efficient/self-supervised settings.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">게임 비디오 품질 평가 방법이 영상 보정에 간접적 참고 가능</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13751v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13751v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13751v1">T2MBench: A Benchmark for Out-of-Distribution Text-to-Motion Generation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13751v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Most existing evaluations of text-to-motion generation focus on in-distribution textual inputs and a limited set of evaluation criteria, which restricts their ability to systematically assess model generalization and motion generation capabilities under complex out-of-distribution (OOD) textual conditions. To address this limitation, we propose a benchmark specifically designed for OOD text-to-motion evaluation, which includes a comprehensive analysis of 14 representative baseline models and the two datasets derived from evaluation results.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Most existing evaluations of text-to-motion generation focus on in-distribution textual inputs and a limited set of evaluation criteria, which restricts their ability to systematically assess model generalization and motion generation capabilities under complex out-of-distribution (OOD) textual conditions. To address this limitation, we propose a benchmark specifically designed for OOD text-to-motion evaluation, which includes a comprehensive analysis of 14 representative baseline models and the two datasets derived from evaluation results. Specifically, we construct an OOD prompt dataset consisting of 1,025 textual descriptions. Based on this prompt dataset, we introduce a unified evaluation framework that integrates LLM-based Evaluation, Multi-factor Motion evaluation, and Fine-grained Accuracy Evaluation. Our experimental results reveal that while different baseline models demonstrate strengths in areas such as text-to-motion semantic alignment, motion generalizability, and physical quality, most models struggle to achieve strong performance with Fine-grained Accuracy Evaluation. These findings highlight the limitations of existing methods in OOD scenarios and offer practical guidance for the design and evaluation of future production-level text-to-motion models.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">텍스트-동작 벤치마크가 스포츠 동작 모델 평가에 참고 가능.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12361v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12361v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12361v1">Thermal Imaging for Contactless Cardiorespiratory and Sudomotor Response Monitoring</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12361v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Thermal infrared imaging captures skin temperature changes driven by autonomic regulation and can potentially provide contactless estimation of electrodermal activity (EDA), heart rate (HR), and breathing rate (BR). While visible-light methods address HR and BR, they cannot access EDA, a standard marker of sympathetic activation.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Thermal infrared imaging captures skin temperature changes driven by autonomic regulation and can potentially provide contactless estimation of electrodermal activity (EDA), heart rate (HR), and breathing rate (BR). While visible-light methods address HR and BR, they cannot access EDA, a standard marker of sympathetic activation. This paper characterizes the extraction of these three biosignals from facial thermal video using a signal-processing pipeline that tracks anatomical regions, applies spatial aggregation, and separates slow sudomotor trends from faster cardiorespiratory components. For HR, we apply an orthogonal matrix image transformation (OMIT) decomposition across multiple facial regions of interest (ROIs), and for BR we average nasal and cheek signals before spectral peak detection. We evaluate 288 EDA configurations and the HR/BR pipeline on 31 sessions from the public SIMULATOR STUDY 1 (SIM1) driver monitoring dataset. The best fixed EDA configuration (nose region, exponential moving average) reaches a mean absolute correlation of $0.40 \pm 0.23$ against palm EDA, with individual sessions reaching 0.89. BR estimation achieves a mean absolute error of $3.1 \pm 1.1$ bpm, while HR estimation yields $13.8 \pm 7.5$ bpm MAE, limited by the low camera frame rate (7.5 Hz). We report signal polarity alternation across sessions, short thermodynamic latency for well-tracked signals, and condition-dependent and demographic effects on extraction quality. These results provide baseline performance bounds and design guidance for thermal contactless biosignal estimation.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">운동 중 생리신호 모니터링 기술 간접적 참조</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12532v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12532v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12532v1">CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12532v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Vision-Language-Action (VLA) models have shown a strong capability in enabling robots to execute general instructions, yet they struggle with contact-rich manipulation tasks, where success requires precise alignment, stable contact maintenance, and effective handling of deformable objects. A fundamental challenge arises from the imbalance between high-entropy vision and language inputs and low-entropy but critical force signals, which often leads to over-reliance on perception and unstable control.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Vision-Language-Action (VLA) models have shown a strong capability in enabling robots to execute general instructions, yet they struggle with contact-rich manipulation tasks, where success requires precise alignment, stable contact maintenance, and effective handling of deformable objects. A fundamental challenge arises from the imbalance between high-entropy vision and language inputs and low-entropy but critical force signals, which often leads to over-reliance on perception and unstable control. To address this, we introduce CRAFT, a force-aware curriculum fine-tuning framework that integrates a variational information bottleneck module to regulate vision and language embeddings during early training. This curriculum strategy encourages the model to prioritize force signals initially, before progressively restoring access to the full multimodal information. To enable force-aware learning, we further design a homologous leader-follower teleoperation system that collects synchronized vision, language, and force data across diverse contact-rich tasks. Real-world experiments demonstrate that CRAFT consistently improves task success, generalizes to unseen objects and novel task variations, and adapts effectively across diverse VLA architectures, enabling robust and generalizable contact-rich manipulation.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">물리적 상호작용 분석 기술이 스포츠 동작 분석에 간접적 참고</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12381v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12381v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12381v1">Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12381v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research. Prior work has highlighted how synthetic images differ from real photographs--unfortunately, SID methods often struggle to generalize to novel generative models and often perform poorly in practical settings. CLIP, a foundational vision-language model which yields semantically rich image-text embeddings, shows strong accuracy and generalization for SID. Yet, the underlying relevant cues embedded in CLIP-features remain unknown. It is unclear, whether CLIP-based detectors simply detect strong visual artifacts or exploit subtle semantic biases, both of which would render them useless in practical settings or on generative models of high quality. We introduce SynthCLIC, a paired dataset of real photographs and high-quality synthetic counterparts from recent diffusion models, designed to reduce semantic bias in SID. Using an interpretable linear head with de-correlated activations and a text-grounded concept-model, we analyze what CLIP-based detectors learn. CLIP-based linear detectors reach 0.96 mAP on a GAN-based benchmark but only 0.92 on our high-quality diffusion dataset SynthCLIC, and generalization across generator families drops to as low as 0.37 mAP. We find that the detectors primarily rely on high-level photographic attributes (e.g., minimalist style, lens flare, or depth layering), rather than overt generator-specific artifacts. CLIP-based detectors perform well overall but generalize unevenly across diverse generative architectures. This highlights the need for continual model updates and broader training exposure, while reinforcing CLIP-based approaches as a strong foundation for more universal, robust SID.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">합성 이미지 감지 기술이 간접적으로 관련됨</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12127v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12127v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12127v1">PosterOmni: Generalized Artistic Poster Creation via Task Distillation and Unified Reward Feedback</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12127v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Image-to-poster generation is a high-demand task requiring not only local adjustments but also high-level design understanding. Models must generate text, layout, style, and visual elements while preserving semantic fidelity and aesthetic coherence.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Image-to-poster generation is a high-demand task requiring not only local adjustments but also high-level design understanding. Models must generate text, layout, style, and visual elements while preserving semantic fidelity and aesthetic coherence. The process spans two regimes: local editing, where ID-driven generation, rescaling, filling, and extending must preserve concrete visual entities; and global creation, where layout- and style-driven tasks rely on understanding abstract design concepts. These intertwined demands make image-to-poster a multi-dimensional process coupling entity-preserving editing with concept-driven creation under image-prompt control. To address these challenges, we propose PosterOmni, a generalized artistic poster creation framework that unlocks the potential of a base edit model for multi-task image-to-poster generation. PosterOmni integrates the two regimes, namely local editing and global creation, within a single system through an efficient data-distillation-reward pipeline: (i) constructing multi-scenario image-to-poster datasets covering six task types across entity-based and concept-based creation; (ii) distilling knowledge between local and global experts for supervised fine-tuning; and (iii) applying unified PosterOmni Reward Feedback to jointly align visual entity-preserving and aesthetic preference across all tasks. Additionally, we establish PosterOmni-Bench, a unified benchmark for evaluating both local editing and global creation. Extensive experiments show that PosterOmni significantly enhances reference adherence, global composition quality, and aesthetic harmony, outperforming all open-source baselines and even surpassing several proprietary systems.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">이미지 편집 기술이 프로젝트의 영상 보정과 관련 있음.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11596v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11596v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11596v1">MAPLE: Modality-Aware Post-training and Learning Ecosystem</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.0</span>
            
            <a href="https://arxiv.org/pdf/2602.11596v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Multimodal language models now integrate text, audio, and video for unified reasoning. Yet existing RL post-training pipelines treat all input signals as equally relevant, ignoring which modalities each task actually requires.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Multimodal language models now integrate text, audio, and video for unified reasoning. Yet existing RL post-training pipelines treat all input signals as equally relevant, ignoring which modalities each task actually requires. This modality-blind training inflates policy-gradient variance, slows convergence, and degrades robustness to real-world distribution shifts where signals may be missing, added, or reweighted. We introduce MAPLE, a complete modality-aware post-training and learning ecosystem comprising: (1) MAPLE-bench, the first benchmark explicitly annotating minimal signal combinations required per task; (2) MAPO, a modality-aware policy optimization framework that stratifies batches by modality requirement to reduce gradient variance from heterogeneous group advantages; (3) Adaptive weighting and curriculum scheduling that balances and prioritizes harder signal combinations. Systematic analysis across loss aggregation, clipping, sampling, and curriculum design establishes MAPO&#39;s optimal training strategy. Adaptive weighting and curriculum focused learning further boost performance across signal combinations. MAPLE narrows uni/multi-modal accuracy gaps by 30.24%, converges 3.18x faster, and maintains stability across all modality combinations under realistic reduced signal access. MAPLE constitutes a complete recipe for deployment-ready multimodal RL post-training.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Indirect multimodal training</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14027v2">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14027v2" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14027v2">Train Short, Inference Long: Training-free Horizon Extension for Autoregressive Video Generation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.0</span>
            
            <a href="https://arxiv.org/pdf/2602.14027v2" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Autoregressive video diffusion models have emerged as a scalable paradigm for long video generation. However, they often suffer from severe extrapolation failure, where rapid error accumulation leads to significant temporal degradation when extending beyond training horizons.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Autoregressive video diffusion models have emerged as a scalable paradigm for long video generation. However, they often suffer from severe extrapolation failure, where rapid error accumulation leads to significant temporal degradation when extending beyond training horizons. We identify that this failure primarily stems from the spectral bias of 3D positional embeddings and the lack of dynamic priors in noise sampling. To address these issues, we propose FLEX (Frequency-aware Length EXtension), a training-free inference-time framework that bridges the gap between short-term training and long-term inference. FLEX introduces Frequency-aware RoPE Modulation to adaptively interpolate under-trained low-frequency components while extrapolating high-frequency ones to preserve multi-scale temporal discriminability. This is integrated with Antiphase Noise Sampling (ANS) to inject high-frequency dynamic priors and Inference-only Attention Sink to anchor global structure. Extensive evaluations on VBench demonstrate that FLEX significantly outperforms state-of-the-art models at 6x extrapolation (30s duration) and matches the performance of long-video fine-tuned baselines at 12x scale (60s duration). As a plug-and-play augmentation, FLEX seamlessly integrates into existing inference pipelines for horizon extension. It effectively pushes the generation limits of models such as LongLive, supporting consistent and dynamic video synthesis at a 4-minute scale. Project page is available at https://ga-lee.github.io/FLEX_demo.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Video generation extension indirectly useful for highlight editing.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13920v2">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13920v2" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13920v2">A Comparative Analysis of Social Network Topology in Reddit and Moltbook</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">54.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13920v2" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.SI</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Recent advances in agent-mediated systems have enabled a new paradigm of social network simulation, where AI agents interact with human-like autonomy. This evolution has fostered the emergence of agent-driven social networks such as Moltbook, a Reddit-like platform populated entirely by AI agents.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent advances in agent-mediated systems have enabled a new paradigm of social network simulation, where AI agents interact with human-like autonomy. This evolution has fostered the emergence of agent-driven social networks such as Moltbook, a Reddit-like platform populated entirely by AI agents. Despite these developments, empirical comparisons between agent-driven and human-driven social networks remain scarce, limiting our understanding of how their network topologies might diverge. This paper presents the first comparative analysis of network topology on Moltbook, utilizing a comment network comprising 33,577 nodes and 697,688 edges. To provide a benchmark, we curated a parallel dataset from Reddit consisting of 7.8 million nodes and 51.8 million edges. We examine key structural differences between agent-drive and human-drive networks, specifically focusing on topological patterns and the edge formation efficacy of their respective posts. Our findings provide a foundational profile of AI-driven social structures, serving as a preliminary step toward developing more robust and authentic agent-mediated social systems.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Social network analysis relevant to sharing platform.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.11464v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.11464v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11464v1">EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">52.4</span>
            
            <a href="https://arxiv.org/pdf/2602.11464v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Robot imitation learning is often hindered by the high cost of collecting large-scale, real-world data. This challenge is especially significant for low-cost robots designed for home use, as they must be both user-friendly and affordable.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Robot imitation learning is often hindered by the high cost of collecting large-scale, real-world data. This challenge is especially significant for low-cost robots designed for home use, as they must be both user-friendly and affordable. To address this, we propose the EasyMimic framework, a low-cost and replicable solution that enables robots to quickly learn manipulation policies from human video demonstrations captured with standard RGB cameras. Our method first extracts 3D hand trajectories from the videos. An action alignment module then maps these trajectories to the gripper control space of a low-cost robot. To bridge the human-to-robot domain gap, we introduce a simple and user-friendly hand visual augmentation strategy. We then use a co-training method, fine-tuning a model on both the processed human data and a small amount of robot data, enabling rapid adaptation to new tasks. Experiments on the low-cost LeRobot platform demonstrate that EasyMimic achieves high performance across various manipulation tasks. It significantly reduces the reliance on expensive robot data collection, offering a practical path for bringing intelligent robots into homes. Project website: https://zt375356.github.io/EasyMimic-Project/.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">로봇 모방 학습 프레임워크가 스포츠 동작 분석에 간접적으로 참고될 수 있음.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14409v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14409v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14409v1">Learning Proposes, Geometry Disposes: A Modular Framework for Efficient Spatial Reasoning</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">52.0</span>
            
            <a href="https://arxiv.org/pdf/2602.14409v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Spatial perception aims to estimate camera motion and scene structure from visual observations, a problem traditionally addressed through geometric modeling and physical consistency constraints. Recent learning-based methods have demonstrated strong representational capacity for geometric perception and are increasingly used to augment classical geometry-centric systems in practice.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Spatial perception aims to estimate camera motion and scene structure from visual observations, a problem traditionally addressed through geometric modeling and physical consistency constraints. Recent learning-based methods have demonstrated strong representational capacity for geometric perception and are increasingly used to augment classical geometry-centric systems in practice. However, whether learning components should directly replace geometric estimation or instead serve as intermediate modules within such pipelines remains an open question.   In this work, we address this gap and investigate an end-to-end modular framework for effective spatial reasoning, where learning proposes geometric hypotheses, while geometric algorithms dispose estimation decisions. In particular, we study this principle in the context of relative camera pose estimation on RGB-D sequences. Using VGGT as a representative learning model, we evaluate learning-based pose and depth proposals under varying motion magnitudes and scene dynamics, followed by a classical point-to-plane RGB-D ICP as the geometric backend. Our experiments on the TUM RGB-D benchmark reveal three consistent findings: (1) learning-based pose proposals alone are unreliable; (2) learning-proposed geometry, when improperly aligned with camera intrinsics, can degrade performance; and (3) when learning-proposed depth is geometrically aligned and followed by a geometric disposal stage, consistent improvements emerge in moderately challenging rigid settings.   These results demonstrate that geometry is not merely a refinement component, but an essential arbiter that validates and absorbs learning-based geometric observations. Our study highlights the importance of modular, geometry-aware system design for robust spatial perception.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Modular spatial reasoning for pose estimation reference</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12215v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12215v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12215v1">LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">50.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12215v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Recent robot foundation models largely rely on large-scale behavior cloning, which imitates expert actions but discards transferable dynamics knowledge embedded in heterogeneous embodied data. While the Unified World Model (UWM) formulation has the potential to leverage such diverse data, existing instantiations struggle to scale to foundation-level due to coarse data usage and fragmented datasets.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recent robot foundation models largely rely on large-scale behavior cloning, which imitates expert actions but discards transferable dynamics knowledge embedded in heterogeneous embodied data. While the Unified World Model (UWM) formulation has the potential to leverage such diverse data, existing instantiations struggle to scale to foundation-level due to coarse data usage and fragmented datasets. We introduce LDA-1B, a robot foundation model that scales through universal embodied data ingestion by jointly learning dynamics, policy, and visual forecasting, assigning distinct roles to data of varying quality. To support this regime at scale, we assemble and standardize EI-30k, an embodied interaction dataset comprising over 30k hours of human and robot trajectories in a unified format. Scalable dynamics learning over such heterogeneous data is enabled by prediction in a structured DINO latent space, which avoids redundant pixel-space appearance modeling. Complementing this representation, LDA-1B employs a multi-modal diffusion transformer to handle asynchronous vision and action streams, enabling stable training at the 1B-parameter scale. Experiments in simulation and the real world show LDA-1B outperforms prior methods (e.g., $π_{0.5}$) by up to 21\%, 48\%, and 23\% on contact-rich, dexterous, and long-horizon tasks, respectively. Notably, LDA-1B enables data-efficient fine-tuning, gaining 10\% by leveraging 30\% low-quality trajectories typically harmful and discarded.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">로봇 기초 모델로 스포츠 동작 분석에 참고될 수 있으나 직접적 관련성은 낮음.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12796v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12796v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12796v1">GSM-GS: Geometry-Constrained Single and Multi-view Gaussian Splatting for Surface Reconstruction</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">50.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12796v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.GR</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Recently, 3D Gaussian Splatting has emerged as a prominent research direction owing to its ultrarapid training speed and high-fidelity rendering capabilities. However, the unstructured and irregular nature of Gaussian point clouds poses challenges to reconstruction accuracy.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Recently, 3D Gaussian Splatting has emerged as a prominent research direction owing to its ultrarapid training speed and high-fidelity rendering capabilities. However, the unstructured and irregular nature of Gaussian point clouds poses challenges to reconstruction accuracy. This limitation frequently causes high-frequency detail loss in complex surface microstructures when relying solely on routine strategies. To address this limitation, we propose GSM-GS: a synergistic optimization framework integrating single-view adaptive sub-region weighting constraints and multi-view spatial structure refinement. For single-view optimization, we leverage image gradient features to partition scenes into texture-rich and texture-less sub-regions. The reconstruction quality is enhanced through adaptive filtering mechanisms guided by depth discrepancy features. This preserves high-weight regions while implementing a dual-branch constraint strategy tailored to regional texture variations, thereby improving geometric detail characterization. For multi-view optimization, we introduce a geometry-guided cross-view point cloud association method combined with a dynamic weight sampling strategy. This constructs 3D structural normal constraints across adjacent point cloud frames, effectively reinforcing multi-view consistency and reconstruction fidelity. Extensive experiments on public datasets demonstrate that our method achieves both competitive rendering quality and geometric reconstruction. See our interactive project page</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">3D reconstruction method, indirectly useful for posture analysis.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.12003v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.12003v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12003v1">Projected Representation Conditioning for High-fidelity Novel View Synthesis</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">50.0</span>
            
            <a href="https://arxiv.org/pdf/2602.12003v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">We propose a novel framework for diffusion-based novel view synthesis in which we leverage external representations as conditions, harnessing their geometric and semantic correspondence properties for enhanced geometric consistency in generated novel viewpoints. First, we provide a detailed analysis exploring the correspondence capabilities emergent in the spatial attention of external visual representations.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">We propose a novel framework for diffusion-based novel view synthesis in which we leverage external representations as conditions, harnessing their geometric and semantic correspondence properties for enhanced geometric consistency in generated novel viewpoints. First, we provide a detailed analysis exploring the correspondence capabilities emergent in the spatial attention of external visual representations. Building from these insights, we propose a representation-guided novel view synthesis through dedicated representation projection modules that inject external representations into the diffusion process, a methodology named ReNoV, short for representation-guided novel view synthesis. Our experiments show that this design yields marked improvements in both reconstruction fidelity and inpainting quality, outperforming prior diffusion-based novel-view methods on standard benchmarks and enabling robust synthesis from sparse, unposed image collections.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Indirectly related to view synthesis for potential analysis.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.13689v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.13689v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13689v1">Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">50.0</span>
            
            <a href="https://arxiv.org/pdf/2602.13689v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Insertion tasks in robotic manipulation demand precise, contact-rich interactions that vision alone cannot resolve. While tactile feedback is intuitively valuable, existing studies have shown that naïve visuo-tactile fusion often fails to deliver consistent improvements.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Insertion tasks in robotic manipulation demand precise, contact-rich interactions that vision alone cannot resolve. While tactile feedback is intuitively valuable, existing studies have shown that naïve visuo-tactile fusion often fails to deliver consistent improvements. In this work, we propose a Cross-Modal Transformer (CMT) for visuo-tactile fusion that integrates wrist-camera observations with tactile signals through structured self- and cross-attention. To stabilize tactile embeddings, we further introduce a physics-informed regularization that encourages bilateral force balance, reflecting principles of human motor control. Experiments on the TacSL benchmark show that CMT with symmetry regularization achieves a 96.59% insertion success rate, surpassing naïve and gated fusion baselines and closely matching the privileged &#34;wrist + contact force&#34; configuration (96.09%). These results highlight two central insights: (i) tactile sensing is indispensable for precise alignment, and (ii) principled multimodal fusion, further strengthened by physics-informed regularization, unlocks complementary strengths of vision and touch, approaching privileged performance under realistic sensing.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">다중 감각 융합 기술이 동작 분석과 간접적 관련 있음.</div>
        
      </div>
      
      <div class="compact-card" data-paper-url="http://arxiv.org/abs/2602.14174v1">
        <div class="rank-title">
          <input type="checkbox" class="paper-checkbox" data-tier="2" data-pdf="https://arxiv.org/pdf/2602.14174v1" onchange="updateToolbar()">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14174v1">Direction Matters: Learning Force Direction Enables Sim-to-Real Contact-Rich Manipulation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">50.0</span>
            
            <a href="https://arxiv.org/pdf/2602.14174v1" class="compact-link">PDF</a>
            
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-body">
          <span class="abstract-preview">Sim-to-real transfer for contact-rich manipulation remains challenging due to the inherent discrepancy in contact dynamics. While existing methods often rely on costly real-world data or utilize blind compliance through fixed controllers, we propose a framework that leverages expert-designed controller logic for transfer.</span>
          
          <details class="abstract-full">
            <summary>more</summary>
            <div class="details-content">Sim-to-real transfer for contact-rich manipulation remains challenging due to the inherent discrepancy in contact dynamics. While existing methods often rely on costly real-world data or utilize blind compliance through fixed controllers, we propose a framework that leverages expert-designed controller logic for transfer. Inspired by the success of privileged supervision in kinematic tasks, we employ a human-designed finite state machine based position/force controller in simulation to provide privileged guidance. The resulting policy is trained to predict the end-effector pose, contact state, and crucially the desired contact force direction. Unlike force magnitudes, which are highly sensitive to simulation inaccuracies, force directions encode high-level task geometry and remain robust across the sim-to-real gap. At deployment, these predictions configure a force-aware admittance controller. By combining the policy&#39;s directional intent with a constant, low-cost manually tuned force magnitude, the system generates adaptive, task-aligned compliance. This tuning is lightweight, typically requiring only a single scalar per contact state. We provide theoretical analysis for stability and robustness to disturbances. Experiments on four real-world tasks, i.e., microwave opening, peg-in-hole, whiteboard wiping, and door opening, demonstrate that our approach significantly outperforms strong baselines in both success rate and robustness. Videos are available at: https://yifei-y.github.io/project-pages/DirectionMatters/.</div>
          </details>
          
        </div>
        
        
        <div class="compact-reason">Sim-to-real transfer indirectly related to physical hardware.</div>
        
      </div>
      
    
  </div>

  <div class="tab-content tab-panel-remind">
    
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.17706v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.17706v1">Parallel Complex Diffusion for Scalable Time Series Generation</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">85.6</span>
            <span class="recommend-count">2회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-body">Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.</div>
        
        
        <div class="compact-reason">이 논문은 시계열 생성의 계산 효율성을 개선해 에지 디바이스에서 비디오 처리 속도 향상에 기여합니다. 주파수 영역 압축으로 FLOPs 50% 감소와 낮은 지연 시간을 달성해 RK3588에서 실시간 하이라이트 생성이 가능합니다.</div>
        
      </div>
      
      <div class="compact-card compact-card--remind" data-paper-url="http://arxiv.org/abs/2602.10102v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.10102v1">VideoWorld 2: Learning Transferable Knowledge from Real-world Videos</a>
          </span>
          <span class="compact-meta">
            <span class="score-detail">82.4</span>
            <span class="recommend-count">2회째</span>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-body">Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.</div>
        
        
        <div class="compact-reason">동작과 시각적 요소를 분리하는 기술로 스포츠 동작 분석 정확도 향상에 직접 활용 가능합니다. 잠재 코드 기반 동역학 모델이 선수의 자세 패턴 인식에 적용되어 경기 전략 분석 품질을 높입니다.</div>
        
      </div>
      
    
  </div>

  <div class="tab-content tab-panel-below">
    
      <div class="empty-message">
        점수 미달 논문이 없습니다.
      </div>
    
  </div>

  <div class="tab-content tab-panel-excluded">
    
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.14153v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14153v1">ARport: An Augmented Reality System for Markerless Image-Guided Port Placement in Robotic Surgery</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-reason">수술용 AR 시스템으로 스포츠 프로젝트와 무관함.</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.13662v3">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13662v3">LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-reason">Plant pathology dataset, unrelated to sports.</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.16206v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.16206v1">Nonplanar Model Predictive Control for Autonomous Vehicles with Recursive Sparse Gaussian Process Dynamics</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
          <span class="cat-tag">eess.SY</span>
          
        </div>
        
        
        <div class="compact-reason">Unrelated to sports filming, editing, or analysis.</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.14672v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.14672v1">MeFEm: Medical Face Embedding model</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
        </div>
        
        
        <div class="compact-reason">의료 얼굴 분석 모델로 스포츠 프로젝트와 전혀 무관함.</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.17797v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.17797v1">Deep Learning for Dermatology: An Innovative Framework for Approaching Precise Skin Cancer Detection</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">eess.IV</span>
          
          <span class="cat-tag">cs.AI</span>
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-reason">Skin cancer detection, unrelated to sports</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.15145v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.15145v1">Exploring Performance Tradeoffs in Age-Aware Remote Monitoring with Satellites</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.NI</span>
          
        </div>
        
        
        <div class="compact-reason">원격 모니터링 기술은 스포츠 촬영 및 분석과 무관</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.18329v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18329v1">G-LoG Bi-filtration for Medical Image Classification</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">math.AT</span>
          
        </div>
        
        
        <div class="compact-reason">Unrelated medical imaging</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.12659v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.12659v1">IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-reason">Unrelated: Focuses on face dataset bias mitigation, no connection to sports or video processing</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.11942v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11942v1">Synthesis of Late Gadolinium Enhancement Images via Implicit Neural Representations for Cardiac Scar Segmentation</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.AI</span>
          
        </div>
        
        
        <div class="compact-reason">Unrelated: Medical image synthesis for cardiac scans, irrelevant to sports analysis</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.11547v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.11547v1">H.265/HEVC Video Steganalysis Based on CU Block Structure Gradients and IPM Mapping</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">eess.IV</span>
          
          <span class="cat-tag">cs.MM</span>
          
        </div>
        
        
        <div class="compact-reason">Unrelated: Video steganalysis for security detection, no relevance to sports filming or analysis</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.13900v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.13900v1">UAV-SEAD: State Estimation Anomaly Dataset for UAVs</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">cs.RO</span>
          
        </div>
        
        
        <div class="compact-reason">Unrelated: UAV anomaly dataset for flight reliability, unrelated to sports video processing</div>
        
      </div>
      
      <div class="compact-card compact-card--discarded" data-paper-url="http://arxiv.org/abs/2602.18350v1">
        <div class="rank-title">
          <span class="paper-title">
            <a href="http://arxiv.org/abs/2602.18350v1">Quantum-enhanced satellite image classification</a>
          </span>
        </div>
        
        <div class="categories">
          
          <span class="cat-tag">quant-ph</span>
          
          <span class="cat-tag">cs.CV</span>
          
          <span class="cat-tag">cs.LG</span>
          
        </div>
        
        
        <div class="compact-reason">양자 위성 이미지 분류로 관련성 없음</div>
        
      </div>
      
    
  </div>
</div>

<div class="download-toolbar" id="downloadToolbar">
  <div class="toolbar-selects">
    <button class="toolbar-btn" onclick="toggleSelect('all')">All</button>
    <button class="toolbar-btn" onclick="toggleSelect('tier1')">Tier 1</button>
    <button class="toolbar-btn" onclick="toggleSelect('tier2')">Tier 2</button>
    <button class="toolbar-btn" onclick="toggleSelect('none')">Deselect</button>
  </div>
  <button class="toolbar-btn primary" id="downloadBtn" onclick="downloadSelected()">Download 0 PDFs</button>
</div>

<footer>
  <p>이 리포트는 arXiv API를 사용하여 생성되었습니다.</p>
  <p>arXiv 논문의 저작권은 각 저자에게 있습니다.</p>
  <p>Thank you to arXiv for use of its open access interoperability.</p>
  <p>run_id: 19 | embedding: en_synthetic
    | weights: {&#39;embed&#39;: 0.35, &#39;llm&#39;: 0.55, &#39;recency&#39;: 0.1}</p>
</footer>

<script>
var READ_KEY = 'paperscout_read';
var SUPABASE_CFG = {
  enabled: true,
  url: 'https://qcvlnebvjzkgbbxainsk.supabase.co',
  key: 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InFjdmxuZWJ2anprZ2JieGFpbnNrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NzE0ODcyNTgsImV4cCI6MjA4NzA2MzI1OH0.Z53_RbpMnN3U6eyMlIxq-4h16hhvhawDzjKDb_iSNvI'
};

/* localStorage helpers */
function getLocalRead() {
  try { return JSON.parse(localStorage.getItem(READ_KEY) || '[]'); }
  catch(e) { return []; }
}
function saveLocalRead(arr) {
  try { localStorage.setItem(READ_KEY, JSON.stringify(arr)); } catch(e) {}
}

/* Supabase helpers */
function supabaseHeaders() {
  return {
    'apikey': SUPABASE_CFG.key,
    'Authorization': 'Bearer ' + SUPABASE_CFG.key,
    'Content-Type': 'application/json',
    'Prefer': 'return=minimal'
  };
}

function fetchReadFromSupabase() {
  if (!SUPABASE_CFG.enabled) return Promise.resolve([]);
  return fetch(SUPABASE_CFG.url + '/rest/v1/read_papers?select=paper_url', {
    headers: supabaseHeaders()
  })
  .then(function(r) { return r.ok ? r.json() : []; })
  .then(function(rows) { return rows.map(function(r) { return r.paper_url; }); })
  .catch(function() { return []; });
}

function markReadOnSupabase(url) {
  if (!SUPABASE_CFG.enabled || !url) return;
  fetch(SUPABASE_CFG.url + '/rest/v1/read_papers', {
    method: 'POST',
    headers: Object.assign({}, supabaseHeaders(), {'Prefer': 'return=minimal,resolution=ignore-duplicates'}),
    body: JSON.stringify({ paper_url: url })
  }).catch(function() {});
}

/* Unified read state */
var _readCache = null;

function getReadPapers() {
  if (_readCache) return _readCache;
  _readCache = getLocalRead();
  return _readCache;
}

function markAsRead(url) {
  if (!url) return;
  var read = getReadPapers();
  if (read.indexOf(url) === -1) {
    read.push(url);
    _readCache = read;
    saveLocalRead(read);
  }
  markReadOnSupabase(url);
}

function isRead(url) {
  return getReadPapers().indexOf(url) !== -1;
}

function applyReadState() {
  var cards = document.querySelectorAll('[data-paper-url]');
  var remindHidden = 0;
  cards.forEach(function(card) {
    var url = card.dataset.paperUrl;
    if (!isRead(url)) return;
    // Remind tab: hide read papers entirely
    if (card.classList.contains('compact-card--remind')) {
      card.style.display = 'none';
      remindHidden++;
    } else {
      card.classList.add('paper-read');
    }
  });
  // Update remind tab count
  var remindLabel = document.querySelector('label[for="tab-remind"]');
  if (remindLabel) {
    var total = document.querySelectorAll('.compact-card--remind').length;
    var visible = total - remindHidden;
    remindLabel.textContent = '\uB2E4\uC2DC \uBCF4\uAE30 (' + visible + ')';
  }
  // Show empty message if all remind papers are hidden
  var remindPanel = document.querySelector('.tab-panel-remind');
  if (remindPanel) {
    var visibleCards = remindPanel.querySelectorAll('.compact-card--remind:not([style*="display: none"])');
    var emptyMsg = remindPanel.querySelector('.empty-message');
    if (visibleCards.length === 0 && !emptyMsg) {
      var msg = document.createElement('div');
      msg.className = 'empty-message';
      msg.textContent = '\uB2E4\uC2DC \uBCF4\uAE30 \uB17C\uBB38\uC774 \uC5C6\uC2B5\uB2C8\uB2E4.';
      remindPanel.appendChild(msg);
    }
  }
}

function trackClicks() {
  document.addEventListener('click', function(e) {
    var link = e.target.closest('a[href]');
    if (!link) return;
    var card = link.closest('[data-paper-url]');
    if (!card) return;
    markAsRead(card.dataset.paperUrl);
    card.classList.add('paper-read');
    // If remind card, hide after brief delay
    if (card.classList.contains('compact-card--remind')) {
      setTimeout(function() {
        card.style.display = 'none';
        applyReadState();
      }, 200);
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  // Apply localStorage state immediately for fast UX
  applyReadState();
  trackClicks();
  // Then merge Supabase data for cross-device sync
  fetchReadFromSupabase().then(function(remote) {
    if (!remote || remote.length === 0) return;
    var local = getLocalRead();
    var merged = local.slice();
    var changed = false;
    remote.forEach(function(url) {
      if (merged.indexOf(url) === -1) {
        merged.push(url);
        changed = true;
      }
    });
    if (changed) {
      _readCache = merged;
      saveLocalRead(merged);
      applyReadState();
    }
  });
});

function showAllTier1() {
  document.querySelectorAll('.paper-card--collapsed').forEach(function(el) {
    el.classList.remove('paper-card--collapsed');
  });
  var btn = document.getElementById('showMoreBtn');
  if (btn) btn.style.display = 'none';
}

function toggleSelect(filter) {
  var boxes = document.querySelectorAll('.tab-panel-today .paper-checkbox');
  boxes.forEach(function(cb) {
    if (filter === 'none') { cb.checked = false; }
    else if (filter === 'all') { cb.checked = true; }
    else if (filter === 'tier1') { cb.checked = cb.dataset.tier === '1'; }
    else if (filter === 'tier2') { cb.checked = cb.dataset.tier === '2'; }
  });
  updateToolbar();
}

function updateToolbar() {
  var checked = document.querySelectorAll('.paper-checkbox:checked');
  var toolbar = document.getElementById('downloadToolbar');
  var btn = document.getElementById('downloadBtn');
  var count = 0;
  checked.forEach(function(cb) { if (cb.dataset.pdf) count++; });
  if (checked.length > 0) {
    toolbar.classList.add('visible');
    document.body.classList.add('toolbar-active');
    btn.textContent = 'Download ' + count + ' PDFs';
    btn.disabled = (count === 0);
  } else {
    toolbar.classList.remove('visible');
    document.body.classList.remove('toolbar-active');
  }
}

function downloadSelected() {
  var checked = document.querySelectorAll('.paper-checkbox:checked');
  var urls = [];
  checked.forEach(function(cb) { if (cb.dataset.pdf) urls.push(cb.dataset.pdf); });
  if (urls.length === 0) return;
  urls.forEach(function(url, i) {
    setTimeout(function() { window.open(url, '_blank'); }, i * 300);
  });
}
</script>

</body>
</html>